# ğŸ”§ Recipe êµ¬ì¡° ë³€ê²½ ì™„ì „ ë¦¬íŒ©í† ë§ ê³„íš

> **ì‹¤ì œ ì†ŒìŠ¤ì½”ë“œ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼**  
> ìƒˆë¡œìš´ Recipe YAML êµ¬ì¡°ì— ë§ì¶° ì‹œìŠ¤í…œì„ ì™„ì „íˆ ë¦¬íŒ©í† ë§í•˜ëŠ” ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšì…ë‹ˆë‹¤.

## ğŸ“‹ ë³€ê²½ ê°œìš”

### **í˜„ì¬ êµ¬ì¡°** â†’ **ìƒˆë¡œìš´ êµ¬ì¡°**

```yaml
# í˜„ì¬ (OLD)
data:
  loader:
    source_uri: "sql/train.sql"
    entity_schema:
      entity_columns: [user_id]
      timestamp_column: event_timestamp
  fetcher:
    type: feature_store
    features:
      - feature_namespace: user_features
        features: [age, gender]
  data_interface:
    task_type: classification
    target_column: label
    id_column: user_id           # âŒ ì œê±°

# ìƒˆë¡œìš´ (NEW)
data:
  loader:
    source_uri: "sql/train.sql"  # entity_schema ì œê±°
  fetcher:
    type: feature_store
    feature_views:               # âœ… ìƒˆë¡œìš´ êµ¬ì¡°
      user_features:
        join_key: user_id
        features: [age, gender]
    timestamp_column: event_timestamp  # âœ… fetcherë¡œ ì´ë™
  data_interface:
    task_type: classification
    target_column: label
    entity_columns: [user_id]    # âœ… id_column â†’ entity_columns
    treatment_column: campaign   # âœ… causal ì „ìš© ì¶”ê°€
    feature_columns: null        # âœ… null = ìë™ ì„ íƒ
```

---

## ğŸ¯ Step 1: Schema ì •ì˜ ìˆ˜ì • (Priority: ğŸ”´ Critical)

### **íŒŒì¼: `src/settings/recipe.py`**

#### **1.1 ìƒˆë¡œìš´ í´ë˜ìŠ¤ ì¶”ê°€**

```python
# line 95 ê·¼ì²˜ì— ì¶”ê°€
class FeatureView(BaseModel):
    """Feast FeatureView ì •ì˜ (ê°œë³„ í”¼ì²˜ ê·¸ë£¹)"""
    join_key: str = Field(..., description="Joiní•  ê¸°ì¤€ ì»¬ëŸ¼ (user_id, item_id ë“±)")
    features: List[str] = Field(..., description="í•´ë‹¹ FeatureViewì—ì„œ ê°€ì ¸ì˜¬ í”¼ì²˜ ëª©ë¡")
```

#### **1.2 ê¸°ì¡´ í´ë˜ìŠ¤ ì œê±°**

```python
# âŒ ì™„ì „ ì œê±°: EntitySchema í´ë˜ìŠ¤ (line 69-73)
# class EntitySchema(BaseModel):
#     entity_columns: List[str] = ...
#     timestamp_column: str = ...

# âŒ ì™„ì „ ì œê±°: FeatureNamespace í´ë˜ìŠ¤ (line 95-99)  
# class FeatureNamespace(BaseModel): ...
```

#### **1.3 Loader í´ë˜ìŠ¤ ìˆ˜ì •**

```python
# line 75-93 ìˆ˜ì •
class Loader(BaseModel):
    """ë°ì´í„° ë¡œë” ì„¤ì •"""
    source_uri: str = Field(..., description="ë°ì´í„° ì†ŒìŠ¤ URI (SQL íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ë°ì´í„° íŒŒì¼ ê²½ë¡œ)")
    # âŒ entity_schema í•„ë“œ ì™„ì „ ì œê±°
    
    def get_adapter_type(self) -> str:
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€
        ...
```

#### **1.4 Fetcher í´ë˜ìŠ¤ ì™„ì „ ì¬ì‘ì„±**

```python
# line 101-123 ì™„ì „ êµì²´
class Fetcher(BaseModel):
    """í”¼ì²˜ í˜ì²˜ ì„¤ì • - Feature Store í†µí•©"""
    type: Literal["feature_store", "pass_through"] = Field(..., description="í˜ì²˜ íƒ€ì…")
    
    # âœ… ìƒˆë¡œìš´ êµ¬ì¡°: feature_views
    feature_views: Optional[Dict[str, FeatureView]] = Field(
        None, 
        description="Feast FeatureView ì„¤ì • (feature_store íƒ€ì…ì—ì„œ ì‚¬ìš©)"
    )
    
    # âœ… ìƒˆë¡œìš´ í•„ë“œ: timestamp_column
    timestamp_column: Optional[str] = Field(
        None,
        description="Point-in-time join ê¸°ì¤€ íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼"
    )
    
    @field_validator('feature_views')
    def validate_feature_views(cls, v, info):
        """feature_store íƒ€ì…ì¼ ë•Œ feature_views ê²€ì¦"""
        if info.data.get('type') == 'feature_store':
            if not v:
                return {}  # ë¹ˆ dict ë°˜í™˜
        return v
```

#### **1.5 DataInterface í´ë˜ìŠ¤ ìˆ˜ì •**

```python
# line 125-141 ìˆ˜ì •
class DataInterface(BaseModel):
    """ë°ì´í„° ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
    task_type: Literal["classification", "regression", "clustering", "causal"] = Field(
        ..., 
        description="ML íƒœìŠ¤í¬ íƒ€ì…"
    )
    target_column: str = Field(..., description="íƒ€ê²Ÿ ì»¬ëŸ¼ ì´ë¦„")
    
    feature_columns: Optional[List[str]] = Field(
        None, 
        description="í”¼ì²˜ ì»¬ëŸ¼ ëª©ë¡ (Noneì´ë©´ target, treatment, entity ì œì™¸ ëª¨ë“  ì»¬ëŸ¼ ì‚¬ìš©)"
    )
    
    treatment_column: Optional[str] = Field(
        None, 
        description="ì²˜ì¹˜ ë³€ìˆ˜ ì»¬ëŸ¼ (causal taskì—ì„œë§Œ ì‚¬ìš©)"
    )
    
    # âœ… id_column â†’ entity_columns ë³€ê²½
    entity_columns: List[str] = Field(..., description="ì—”í‹°í‹° ì»¬ëŸ¼ ëª©ë¡ (user_id, item_id ë“±)")
```

---

## ğŸ¯ Step 2: Recipe Builder ìˆ˜ì • (Priority: ğŸŸ¡ High)

### **íŒŒì¼: `src/cli/utils/recipe_builder.py`**

#### **2.1 Entity Columns ì…ë ¥ ì¶”ê°€**

```python
# line 249 ì´í›„ì— ì¶”ê°€
        # Entity columns ì„¤ì • (ìƒˆë¡œ ì¶”ê°€)
        self.ui.show_info("ğŸ”— Entity Columns ì„¤ì •")
        entity_columns_str = self.ui.text_input(
            "Entity column(s) ì´ë¦„ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: user_id,item_id)",
            default="user_id"
        )
        entity_columns = [col.strip() for col in entity_columns_str.split(",")]
        selections["entity_columns"] = entity_columns
        
        # Feature columns ì²˜ë¦¬ ë°©ë²• ì•ˆë‚´
        self.ui.show_info("ğŸ“Š Feature Columns ìë™ ì²˜ë¦¬")
        self.ui.show_info(
            "ğŸ’¡ Feature columnsëŠ” ìë™ ì²˜ë¦¬ë©ë‹ˆë‹¤:\n"
            "   - Target, Treatment, Entity columnsë¥¼ ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼ ì‚¬ìš©\n"
            "   - ë³„ë„ ì„¤ì • ë¶ˆí•„ìš”"
        )
```

#### **2.2 Template ë³€ìˆ˜ ì—…ë°ì´íŠ¸**

```python
# generate_recipe ë©”ì„œë“œì—ì„œ template_vars ì—…ë°ì´íŠ¸
        template_vars = {
            # ... ê¸°ì¡´ ë³€ìˆ˜ë“¤ ...
            "entity_columns": selections["entity_columns"],
            # feature_columnsëŠ” í•­ìƒ null (ìë™ ì²˜ë¦¬)
        }
```

---

## ğŸ¯ Step 3: Factory í´ë˜ìŠ¤ ìˆ˜ì • (Priority: ğŸ”´ Critical)

### **íŒŒì¼: `src/factory/factory.py`**

#### **3.1 create_data_adapter ë©”ì„œë“œ ìˆ˜ì •**

```python
# line 162-182 ìˆ˜ì •
    def create_data_adapter(self, adapter_type: Optional[str] = None) -> "BaseAdapter":
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€í•˜ë˜ ê²½ë¡œ ë³€ê²½
        if adapter_type:
            target_type = adapter_type
        else:
            # âœ… ìƒˆë¡œìš´ ê²½ë¡œ: entity_schema ê²½ìœ í•˜ì§€ ì•ŠìŒ
            source_uri = self._data.loader.source_uri
            target_type = self._detect_adapter_type_from_uri(source_uri)
            # ... ë‚˜ë¨¸ì§€ ë™ì¼
```

#### **3.2 create_pyfunc_wrapper ë©”ì„œë“œ ìˆ˜ì •**

```python
# line 410-463 ìˆ˜ì •: entity_schema ì ‘ê·¼ ë°©ì‹ ë³€ê²½
    def create_pyfunc_wrapper(
        self, 
        trained_model: Any, 
        trained_preprocessor: Optional[BasePreprocessor],
        trained_fetcher: Optional['BaseFetcher'],
        training_df: Optional[pd.DataFrame] = None,
        training_results: Optional[Dict[str, Any]] = None
    ) -> PyfuncWrapper:
        """PyfuncWrapper ìƒì„± - ìƒˆë¡œìš´ ìŠ¤í‚¤ë§ˆ êµ¬ì¡° ëŒ€ì‘"""
        from src.factory.artifact import PyfuncWrapper
        logger.info("Creating PyfuncWrapper artifact...")
        
        signature, data_schema = None, None
        if training_df is not None:
            logger.info("Generating model signature and data schema from training_df...")
            from src.utils.integrations.mlflow_integration import create_enhanced_model_signature_with_schema
            
            # âœ… ìƒˆë¡œìš´ êµ¬ì¡°ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
            fetcher_conf = self._recipe.data.fetcher
            data_interface = self._recipe.data.data_interface
            
            # Timestamp ì»¬ëŸ¼ ì²˜ë¦¬
            ts_col = fetcher_conf.timestamp_column if fetcher_conf else None
            if ts_col and ts_col in training_df.columns:
                import pandas as pd
                if not pd.api.types.is_datetime64_any_dtype(training_df[ts_col]):
                    training_df = training_df.copy()
                    training_df[ts_col] = pd.to_datetime(training_df[ts_col], errors='coerce')

            # âœ… ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ data_interface_config êµ¬ì„±
            data_interface_config = {
                'entity_columns': data_interface.entity_columns,
                'timestamp_column': ts_col,
                'task_type': data_interface.task_type,
                'target_column': data_interface.target_column,
                'treatment_column': getattr(data_interface, 'treatment_column', None),
            }
            
            signature, data_schema = create_enhanced_model_signature_with_schema(
                training_df, 
                data_interface_config
            )
            logger.info("âœ… Signature and data schema created successfully.")
        
        return PyfuncWrapper(
            settings=self.settings,
            trained_model=trained_model,
            trained_preprocessor=trained_preprocessor,
            trained_fetcher=trained_fetcher,
            training_results=training_results,
            signature=signature,
            data_schema=data_schema,
        )
```

---

## ğŸ¯ Step 4: Data Handler ìˆ˜ì • (Priority: ğŸŸ¡ High)

### **íŒŒì¼: `src/components/trainer/modules/data_handler.py`**

#### **4.1 _get_exclude_columns í•¨ìˆ˜ ìˆ˜ì •**

```python
# line 19-33 ìˆ˜ì •
def _get_exclude_columns(settings: Settings, df: pd.DataFrame) -> list:
    preproc = getattr(settings.recipe.model, "preprocessor", None)
    params = getattr(preproc, "params", None) if preproc else None
    recipe_exclude = params.get("exclude_cols", []) if isinstance(params, dict) else []

    # âœ… ìƒˆë¡œìš´ êµ¬ì¡°ì—ì„œ ì—”í‹°í‹°/íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼ ìˆ˜ì§‘
    data_interface = settings.recipe.data.data_interface
    fetcher_conf = settings.recipe.data.fetcher
    
    default_exclude = []
    
    # Entity columns ì¶”ê°€
    try:
        default_exclude.extend(data_interface.entity_columns or [])
    except Exception:
        pass
    
    # Timestamp column ì¶”ê°€
    try:
        ts_col = fetcher_conf.timestamp_column if fetcher_conf else None
        if ts_col:
            default_exclude.append(ts_col)
    except Exception:
        pass

    # êµì°¨ ì ìš©
    candidates = set(default_exclude) | set(recipe_exclude)
    return [c for c in candidates if c in df.columns]
```

#### **4.2 prepare_training_data í•¨ìˆ˜ ìˆ˜ì •**

```python
# line 34-70 ìˆ˜ì •: feature_columns null ì²˜ë¦¬ ë¡œì§ ì¶”ê°€
def prepare_training_data(df: pd.DataFrame, settings: Settings) -> Tuple[pd.DataFrame, pd.Series, Dict[str, Any]]:
    """ë™ì  ë°ì´í„° ì¤€ë¹„ + feature_columns null ì²˜ë¦¬"""
    data_interface = settings.recipe.data.data_interface
    task_type = data_interface.task_type
    exclude_cols = _get_exclude_columns(settings, df)
    
    if task_type in ["classification", "regression"]:
        target_col = data_interface.target_column
        
        # âœ… feature_columns null ì²˜ë¦¬ ë¡œì§
        if data_interface.feature_columns is None:
            # ìë™ ì„ íƒ: target, treatment, entity ì œì™¸ ëª¨ë“  ì»¬ëŸ¼
            auto_exclude = [target_col] + exclude_cols
            if data_interface.treatment_column:
                auto_exclude.append(data_interface.treatment_column)
            
            X = df.drop(columns=[c for c in auto_exclude if c in df.columns])
            logger.info(f"Feature columns ìë™ ì„ íƒ: {list(X.columns)}")
        else:
            # ëª…ì‹œì  ì„ íƒ
            X = df[data_interface.feature_columns]
            
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        X = X.select_dtypes(include=[np.number])
        y = df[target_col]
        additional_data = {}
        
    elif task_type == "clustering":
        # âœ… feature_columns null ì²˜ë¦¬
        if data_interface.feature_columns is None:
            auto_exclude = exclude_cols
            X = df.drop(columns=[c for c in auto_exclude if c in df.columns])
            logger.info(f"Feature columns ìë™ ì„ íƒ (clustering): {list(X.columns)}")
        else:
            X = df[data_interface.feature_columns]
            
        X = X.select_dtypes(include=[np.number])
        y = None
        additional_data = {}
        
    elif task_type == "causal":
        target_col = data_interface.target_column
        treatment_col = data_interface.treatment_column
        
        # âœ… feature_columns null ì²˜ë¦¬
        if data_interface.feature_columns is None:
            auto_exclude = [target_col, treatment_col] + exclude_cols
            X = df.drop(columns=[c for c in auto_exclude if c in df.columns])
            logger.info(f"Feature columns ìë™ ì„ íƒ (causal): {list(X.columns)}")
        else:
            X = df[data_interface.feature_columns]
            
        X = X.select_dtypes(include=[np.number])
        y = df[target_col]
        additional_data = {
            'treatment': df[treatment_col],
            'treatment_value': getattr(data_interface, 'treatment_value', 1)
        }
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” task_type: {task_type}")
    
    return X, y, additional_data
```

---

## ğŸ¯ Step 5: Feature Store Fetcher ìˆ˜ì • (Priority: ğŸŸ¡ High)

### **íŒŒì¼: `src/components/fetcher/modules/feature_store_fetcher.py`**

#### **5.1 fetch ë©”ì„œë“œ ì™„ì „ ìˆ˜ì •**

```python
# line 18-60 ì™„ì „ êµì²´
    def fetch(self, df: pd.DataFrame, run_mode: str = "batch") -> pd.DataFrame:
        logger.info("Feature Storeë¥¼ í†µí•´ í”¼ì²˜ ì¦ê°•ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        # âœ… ìƒˆë¡œìš´ êµ¬ì¡°ì—ì„œ ì„¤ì • ìˆ˜ì§‘
        data_interface = self.settings.recipe.data.data_interface
        fetcher_conf = self.settings.recipe.data.fetcher

        # âœ… ìƒˆë¡œìš´ feature_views êµ¬ì¡°ì—ì„œ features ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
        features: List[str] = []
        if fetcher_conf and fetcher_conf.feature_views:
            for view_name, view_config in fetcher_conf.feature_views.items():
                for feature in view_config.features:
                    features.append(f"{view_name}:{feature}")

        # âœ… ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ data_interface_config êµ¬ì„±
        data_interface_config: Dict[str, Any] = {
            'entity_columns': data_interface.entity_columns,
            'timestamp_column': fetcher_conf.timestamp_column if fetcher_conf else None,
            'task_type': data_interface.task_type,
            'target_column': data_interface.target_column,
            'treatment_column': getattr(data_interface, 'treatment_column', None),
        }

        if run_mode in ("train", "batch"):
            # ì˜¤í”„ë¼ì¸ PIT ì¡°íšŒ
            result = self.feature_store_adapter.get_historical_features_with_validation(
                entity_df=df,
                features=features,
                data_interface_config=data_interface_config,
            )
            logger.info("í”¼ì²˜ ì¦ê°• ì™„ë£Œ(offline).")
            return result
        elif run_mode == "serving":
            # ì˜¨ë¼ì¸ ì¡°íšŒ
            entity_rows = df[data_interface.entity_columns].to_dict(orient="records")
            result = self.feature_store_adapter.get_online_features(
                entity_rows=entity_rows,
                features=features,
            )
            logger.info("í”¼ì²˜ ì¦ê°• ì™„ë£Œ(online).")
            return result
        else:
            raise ValueError(f"Unsupported run_mode: {run_mode}")
```

---

## ğŸ¯ Step 6: Schema Utils ìˆ˜ì • (Priority: ğŸŸ¡ High)

### **íŒŒì¼: `src/utils/system/schema_utils.py`**

#### **6.1 validate_schema í•¨ìˆ˜ ìˆ˜ì •**

```python
# line 25-35 ìˆ˜ì •
def validate_schema(df: pd.DataFrame, settings: "Settings", for_training: bool = False):
    """ìŠ¤í‚¤ë§ˆ ê²€ì¦ - ìƒˆë¡œìš´ êµ¬ì¡° ëŒ€ì‘"""
    logger.info(f"ëª¨ë¸ ì…ë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆë¥¼ ê²€ì¦í•©ë‹ˆë‹¤... (for_training: {for_training})")

    # âœ… ìƒˆë¡œìš´ êµ¬ì¡°ì—ì„œ ì„¤ì • ìˆ˜ì§‘
    data_interface = settings.recipe.data.data_interface
    fetcher_conf = settings.recipe.data.fetcher
    
    errors = []
    required_columns = []
    
    if not for_training:
        # ì›ë³¸ ë°ì´í„° ê²€ì¦: Entity + Timestamp í•„ìˆ˜
        required_columns = data_interface.entity_columns[:]
        if fetcher_conf and fetcher_conf.timestamp_column:
            required_columns.append(fetcher_conf.timestamp_column)
        
        # Target ì»¬ëŸ¼ (clustering ì œì™¸)
        if data_interface.task_type != "clustering" and data_interface.target_column:
            required_columns.append(data_interface.target_column)
    else:
        # ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°: entity/timestamp ì œì™¸
        logger.info("ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ê²€ì¦: entity_columns, timestamp_column ì œì™¸")
        required_columns = []
        
    # Treatment ì»¬ëŸ¼ (causal ì „ìš©)
    if data_interface.task_type == "causal" and data_interface.treatment_column:
        required_columns.append(data_interface.treatment_column)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
    for col in required_columns:
        if col not in df.columns:
            errors.append(f"- í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: '{col}' (task_type: {data_interface.task_type})")
    
    # Timestamp íƒ€ì… ê²€ì¦
    ts_col = fetcher_conf.timestamp_column if fetcher_conf else None
    if ts_col and ts_col in df.columns:
        if not pd.api.types.is_datetime64_any_dtype(df[ts_col]):
            try:
                pd.to_datetime(df[ts_col])
                logger.info(f"Timestamp ì»¬ëŸ¼ '{ts_col}' ìë™ ë³€í™˜ ê°€ëŠ¥")
            except Exception:
                errors.append(f"- Timestamp ì»¬ëŸ¼ '{ts_col}' íƒ€ì… ì˜¤ë¥˜: datetime ë³€í™˜ ë¶ˆê°€")

    if errors:
        error_message = "ëª¨ë¸ ì…ë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨:\n" + "\n".join(errors)
        error_message += f"\n\ní•„ìˆ˜ ì»¬ëŸ¼: {required_columns}"
        error_message += f"\nì‹¤ì œ ì»¬ëŸ¼: {list(df.columns)}"
        raise TypeError(error_message)
    
    logger.info(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì„±ê³µ (task_type: {data_interface.task_type})")
```

---

## ğŸ¯ Step 7: Template êµì²´ (Priority: ğŸŸ¢ Low)

### **íŒŒì¼: `src/cli/templates/recipes/recipe.yaml.j2`**

ì‚¬ìš©ìê°€ ì œê³µí•œ ìƒˆë¡œìš´ í…œí”Œë¦¿ìœ¼ë¡œ **ì™„ì „ êµì²´**í•˜ë©´ ë©ë‹ˆë‹¤.

---

## ğŸ¯ Step 8: í…ŒìŠ¤íŠ¸ ìˆ˜ì • (Priority: ğŸŸ¡ High)

### **ì˜í–¥ë°›ëŠ” í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤**

1. **`tests/unit/settings/test_recipe.py`** - Recipe ìŠ¤í‚¤ë§ˆ í…ŒìŠ¤íŠ¸
2. **`tests/conftest.py`** - í…ŒìŠ¤íŠ¸ í”½ìŠ¤ì²˜
3. **`tests/helpers/builders.py`** - í…ŒìŠ¤íŠ¸ ë¹Œë”
4. **Feature Store ê´€ë ¨ í…ŒìŠ¤íŠ¸ë“¤**

#### **8.1 ê¸°ë³¸ ìˆ˜ì • ë°©í–¥**

```python
# ê¸°ì¡´ EntitySchema ì‚¬ìš© â†’ ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ ë³€ê²½
# OLD
entity_schema = EntitySchema(
    entity_columns=["user_id"],
    timestamp_column="event_timestamp"
)

# NEW  
fetcher = Fetcher(
    type="feature_store",
    feature_views={
        "user_features": FeatureView(
            join_key="user_id",
            features=["age", "gender"]
        )
    },
    timestamp_column="event_timestamp"
)

data_interface = DataInterface(
    task_type="classification",
    target_column="label",
    entity_columns=["user_id"]  # id_column â†’ entity_columns
)
```

---

## ğŸ“‹ ì‹¤í–‰ ìˆœì„œ (Critical Path)

### **Phase 1: í•µì‹¬ ìŠ¤í‚¤ë§ˆ ë³€ê²½** âš¡
1. `src/settings/recipe.py` ìˆ˜ì •
2. `src/factory/factory.py` ìˆ˜ì •
3. ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ìˆ˜ì •

### **Phase 2: ë°ì´í„° ì²˜ë¦¬ ë¡œì§** ğŸ”§
4. `src/components/trainer/modules/data_handler.py` ìˆ˜ì •
5. `src/components/fetcher/modules/feature_store_fetcher.py` ìˆ˜ì •
6. `src/utils/system/schema_utils.py` ìˆ˜ì •

### **Phase 3: ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤** ğŸ–¥ï¸
7. `src/cli/utils/recipe_builder.py` ìˆ˜ì •
8. Template êµì²´

### **Phase 4: í†µí•© í…ŒìŠ¤íŠ¸** âœ…
9. ëª¨ë“  í…ŒìŠ¤íŠ¸ ìˆ˜ì •
10. í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### **í˜¸í™˜ì„± ë³´ì¥**
- ê¸°ì¡´ Recipe íŒŒì¼ë“¤ì´ **ì¦‰ì‹œ ê¹¨ì§‘ë‹ˆë‹¤**
- ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ í•„ìš”í•  ìˆ˜ ìˆìŒ
- ë‹¨ê³„ì  ë°°í¬ ê¶Œì¥

### **ë°ì´í„° ê²€ì¦**
- `feature_columns: null` ë¡œì§ ì² ì €í•œ í…ŒìŠ¤íŠ¸ í•„ìš”
- Entity columns ì¤‘ë³µ ì²˜ë¦¬ í™•ì¸
- Causal taskì—ì„œ treatment_column í•„ìˆ˜ ê²€ì¦

### **Feature Store í†µí•©**
- Feast adapter í˜¸í™˜ì„± í™•ì¸
- Point-in-time join ë¡œì§ ê²€ì¦
- Online/Offline store ëª¨ë‘ í…ŒìŠ¤íŠ¸

---

## ğŸ¯ ì™„ë£Œ ê¸°ì¤€

- [ ] ìƒˆë¡œìš´ Recipe YAMLë¡œ ì •ìƒ í•™ìŠµ ê°€ëŠ¥
- [ ] Feature Store ì—°ë™ ì •ìƒ ë™ì‘  
- [ ] Causal taskì—ì„œ treatment_column ì •ìƒ ì²˜ë¦¬
- [ ] feature_columns nullì¼ ë•Œ ìë™ ì„ íƒ ì •ìƒ ë™ì‘
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] CLI Recipe Builder ì •ìƒ ë™ì‘

ì´ ê³„íšëŒ€ë¡œ ì§„í–‰í•˜ë©´ ìƒˆë¡œìš´ Recipe êµ¬ì¡°ë¡œ ì™„ì „íˆ ë¦¬íŒ©í† ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€