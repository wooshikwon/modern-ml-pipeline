import uvicorn
import pandas as pd
import mlflow
import mlflow.pyfunc
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from typing import Dict, Any, List, Type
from pydantic import BaseModel, create_model

from src.settings import Settings, load_settings
from src.utils.integrations import mlflow_integration as mlflow_utils
from serving.schemas import (
    get_pk_from_loader_sql,
    create_dynamic_prediction_request,
    create_batch_prediction_request,
    PredictionResponse,
    BatchPredictionResponse,
    HealthCheckResponse,
    ModelMetadataResponse,
    OptimizationHistoryResponse,
    HyperparameterOptimizationInfo,
    TrainingMethodologyInfo,
)
from src.utils.system.logger import logger

class AppContext:
    def __init__(self):
        self.model: mlflow.pyfunc.PyFuncModel | None = None
        self.model_uri: str = ""
        self.settings: Settings | None = None
        self.PredictionRequest: Type[BaseModel] = create_model("DefaultPredictionRequest")
        self.BatchPredictionRequest: Type[BaseModel] = create_model("DefaultBatchPredictionRequest")

app_context = AppContext()

def setup_api_context(run_id: str, settings: Settings):
    """í…ŒìŠ¤íŠ¸ ë˜ëŠ” ì„œë²„ ì‹œì‘ ì‹œ API ì»¨í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜"""
    try:
        model_uri = f"runs:/{run_id}/model"
        app_context.model = mlflow.pyfunc.load_model(model_uri)
        app_context.model_uri = model_uri
        app_context.settings = settings
        
        wrapped_model = app_context.model.unwrap_python_model()
        loader_sql = getattr(wrapped_model, 'loader_sql_snapshot', 'SELECT user_id FROM DUAL')
        
        from src.utils.system.sql_utils import parse_select_columns
        pk_fields = parse_select_columns(loader_sql)
        
        app_context.PredictionRequest = create_dynamic_prediction_request(
            model_name="DynamicPredictionRequest", pk_fields=pk_fields
        )
        app_context.BatchPredictionRequest = create_batch_prediction_request(
            app_context.PredictionRequest
        )
        logger.info(f"API ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ì™„ë£Œ: {model_uri}")
    except Exception as e:
        logger.error(f"API ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}", exc_info=True)
        raise

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹¤ì œ ì„œë²„ ì‹¤í–‰ ì‹œ ì‚¬ìš©í•  lifespan
    # run_idëŠ” í™˜ê²½ ë³€ìˆ˜ ë“±ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
    run_id = mlflow_utils.get_latest_run_id(
        experiment_name=load_settings().mlflow.experiment_name
    )
    settings = load_settings()
    setup_api_context(run_id, settings)
    logger.info("FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘...")
    yield
    logger.info("FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ.")

# í…ŒìŠ¤íŠ¸ì™€ ì‹¤ì œ ì„œë¹™ ëª¨ë‘ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ìµœìƒìœ„ app ê°ì²´
app = FastAPI(
    title="Modern ML Pipeline API",
    description="Blueprint v17.0 ê¸°ë°˜ ëª¨ë¸ ì„œë¹™ API",
    version="17.0.0",
    lifespan=lifespan  # ì‹¤ì œ ì„œë²„ ì‹¤í–‰ ì‹œì—ë§Œ lifespanì´ í™œì„±í™”ë¨
)

@app.on_event("startup")
async def startup_event():
    # ì‹¤ì œ run_idëŠ” run_api_server í•¨ìˆ˜ì—ì„œ ì£¼ì…ë°›ì•„ ì„¤ì •ë©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” FastAPIì˜ ë¼ì´í”„ì‚¬ì´í´ ì´ë²¤íŠ¸ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ì˜ˆì‹œì…ë‹ˆë‹¤.
    pass

@app.get("/", tags=["General"])
def root() -> Dict[str, str]:
    return {
        "message": "Modern ML Pipeline API",
        "status": "ready" if app_context.model else "error",
        "model_uri": app_context.model_uri,
    }

@app.get("/health", response_model=HealthCheckResponse, tags=["General"])
def health() -> HealthCheckResponse:
    if not app_context.model or not app_context.settings:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    model_info = "unknown"
    try:
        wrapped_model = app_context.model.unwrap_python_model()
        model_info = getattr(wrapped_model, 'model_class_path', 'unknown')
    except Exception:
        pass

    return HealthCheckResponse(
        status="healthy",
        model_uri=app_context.model_uri,
        model_name=model_info,
    )

@app.post("/predict", response_model=PredictionResponse, tags=["Prediction"])
def predict(request: BaseModel) -> PredictionResponse:
    if not app_context.model or not app_context.settings:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    try:
        DynamicRequest = app_context.PredictionRequest
        validated_request = DynamicRequest(**request.model_dump())
        
        input_df = pd.DataFrame([validated_request.model_dump()])
        
        predict_params = { "run_mode": "serving", "return_intermediate": True }
        
        predictions = app_context.model.predict(input_df, params=predict_params)
        
        # ê²°ê³¼ êµ¬ì¡°ê°€ DataFrameì´ë¼ê³  ê°€ì •
        prediction_value = predictions["prediction"].iloc[0]
        input_features_dict = predictions["input_features"].iloc[0]

        return PredictionResponse(
            prediction=prediction_value,
            input_features=input_features_dict,
        )
    except Exception as e:
        logger.error(f"ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predict_batch", response_model=BatchPredictionResponse, tags=["Prediction"])
def predict_batch(request: BaseModel) -> BatchPredictionResponse:
    if not app_context.model or not app_context.settings:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    try:
        DynamicBatchRequest = app_context.BatchPredictionRequest
        validated_request = DynamicBatchRequest(**request.model_dump())

        input_df = pd.DataFrame([sample.model_dump() for sample in validated_request.samples])
        if input_df.empty:
            raise HTTPException(status_code=400, detail="ì…ë ¥ ìƒ˜í”Œì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        predict_params = { "run_mode": "serving", "return_intermediate": False }
        predictions_df = app_context.model.predict(input_df, params=predict_params)
        
        return BatchPredictionResponse(
            predictions=predictions_df.to_dict(orient="records"),
            model_uri=app_context.model_uri,
            sample_count=len(predictions_df)
        )
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# ğŸ†• Blueprint v17.0: ëª¨ë¸ ë©”íƒ€ë°ì´í„° ìê¸° ê¸°ìˆ  ì—”ë“œí¬ì¸íŠ¸ë“¤
    
@app.get("/model/metadata", response_model=ModelMetadataResponse, tags=["Model Metadata"])
def get_model_metadata() -> ModelMetadataResponse:
    """
    ëª¨ë¸ì˜ ì™„ì „í•œ ë©”íƒ€ë°ì´í„° ë°˜í™˜ (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”, Data Leakage ë°©ì§€ ì •ë³´ í¬í•¨)
    """
    try:
        if app_context.model is None:
            raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì •ë³´ êµ¬ì„±
        hpo_info = app_context.model.hyperparameter_optimization
        hyperparameter_optimization = HyperparameterOptimizationInfo(
            enabled=hpo_info.get("enabled", False),
            engine=hpo_info.get("engine", ""),
            best_params=hpo_info.get("best_params", {}),
            best_score=hpo_info.get("best_score", 0.0),
            total_trials=hpo_info.get("total_trials", 0),
            pruned_trials=hpo_info.get("pruned_trials", 0),
            optimization_time=str(hpo_info.get("optimization_time", "")),
        )
        
        # í•™ìŠµ ë°©ë²•ë¡  ì •ë³´ êµ¬ì„±
        tm_info = app_context.model.training_methodology
        training_methodology = TrainingMethodologyInfo(
            train_test_split_method=tm_info.get("train_test_split_method", ""),
            train_ratio=tm_info.get("train_ratio", 0.8),
            validation_strategy=tm_info.get("validation_strategy", ""),
            preprocessing_fit_scope=tm_info.get("preprocessing_fit_scope", ""),
            random_state=tm_info.get("random_state", 42),
        )
        
        # API ìŠ¤í‚¤ë§ˆ ì •ë³´ êµ¬ì„±
        api_schema = {
            "input_fields": [field for field in app_context.PredictionRequest.__fields__.keys()],
            "sql_source": "loader_sql_snapshot",
            "feature_columns": app_context.model.feature_columns, # ì‹¤ì œ ëª¨ë¸ì—ì„œ ê°€ì ¸ì˜´
            "join_key": app_context.model.join_key, # ì‹¤ì œ ëª¨ë¸ì—ì„œ ê°€ì ¸ì˜´
        }
        
        return ModelMetadataResponse(
            model_uri=app_context.model_uri,
            model_class_path=getattr(app_context.model, "model_class_path", ""),
            hyperparameter_optimization=hyperparameter_optimization,
            training_methodology=training_methodology,
            training_metadata=app_context.model.training_metadata,
            api_schema=api_schema,
        )
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/model/optimization", response_model=OptimizationHistoryResponse, tags=["Model Metadata"])
def get_optimization_history() -> OptimizationHistoryResponse:
    """
    í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê³¼ì •ì˜ ìƒì„¸ íˆìŠ¤í† ë¦¬ ë°˜í™˜
    """
    try:
        if app_context.model is None:
            raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        hpo_info = app_context.model.hyperparameter_optimization
        
        if not hpo_info.get("enabled", False):
            return OptimizationHistoryResponse(
                enabled=False,
                optimization_history=[],
                search_space={},
                convergence_info={"message": "í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤."},
                timeout_occurred=False,
            )
        
        return OptimizationHistoryResponse(
            enabled=True,
            optimization_history=hpo_info.get("optimization_history", []),
            search_space=hpo_info.get("search_space", {}),
            convergence_info={
                "best_score": hpo_info.get("best_score", 0.0),
                "total_trials": hpo_info.get("total_trials", 0),
                "pruned_trials": hpo_info.get("pruned_trials", 0),
                "optimization_time": str(hpo_info.get("optimization_time", "")),
            },
            timeout_occurred=hpo_info.get("timeout_occurred", False),
        )
        
    except Exception as e:
        logger.error(f"ìµœì í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/model/schema", tags=["Model Metadata"])
def get_api_schema() -> Dict[str, Any]:
    """
    ë™ì ìœ¼ë¡œ ìƒì„±ëœ API ìŠ¤í‚¤ë§ˆ ì •ë³´ ë°˜í™˜
    """
    try:
        if app_context.model is None:
            raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return {
            "prediction_request_schema": app_context.PredictionRequest.schema(),
            "batch_prediction_request_schema": app_context.BatchPredictionRequest.schema(),
            "loader_sql_snapshot": app_context.model.loader_sql_snapshot,
            "extracted_fields": [field for field in app_context.PredictionRequest.__fields__.keys()],
            "feature_store_info": {
                "feature_columns": app_context.model.feature_columns,
                "join_key": app_context.model.join_key,
                "feature_store_config": app_context.settings.serving.get('realtime_feature_store', {}),
            },
        }
        
    except Exception as e:
        logger.error(f"API ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

def run_api_server(settings: Settings, run_id: str, host: str = "0.0.0.0", port: int = 8000):
    """
    FastAPI ì„œë²„ë¥¼ ì‹¤í–‰í•˜ê³ , Lifespan ì´ë²¤íŠ¸ë¥¼ í†µí•´ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    # Blueprint ì›ì¹™ 9: í™˜ê²½ë³„ ê¸°ëŠ¥ ë¶„ë¦¬ - API ì„œë¹™ ì‹œìŠ¤í…œì  ì°¨ë‹¨
    serving_settings = getattr(settings, 'serving', None)
    if not serving_settings or not getattr(serving_settings, 'enabled', True):
        logger.error(f"'{settings.environment.app_env}' í™˜ê²½ì—ì„œëŠ” API ì„œë¹™ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    # ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
    model_uri = mlflow_utils.get_model_uri(run_id)
    app_context.model = mlflow_utils.load_pyfunc_model(settings, model_uri)
    
    # ë™ì  ë¼ìš°íŠ¸ ìƒì„±
    PredictionRequest = create_dynamic_prediction_request(app_context.model)
    
    @app.post("/predict", tags=["Predictions"])
    def predict(request: PredictionRequest):
        # Pydantic ëª¨ë¸ì„ dictìœ¼ë¡œ ë³€í™˜ í›„, DataFrameìœ¼ë¡œ ë³€í™˜
        request_df = pd.DataFrame([request.dict()])
        
        # ëª¨ë¸ ì˜ˆì¸¡
        predictions = app_context.model.predict(request_df)
        return {"predictions": predictions.to_dict(orient="records")}

    uvicorn.run(app, host=host, port=port)
