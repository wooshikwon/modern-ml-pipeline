import uvicorn
import pandas as pd
import mlflow
import mlflow.pyfunc
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from typing import Dict, Any, List, Type
from pydantic import BaseModel, create_model

from src.settings import Settings, load_settings
from src.utils.system.logger import logger
from serving.schemas import (
    get_pk_from_loader_sql,
    create_dynamic_prediction_request,
    create_batch_prediction_request,
    PredictionResponse,
    BatchPredictionResponse,
    HealthCheckResponse,
    ModelMetadataResponse,
    OptimizationHistoryResponse,
    HyperparameterOptimizationInfo,
    TrainingMethodologyInfo,
)

class AppContext:
    def __init__(self):
        self.model: mlflow.pyfunc.PyFuncModel | None = None
        self.model_uri: str = ""
        self.settings: Settings | None = None
        self.PredictionRequest: Type[BaseModel] = create_model("DefaultPredictionRequest")
        self.BatchPredictionRequest: Type[BaseModel] = create_model("DefaultBatchPredictionRequest")

app_context = AppContext()

def setup_api_context(run_id: str, settings: Settings):
    """í…ŒìŠ¤íŠ¸ ë˜ëŠ” ì„œë²„ ì‹œì‘ ì‹œ API ì»¨í…ìŠ¤íŠ¸ë¥¼ ì„¤ì •í•˜ëŠ” í•¨ìˆ˜"""
    try:
        model_uri = f"runs:/{run_id}/model"
        app_context.model = mlflow.pyfunc.load_model(model_uri)
        app_context.model_uri = model_uri
        app_context.settings = settings
        
        wrapped_model = app_context.model.unwrap_python_model()
        loader_sql = getattr(wrapped_model, 'loader_sql_snapshot', 'SELECT user_id FROM DUAL')
        
        from src.utils.system.sql_utils import parse_select_columns
        pk_fields = parse_select_columns(loader_sql)
        
        app_context.PredictionRequest = create_dynamic_prediction_request(
            model_name="DynamicPredictionRequest", pk_fields=pk_fields
        )
        app_context.BatchPredictionRequest = create_batch_prediction_request(
            app_context.PredictionRequest
        )
        logger.info(f"API ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ì™„ë£Œ: {model_uri}")
    except Exception as e:
        logger.error(f"API ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}", exc_info=True)
        raise

@asynccontextmanager
async def lifespan(app: FastAPI):
    # API ì„œë²„ ì‹œì‘
    logger.info("ğŸš€ Modern ML Pipeline API ì„œë²„ ì‹œì‘...")
    yield
    # API ì„œë²„ ì¢…ë£Œ
    logger.info("âœ… Modern ML Pipeline API ì„œë²„ ì¢…ë£Œ.")

# í…ŒìŠ¤íŠ¸ì™€ ì‹¤ì œ ì„œë¹™ ëª¨ë‘ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ìµœìƒìœ„ app ê°ì²´
app = FastAPI(
    title="Modern ML Pipeline API",
    description="Blueprint v17.0 ê¸°ë°˜ ëª¨ë¸ ì„œë¹™ API",
    version="17.0.0",
    lifespan=lifespan,
)

@app.get("/", tags=["General"])
def root() -> Dict[str, str]:
    return {
        "message": "Modern ML Pipeline API",
        "status": "ready" if app_context.model else "error",
        "model_uri": app_context.model_uri,
    }

@app.get("/health", response_model=HealthCheckResponse, tags=["General"])
def health() -> HealthCheckResponse:
    if not app_context.model or not app_context.settings:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    model_info = "unknown"
    try:
        wrapped_model = app_context.model.unwrap_python_model()
        model_info = getattr(wrapped_model, 'model_class_path', 'unknown')
    except Exception:
        pass

    return HealthCheckResponse(
        status="healthy",
        model_uri=app_context.model_uri,
        model_name=model_info,
    )



@app.post("/predict_batch", response_model=BatchPredictionResponse, tags=["Prediction"])
def predict_batch(request: BaseModel) -> BatchPredictionResponse:
    if not app_context.model or not app_context.settings:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    try:
        DynamicBatchRequest = app_context.BatchPredictionRequest
        validated_request = DynamicBatchRequest(**request.model_dump())

        input_df = pd.DataFrame([sample.model_dump() for sample in validated_request.samples])
        if input_df.empty:
            raise HTTPException(status_code=400, detail="ì…ë ¥ ìƒ˜í”Œì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        predict_params = { "run_mode": "serving", "return_intermediate": False }
        predictions_df = app_context.model.predict(input_df, params=predict_params)
        
        return BatchPredictionResponse(
            predictions=predictions_df.to_dict(orient="records"),
            model_uri=app_context.model_uri,
            sample_count=len(predictions_df)
        )
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# ğŸ†• Blueprint v17.0: ëª¨ë¸ ë©”íƒ€ë°ì´í„° ìê¸° ê¸°ìˆ  ì—”ë“œí¬ì¸íŠ¸ë“¤
    
@app.get("/model/metadata", response_model=ModelMetadataResponse, tags=["Model Metadata"])
def get_model_metadata() -> ModelMetadataResponse:
    """
    ëª¨ë¸ì˜ ì™„ì „í•œ ë©”íƒ€ë°ì´í„° ë°˜í™˜ (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”, Data Leakage ë°©ì§€ ì •ë³´ í¬í•¨)
    """
    try:
        if app_context.model is None:
            raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì •ë³´ êµ¬ì„±
        hpo_info = app_context.model.hyperparameter_optimization
        hyperparameter_optimization = HyperparameterOptimizationInfo(
            enabled=hpo_info.get("enabled", False),
            engine=hpo_info.get("engine", ""),
            best_params=hpo_info.get("best_params", {}),
            best_score=hpo_info.get("best_score", 0.0),
            total_trials=hpo_info.get("total_trials", 0),
            pruned_trials=hpo_info.get("pruned_trials", 0),
            optimization_time=str(hpo_info.get("optimization_time", "")),
        )
        
        # í•™ìŠµ ë°©ë²•ë¡  ì •ë³´ êµ¬ì„±
        tm_info = app_context.model.training_methodology
        training_methodology = TrainingMethodologyInfo(
            train_test_split_method=tm_info.get("train_test_split_method", ""),
            train_ratio=tm_info.get("train_ratio", 0.8),
            validation_strategy=tm_info.get("validation_strategy", ""),
            preprocessing_fit_scope=tm_info.get("preprocessing_fit_scope", ""),
            random_state=tm_info.get("random_state", 42),
        )
        
        # API ìŠ¤í‚¤ë§ˆ ì •ë³´ êµ¬ì„±
        api_schema = {
            "input_fields": [field for field in app_context.PredictionRequest.__fields__.keys()],
            "sql_source": "loader_sql_snapshot",
            "feature_columns": app_context.model.feature_columns, # ì‹¤ì œ ëª¨ë¸ì—ì„œ ê°€ì ¸ì˜´
            "join_key": app_context.model.join_key, # ì‹¤ì œ ëª¨ë¸ì—ì„œ ê°€ì ¸ì˜´
        }
        
        return ModelMetadataResponse(
            model_uri=app_context.model_uri,
            model_class_path=getattr(app_context.model, "model_class_path", ""),
            hyperparameter_optimization=hyperparameter_optimization,
            training_methodology=training_methodology,
            training_metadata=app_context.model.training_metadata,
            api_schema=api_schema,
        )
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/model/optimization", response_model=OptimizationHistoryResponse, tags=["Model Metadata"])
def get_optimization_history() -> OptimizationHistoryResponse:
    """
    í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê³¼ì •ì˜ ìƒì„¸ íˆìŠ¤í† ë¦¬ ë°˜í™˜
    """
    try:
        if app_context.model is None:
            raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        hpo_info = app_context.model.hyperparameter_optimization
        
        if not hpo_info.get("enabled", False):
            return OptimizationHistoryResponse(
                enabled=False,
                optimization_history=[],
                search_space={},
                convergence_info={"message": "í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤."},
                timeout_occurred=False,
            )
        
        return OptimizationHistoryResponse(
            enabled=True,
            optimization_history=hpo_info.get("optimization_history", []),
            search_space=hpo_info.get("search_space", {}),
            convergence_info={
                "best_score": hpo_info.get("best_score", 0.0),
                "total_trials": hpo_info.get("total_trials", 0),
                "pruned_trials": hpo_info.get("pruned_trials", 0),
                "optimization_time": str(hpo_info.get("optimization_time", "")),
            },
            timeout_occurred=hpo_info.get("timeout_occurred", False),
        )
        
    except Exception as e:
        logger.error(f"ìµœì í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/model/schema", tags=["Model Metadata"])
def get_api_schema() -> Dict[str, Any]:
    """
    ë™ì ìœ¼ë¡œ ìƒì„±ëœ API ìŠ¤í‚¤ë§ˆ ì •ë³´ ë°˜í™˜
    """
    try:
        if app_context.model is None:
            raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return {
            "prediction_request_schema": app_context.PredictionRequest.schema(),
            "batch_prediction_request_schema": app_context.BatchPredictionRequest.schema(),
            "loader_sql_snapshot": app_context.model.loader_sql_snapshot,
            "extracted_fields": [field for field in app_context.PredictionRequest.__fields__.keys()],
            "feature_store_info": {
                "feature_columns": app_context.model.feature_columns,
                "join_key": app_context.model.join_key,
                "feature_store_config": app_context.settings.serving.get('realtime_feature_store', {}),
            },
        }
        
    except Exception as e:
        logger.error(f"API ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

def run_api_server(settings: Settings, run_id: str, host: str = "0.0.0.0", port: int = 8000):
    """
    FastAPI ì„œë²„ë¥¼ ì‹¤í–‰í•˜ê³ , Lifespan ì´ë²¤íŠ¸ë¥¼ í†µí•´ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    # Blueprint ì›ì¹™ 9: í™˜ê²½ë³„ ê¸°ëŠ¥ ë¶„ë¦¬ - API ì„œë¹™ ì‹œìŠ¤í…œì  ì°¨ë‹¨
    serving_settings = getattr(settings, 'serving', None)
    if not serving_settings or not getattr(serving_settings, 'enabled', True):
        logger.error(f"'{settings.environment.app_env}' í™˜ê²½ì—ì„œëŠ” API ì„œë¹™ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    # ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
    model_uri = f"runs:/{run_id}/model"
    logger.info(f"MLflow ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_uri}")
    app_context.model = mlflow.pyfunc.load_model(model_uri)
    
    # ëª¨ë¸ì—ì„œ loader_sql_snapshot ì¶”ì¶œ
    try:
        # ë‹¤ì–‘í•œ ê²½ë¡œë¡œ loader_sql_snapshot ì°¾ê¸°
        loader_sql = None
        
        # ë°©ë²• 1: ì§ì ‘ ì†ì„± ì ‘ê·¼
        if hasattr(app_context.model, 'loader_sql_snapshot'):
            loader_sql = app_context.model.loader_sql_snapshot
            logger.info("Method 1: ì§ì ‘ loader_sql_snapshot ì†ì„± ë°œê²¬")
        
        # ë°©ë²• 2: _model_implì„ í†µí•œ ì ‘ê·¼
        elif hasattr(app_context.model, '_model_impl'):
            model_impl = app_context.model._model_impl
            if hasattr(model_impl, 'loader_sql_snapshot'):
                loader_sql = model_impl.loader_sql_snapshot
                logger.info("Method 2: _model_implì„ í†µí•´ loader_sql_snapshot ë°œê²¬")
        
        # ë°©ë²• 3: unwrap_python_modelì„ í†µí•œ ì ‘ê·¼
        elif hasattr(app_context.model, 'unwrap_python_model'):
            python_model = app_context.model.unwrap_python_model()
            if hasattr(python_model, 'loader_sql_snapshot'):
                loader_sql = python_model.loader_sql_snapshot
                logger.info("Method 3: unwrap_python_modelì„ í†µí•´ loader_sql_snapshot ë°œê²¬")
        
        # ë°©ë²• 4: ëª¨ë¸ ê°ì²´ì˜ ëª¨ë“  ì†ì„± íƒìƒ‰ (ë””ë²„ê¹…)
        else:
            logger.info("ëª¨ë¸ ê°ì²´ ì†ì„± íƒìƒ‰:")
            for attr in dir(app_context.model):
                if not attr.startswith('_'):
                    logger.info(f"  - {attr}: {type(getattr(app_context.model, attr, None))}")
            
            # ìµœí›„ ìˆ˜ë‹¨: ê¸°ë³¸ê°’ ì‚¬ìš©
            logger.warning("ëª¨ë“  ë°©ë²•ìœ¼ë¡œ loader_sql_snapshotì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ PK í•„ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            pk_fields = ['session_id', 'user_id', 'product_id']
            
        if loader_sql:
            # SQLì—ì„œ PK í•„ë“œ ì¶”ì¶œ
            from serving.schemas import get_pk_from_loader_sql
            pk_fields = get_pk_from_loader_sql(loader_sql)
            logger.info(f"SQLì—ì„œ ì¶”ì¶œëœ PK í•„ë“œ: {pk_fields}")
        else:
            # ê¸°ë³¸ê°’ ì‚¬ìš©
            pk_fields = ['session_id', 'user_id', 'product_id']
            logger.info(f"ê¸°ë³¸ PK í•„ë“œ ì‚¬ìš©: {pk_fields}")
            
    except Exception as e:
        logger.error(f"PK í•„ë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ê°’ ì‚¬ìš©
        pk_fields = ['session_id', 'user_id', 'product_id']
    
    # ë™ì  ë¼ìš°íŠ¸ ìƒì„±
    model_name = f"model_{run_id[:8]}"
    PredictionRequest = create_dynamic_prediction_request(model_name, pk_fields)
    
    # app_contextì— í•„ìš”í•œ ëª¨ë“  ê°’ ì„¤ì •
    app_context.settings = settings
    app_context.model_uri = model_uri
    app_context.PredictionRequest = PredictionRequest
    
    logger.info(f"API ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ì™„ë£Œ: ëª¨ë¸={model_uri}, PKí•„ë“œ={pk_fields}")
    
    @app.post("/predict", tags=["Predictions"])
    def predict(request: Dict[str, Any]):
        # Dict í˜•íƒœë¡œ ë°›ì•„ì„œ DataFrameìœ¼ë¡œ ë³€í™˜
        request_df = pd.DataFrame([request])
        
        # ëª¨ë¸ ì˜ˆì¸¡
        predictions = app_context.model.predict(request_df)
        
        # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì ì ˆí•œ í˜•íƒœë¡œ ë°˜í™˜
        if hasattr(predictions, 'iloc'):
            # DataFrameì¸ ê²½ìš°
            prediction_result = predictions.to_dict(orient="records")[0]
        else:
            # arrayì¸ ê²½ìš°
            prediction_result = {"prediction": predictions[0]}
            
        return {"predictions": [prediction_result], "model_uri": app_context.model_uri}

    uvicorn.run(app, host=host, port=port)
