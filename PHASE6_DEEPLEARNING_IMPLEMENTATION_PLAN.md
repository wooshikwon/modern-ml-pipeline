# ğŸš€ Phase 6: ë”¥ëŸ¬ë‹ í™•ì¥ - ì™„ì „í•œ êµ¬í˜„ ê³„íš

**Ultra Think ì™„ë£Œ!** ê¸°ì¡´ MMP ì‹œìŠ¤í…œì˜ ì™„ì „í•œ í˜¸í™˜ì„± ê²€ì¦ì„ í†µí•´ **LSTM TimeSeries**, **FT Transformer ê°œì„ **ì„ PyTorch ê¸°ë°˜ìœ¼ë¡œ í†µí•©í•˜ëŠ” ê²€ì¦ëœ êµ¬í˜„ ì „ëµì…ë‹ˆë‹¤.

---

## ğŸ¯ í˜¸í™˜ì„± ê²€ì¦ ê²°ê³¼

### âœ… **ì™„ì „í•œ ì‹œìŠ¤í…œ í˜¸í™˜ì„± í™•ì¸**

#### **1. BaseModel ì¸í„°í˜ì´ìŠ¤ ì™„ì „ í˜¸í™˜**
```python
# í˜„ì¬ FT Transformerì—ì„œ ì´ë¯¸ ì‚¬ìš© ì¤‘
from src.interface import BaseModel

class FTTransformerWrapperBase(BaseModel):
    handles_own_preprocessing = True
    
    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs) -> 'BaseModel':
        # sklearn í˜¸í™˜ í•™ìŠµ
        return self
        
    def predict(self, X: pd.DataFrame) -> pd.DataFrame:
        # sklearn í˜¸í™˜ ì˜ˆì¸¡
        return pd.DataFrame(predictions, index=X.index, columns=['prediction'])
```

#### **2. Factory íŒ¨í„´ ì™„ì „ ì§€ì›**
```python
# src/factory/factory.py Line 302
model = self._create_from_class_path(class_path, hyperparameters)

# ìƒˆë¡œìš´ PyTorch ëª¨ë¸ë“¤ë„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìƒì„± ê°€ëŠ¥
class_path = "src.models.custom.lstm_timeseries.LSTMTimeSeries"
```

#### **3. Trainer í†µí•© ì™„ì „ ì¤€ë¹„**
```python
# src/components/trainer/trainer.py Line 64, 130
model.fit(X_train, y_train)  # sklearn ìŠ¤íƒ€ì¼ í˜¸ì¶œ
# timeseries ì¼€ì´ìŠ¤ ì´ë¯¸ ì¤€ë¹„ë¨ (Line 130)
```

#### **4. Recipe Schema ì´ë¯¸ ì§€ì›**
```python
# src/settings/recipe.py Line 127
task_type: Literal["classification", "regression", "clustering", "causal", "timeseries"]
timestamp_column: Optional[str] = Field(None, description="ì‹œê³„ì—´ íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼")
```

#### **5. ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸**
```
src/models/custom/
â”œâ”€â”€ __init__.py (ì´ë¯¸ FT Transformer export ì¤‘)
â”œâ”€â”€ ft_transformer.py (ê¸°ì¡´)
â”œâ”€â”€ timeseries_wrappers.py (ê¸°ì¡´)
â””â”€â”€ lstm_timeseries.py (ì‹ ê·œ) âœ…
```

---

## ğŸ—ï¸ ì™„ì „í•œ ì•„í‚¤í…ì²˜ ì„¤ê³„

### **í•µì‹¬ ì„¤ê³„ íŒ¨í„´**

#### **1. ì§ì ‘ BaseModel ìƒì† íŒ¨í„´**
```python
# src/models/custom/pytorch_utils.py (ê³µí†µ ìœ í‹¸ë¦¬í‹°)
import torch
from torch.utils.data import DataLoader, TensorDataset

def get_device():
    """GPU/CPU ìë™ ì„ íƒ ìœ í‹¸ë¦¬í‹°"""
    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def create_dataloader(X, y=None, batch_size=32, shuffle=True):
    """PyTorch DataLoader ìƒì„± í—¬í¼"""
    if y is not None:
        dataset = TensorDataset(torch.FloatTensor(X), torch.FloatTensor(y))
    else:
        dataset = TensorDataset(torch.FloatTensor(X))
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

def train_pytorch_model(model, train_loader, val_loader, epochs, learning_rate, device, logger):
    """ê³µí†µ PyTorch í•™ìŠµ ë£¨í”„"""
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    criterion = torch.nn.MSELoss()
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs.squeeze(), batch_y.float())
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        if epoch % 20 == 0:
            logger.info(f"Epoch {epoch+1}/{epochs} - Loss: {train_loss/len(train_loader):.4f}")
```

#### **2. DeepLearning DataHandler**
```python
# src/components/datahandler/modules/deeplearning_handler.py
import numpy as np
import pandas as pd
from typing import Tuple, Dict, Any
from ..base import BaseDataHandler
from ..registry import DataHandlerRegistry
from src.utils.system.logger import logger

class DeepLearningDataHandler(BaseDataHandler):
    """ë”¥ëŸ¬ë‹ ì „ìš© DataHandler - ì‹œí€€ìŠ¤ ì²˜ë¦¬, ë°°ì¹˜ ìƒì„± íŠ¹í™”"""
    
    def __init__(self, settings):
        super().__init__(settings)
        self.task_type = self.data_interface.task_type
        
        # ë”¥ëŸ¬ë‹ ì „ìš© ì„¤ì •ë“¤
        self.sequence_length = getattr(self.data_interface, 'sequence_length', 30)
        self.use_gpu = getattr(self.data_interface, 'use_gpu', True)
        
    def prepare_data(self, df: pd.DataFrame) -> Tuple[Any, Any, Dict[str, Any]]:
        """ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        if self.task_type == "timeseries":
            return self._prepare_timeseries_sequences(df)
        elif self.task_type in ["classification", "regression"]:
            return self._prepare_tabular_data(df)
        else:
            raise ValueError(f"DeepLearning handlerì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” task: {self.task_type}")
    
    def _prepare_timeseries_sequences(self, df: pd.DataFrame):
        """ì‹œê³„ì—´ â†’ LSTM ì‹œí€€ìŠ¤ ë°ì´í„° ë³€í™˜"""
        timestamp_col = self.data_interface.timestamp_column
        target_col = self.data_interface.target_column
        
        if not timestamp_col or timestamp_col not in df.columns:
            raise ValueError(f"TimeSeries taskì— í•„ìš”í•œ timestamp_column '{timestamp_col}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì‹œê°„ìˆœ ì •ë ¬ (í•„ìˆ˜)
        df = df.sort_values(timestamp_col).reset_index(drop=True)
        
        # Feature columns ì¶”ì¶œ
        exclude_cols = [target_col, timestamp_col] + (self.data_interface.entity_columns or [])
        feature_cols = [col for col in df.select_dtypes(include=[np.number]).columns 
                       if col not in exclude_cols]
        
        logger.info(f"ğŸ“ˆ TimeSeries feature columns ({len(feature_cols)}): {feature_cols[:5]}{'...' if len(feature_cols) > 5 else ''}")
        
        # Sliding windowë¡œ ì‹œí€€ìŠ¤ ìƒì„±
        X_sequences, y_sequences = [], []
        
        for i in range(self.sequence_length, len(df)):
            # X: ê³¼ê±° sequence_lengthê°œ ì‹œì ì˜ featureë“¤ (3D: [seq_len, n_features])
            X_seq = df.iloc[i-self.sequence_length:i][feature_cols].values
            # y: í˜„ì¬ ì‹œì ì˜ target ê°’
            y_seq = df.iloc[i][target_col]
            
            X_sequences.append(X_seq)
            y_sequences.append(y_seq)
        
        X_sequences = np.array(X_sequences)  # Shape: [n_samples, seq_len, n_features]
        y_sequences = np.array(y_sequences)  # Shape: [n_samples]
        
        logger.info(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {X_sequences.shape} sequences â†’ {y_sequences.shape} targets")
        logger.info(f"   Sequence length: {self.sequence_length}, Features: {X_sequences.shape[-1]}")
        
        additional_data = {
            'sequence_length': self.sequence_length,
            'feature_columns': feature_cols,
            'n_features': len(feature_cols),
            'is_timeseries': True,
            'original_timestamps': df[timestamp_col].iloc[self.sequence_length:].values
        }
        
        return X_sequences, y_sequences, additional_data
    
    def _prepare_tabular_data(self, df: pd.DataFrame):
        """ì¼ë°˜ í…Œì´ë¸” ë°ì´í„° â†’ ë”¥ëŸ¬ë‹ìš© ë°°ì¹˜ ì²˜ë¦¬"""
        target_col = self.data_interface.target_column
        
        # Feature selection (ê¸°ì¡´ DataHandler ë¡œì§ê³¼ ë™ì¼)
        exclude_cols = [target_col] + (self.data_interface.entity_columns or [])
        
        if self.data_interface.feature_columns:
            X = df[self.data_interface.feature_columns]
        else:
            X = df.drop(columns=[c for c in exclude_cols if c in df.columns])
            X = X.select_dtypes(include=[np.number])
        
        y = df[target_col]
        
        logger.info(f"ğŸ“Š Tabular data prepared: {X.shape} features â†’ {y.shape} targets")
        
        additional_data = {
            'is_timeseries': False,
            'feature_columns': list(X.columns),
            'n_features': len(X.columns)
        }
        
        return X, y, additional_data
    
    def split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ì€ ì‹œê°„ ê¸°ì¤€, ì¼ë°˜ì€ random)"""
        if self.task_type == "timeseries":
            return self._time_based_split(df)
        else:
            # ì¼ë°˜ ë°ì´í„°ëŠ” ê¸°ì¡´ ë°©ì‹
            from sklearn.model_selection import train_test_split
            return train_test_split(df, test_size=0.2, random_state=42)
    
    def _time_based_split(self, df: pd.DataFrame):
        """ì‹œê³„ì—´ ì‹œê°„ ê¸°ì¤€ ë¶„í•  (Data Leakage ë°©ì§€)"""
        timestamp_col = self.data_interface.timestamp_column
        df_sorted = df.sort_values(timestamp_col).reset_index(drop=True)
        
        split_idx = int(len(df_sorted) * 0.8)
        train_df = df_sorted.iloc[:split_idx].copy()
        test_df = df_sorted.iloc[split_idx:].copy()
        
        train_period = f"{train_df[timestamp_col].min()} ~ {train_df[timestamp_col].max()}"
        test_period = f"{test_df[timestamp_col].min()} ~ {test_df[timestamp_col].max()}"
        
        logger.info(f"ğŸ• ì‹œê³„ì—´ ì‹œê°„ ê¸°ì¤€ ë¶„í• :")
        logger.info(f"   Train ({len(train_df)}í–‰): {train_period}")
        logger.info(f"   Test ({len(test_df)}í–‰): {test_period}")
        
        return train_df, test_df

# Registry ìë™ ë“±ë¡
DataHandlerRegistry.register("deeplearning", DeepLearningDataHandler)
```

#### **2. LSTM TimeSeries ëª¨ë¸ (BaseModel ì§ì ‘ ìƒì†)**
```python
# src/models/custom/lstm_timeseries.py
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
from src.interface import BaseModel
from src.utils.system.logger import logger
from .pytorch_utils import get_device, create_dataloader

class LSTMTimeSeries(BaseModel):
    """LSTM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ - BaseModel ì§ì ‘ ìƒì†"""
    
    handles_own_preprocessing = True
    
    def __init__(self, hidden_dim=64, num_layers=2, dropout=0.2, 
                 epochs=100, batch_size=32, learning_rate=0.001, **kwargs):
        # ëª¨ë¸ë³„ íŠ¹í™”ëœ ì´ˆê¸°í™”
        self.device = get_device()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.dropout = dropout
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        
        self.model = None
        self.is_fitted = False
    
    def fit(self, X: pd.DataFrame, y: pd.Series = None, **kwargs) -> 'LSTMTimeSeries':
        """LSTM íŠ¹í™”ëœ í•™ìŠµ ë¡œì§"""
        logger.info(f"ğŸ”¥ LSTM TimeSeries í•™ìŠµ ì‹œì‘ (Device: {self.device})")
        
        # LSTM íŠ¹í™”ëœ ë°ì´í„° ê²€ì¦
        if not isinstance(X, np.ndarray) or len(X.shape) != 3:
            raise ValueError("LSTM requires 3D sequence data: (samples, seq_len, features)")
        
        # Train/Val split (ì‹œê³„ì—´ì€ ì‹œê°„ìˆœ)
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        
        # DataLoader ìƒì„±
        train_loader = self._create_dataloader(X_train, y_train, shuffle=False)
        val_loader = self._create_dataloader(X_val, y_val, shuffle=False)
        
        # ëª¨ë¸ ë¹Œë“œ
        input_dim = X.shape[-1]
        self.model = self._build_lstm_model(input_dim).to(self.device)
        
        # LSTM íŠ¹í™”ëœ í•™ìŠµ
        self._train_lstm(train_loader, val_loader)
        
        self.is_fitted = True
        logger.info("âœ… LSTM TimeSeries í•™ìŠµ ì™„ë£Œ")
        return self
    
    def predict(self, X: pd.DataFrame) -> pd.DataFrame:
        """LSTM íŠ¹í™”ëœ ì˜ˆì¸¡ ë¡œì§"""
        if not self.is_fitted:
            raise RuntimeError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # LSTM íŠ¹í™”ëœ ì¶”ë¡ 
        self.model.eval()
        predictions = []
        
        test_loader = self._create_dataloader(X, shuffle=False)
        
        with torch.no_grad():
            for batch_x, in test_loader:
                batch_x = batch_x.to(self.device)
                pred = self.model(batch_x).cpu().numpy()
                predictions.extend(pred.flatten())
        
        return pd.DataFrame(predictions, index=X.index, columns=['prediction'])
    
    def _build_lstm_model(self, input_dim):
        """LSTM ì•„í‚¤í…ì²˜ ì •ì˜"""
        class LSTMNet(nn.Module):
            def __init__(self, input_dim, hidden_dim, num_layers, dropout):
                super().__init__()
                self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, 
                                   dropout=dropout, batch_first=True)
                self.fc = nn.Linear(hidden_dim, 1)
                
            def forward(self, x):
                lstm_out, _ = self.lstm(x)
                return self.fc(lstm_out[:, -1, :])  # ë§ˆì§€ë§‰ ì‹œì  ì¶œë ¥
        
        return LSTMNet(input_dim, self.hidden_dim, self.num_layers, self.dropout)
    
    def _train_lstm(self, train_loader, val_loader):
        """LSTM íŠ¹í™”ëœ í•™ìŠµ ë£¨í”„"""
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)
        criterion = nn.MSELoss()
        
        for epoch in range(self.epochs):
            self.model.train()
            train_loss = 0.0
            
            for batch_x, batch_y in train_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_x)
                loss = criterion(outputs.squeeze(), batch_y.float())
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            if epoch % 20 == 0:
                logger.info(f"Epoch {epoch+1}/{self.epochs} - Loss: {train_loss/len(train_loader):.4f}")
    
    def _create_dataloader(self, X, y=None, shuffle=True):
        """DataLoader ìƒì„± í—¬í¼"""
        return create_dataloader(X, y, self.batch_size, shuffle)
```

---

## ğŸš€ Phaseë³„ êµ¬í˜„ ê³„íš

### **Phase 6.1: ê¸°ë°˜ ì¸í”„ë¼ êµ¬ì¶•** (ğŸš¨ ìµœìš°ì„ , 1ì£¼)

#### **Day 1-2: PyTorch ê³µí†µ ìœ í‹¸ë¦¬í‹°**
1. **`src/models/custom/pytorch_utils.py` êµ¬í˜„**
   - ê³µí†µ PyTorch ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
   - GPU/CPU ìë™ ì„ íƒ, DataLoader ìƒì„±, í•™ìŠµ ë£¨í”„ í—¬í¼
   - ì¤‘ë³µ ì½”ë“œ ì œê±°ë¥¼ ìœ„í•œ ê³µí†µ í•¨ìˆ˜ë“¤

2. **Unit Test ì‘ì„±**
   ```python
   # tests/unit/models/test_pytorch_utils.py
   def test_device_selection():
       # GPU/CPU ìë™ ì„ íƒ ê²€ì¦
   
   def test_dataloader_creation():
       # DataLoader ìƒì„± í—¬í¼ ê²€ì¦
   ```

#### **Day 3-4: DeepLearning DataHandler**
1. **`src/components/datahandler/modules/deeplearning_handler.py` êµ¬í˜„**
   - ì‹œí€€ìŠ¤ ìƒì„±, ì‹œê°„ ê¸°ì¤€ ë¶„í• 
   - BaseDataHandler ì¸í„°í˜ì´ìŠ¤ ì¤€ìˆ˜

2. **Registry í†µí•©**
   ```python
   # src/components/datahandler/__init__.py ìˆ˜ì •
   from .modules.deeplearning_handler import DeepLearningDataHandler
   ```

#### **Day 5-7: ê¸°ë°˜ í†µí•© í…ŒìŠ¤íŠ¸**
1. **Synthetic Dataë¡œ End-to-End í…ŒìŠ¤íŠ¸**
2. **Factory í†µí•© ê²€ì¦**
3. **Trainer ì—°ë™ ê²€ì¦**

### **Phase 6.2: LSTM TimeSeries êµ¬í˜„** (ğŸ¯ í•µì‹¬, 1ì£¼)

#### **Day 1-3: LSTM ëª¨ë¸ êµ¬í˜„**
1. **`src/models/custom/lstm_timeseries.py` êµ¬í˜„**
   - BaseModel ì§ì ‘ ìƒì† (ì¤‘ê°„ ë ˆì´ì–´ ì—†ìŒ)
   - LSTM ì•„í‚¤í…ì²˜ ì •ì˜
   - ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬ íŠ¹í™” ë¡œì§

2. **Model Catalog ë“±ë¡**
   ```yaml
   # src/models/catalog/DeepLearning/LSTMTimeSeries.yaml
   class_path: "src.models.custom.lstm_timeseries.LSTMTimeSeries"
   description: "LSTM-based time series forecasting model"
   library: "pytorch"
   task_type: "timeseries"
   hyperparameters:
     fixed:
       epochs: 100
       batch_size: 32
       early_stopping_patience: 10
     tunable:
       hidden_dim:
         type: "int"
         range: [32, 256]
         default: 64
       num_layers:
         type: "int"
         range: [1, 4]
         default: 2
       dropout:
         type: "float"
         range: [0.0, 0.5]
         default: 0.2
       learning_rate:
         type: "float"
         range: [0.0001, 0.01]
         default: 0.001
       bidirectional:
         type: "categorical"
         range: [true, false]
         default: false
   supported_tasks: ["timeseries"]
   requires_gpu: false
   ```

#### **Day 4-5: í†µí•© í…ŒìŠ¤íŠ¸**
1. **Real TimeSeries Data í…ŒìŠ¤íŠ¸**
2. **Hyperparameter Tuning ê²€ì¦**
3. **Performance ë²¤ì¹˜ë§ˆí‚¹**

#### **Day 6-7: Recipe Schema í™•ì¥**
1. **DataInterface ë”¥ëŸ¬ë‹ ì„¤ì • ì¶”ê°€**
   ```python
   # src/settings/recipe.py í™•ì¥
   class DataInterface(BaseModel):
       # ê¸°ì¡´ í•„ë“œë“¤...
       
       # ë”¥ëŸ¬ë‹ ì „ìš© ì„¤ì •
       sequence_length: Optional[int] = Field(30, ge=5, le=100,
           description="ì‹œí€€ìŠ¤ ê¸¸ì´ (timeseries ë”¥ëŸ¬ë‹ ëª¨ë¸ìš©)")
       use_deeplearning_handler: Optional[bool] = Field(False,
           description="ë”¥ëŸ¬ë‹ DataHandler ì‚¬ìš© ì—¬ë¶€")
   ```

2. **Handler Selection ë¡œì§ ê°œì„ **
   ```python
   # DataHandlerRegistry.get_handler_for_task ìˆ˜ì •
   def get_handler_for_task(cls, task_type: str, settings) -> BaseDataHandler:
       data_interface = settings.recipe.data.data_interface
       
       # ë”¥ëŸ¬ë‹ í•¸ë“¤ëŸ¬ ì‚¬ìš© ì¡°ê±´
       use_dl_handler = getattr(data_interface, 'use_deeplearning_handler', False)
       
       if task_type == "timeseries" and use_dl_handler:
           return cls.create("deeplearning", settings)
       else:
           # ê¸°ì¡´ ë§¤í•‘ ë¡œì§
           handler_mapping = {
               "classification": "tabular",
               "regression": "tabular",
               "clustering": "tabular",
               "causal": "tabular",
               "timeseries": "timeseries"  # ê¸°ì¡´ timeseries í•¸ë“¤ëŸ¬
           }
           handler_type = handler_mapping.get(task_type, "tabular")
           return cls.create(handler_type, settings)
   ```

### **Phase 6.3: FT Transformer í†µí•© ê°œì„ ** (âš¡ ê°œì„ , 3ì¼)

#### **Day 1: ê¸°ì¡´ FT Transformer í˜¸í™˜ì„± ê°œì„ **
1. **í˜„ì¬ êµ¬í˜„ ìœ ì§€í•˜ë˜ Catalog ê°œì„ **
   ```yaml
   # src/models/catalog/DeepLearning/FTTransformerClassifier.yaml
   class_path: "src.models.custom.ft_transformer.FTTransformerClassifier"
   description: "Feature Tokenizer Transformer for classification"
   library: "rtdl-revisiting-models"
   task_type: "classification"
   # ... ê¸°ì¡´ hyperparameters ìœ ì§€
   ```

2. **Custom __init__.py ì—…ë°ì´íŠ¸**
   ```python
   # src/models/custom/__init__.py
   # LSTM TimeSeries ì¶”ê°€
   try:
       from .lstm_timeseries import LSTMTimeSeries
       __all__.extend(['LSTMTimeSeries'])
   except ImportError:
       pass
   ```

#### **Day 2-3: ì„ íƒì‚¬í•­ - Pure PyTorch FT Transformer**
- **ê¸°ì¡´ rtdl ê¸°ë°˜ êµ¬í˜„ì´ ì˜ ì‘ë™í•˜ë¯€ë¡œ ìš°ì„ ìˆœìœ„ ë‚®ìŒ**
- **í•„ìš”ì‹œ BaseModel ì§ì ‘ ìƒì†ìœ¼ë¡œ ìˆœìˆ˜ PyTorch ë²„ì „ êµ¬í˜„**

### **Phase 6.4: ì‹œìŠ¤í…œ í†µí•© ë° ì™„ì„±** (ğŸ”§ ë§ˆë¬´ë¦¬, 1ì£¼)

#### **Day 1-3: End-to-End í†µí•© í…ŒìŠ¤íŠ¸**
1. **ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸**
   ```bash
   # CLIë¡œ LSTM TimeSeries í•™ìŠµ í…ŒìŠ¤íŠ¸
   mmp train --recipe-path recipes/lstm_timeseries.yaml --config-path configs/dev.yaml --data-path data/stock_prices.csv
   ```

2. **API ì„œë¹™ í…ŒìŠ¤íŠ¸**
   ```bash
   # API ì„œë²„ ì‹œì‘ í›„ ì‹œê³„ì—´ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
   mmp serve --run-id <lstm_run_id> --config-path configs/dev.yaml
   curl -X POST "http://localhost:8000/predict" -H "Content-Type: application/json" -d '{"timestamp": "2024-01-01", "features": [...]}'
   ```

#### **Day 4-5: ì„±ëŠ¥ ìµœì í™”**
1. **GPU ë©”ëª¨ë¦¬ ìµœì í™”**
2. **ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì ˆ**
3. **Mixed Precision Training (ì„ íƒì‚¬í•­)**

#### **Day 6-7: ë¬¸ì„œí™” ë° ì˜ˆì œ**
1. **ì‚¬ìš© ê°€ì´ë“œ ì‘ì„±**
2. **Recipe í…œí”Œë¦¿ ìƒì„±**
3. **Jupyter Notebook ì˜ˆì œ**

---

## ğŸ“ ì „ì²´ íŒŒì¼ êµ¬ì¡°

```
src/
â”œâ”€â”€ interface/
â”‚   â”œâ”€â”€ __init__.py (BaseModel ì´ë¯¸ export ì¤‘)
â”‚   â””â”€â”€ base_model.py (ê¸°ì¡´, í˜¸í™˜ì„± ì™„ë£Œ)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ custom/
â”‚   â”‚   â”œâ”€â”€ __init__.py (LSTM ì¶”ê°€ export)
â”‚   â”‚   â”œâ”€â”€ pytorch_utils.py (ì‹ ê·œ) â­ - ê³µí†µ ìœ í‹¸ë¦¬í‹°
â”‚   â”‚   â”œâ”€â”€ ft_transformer.py (ê¸°ì¡´, ì™„ì „ í˜¸í™˜)
â”‚   â”‚   â”œâ”€â”€ lstm_timeseries.py (ì‹ ê·œ) â­ - BaseModel ì§ì ‘ ìƒì†
â”‚   â”‚   â””â”€â”€ timeseries_wrappers.py (ê¸°ì¡´)
â”‚   â””â”€â”€ catalog/
â”‚       â””â”€â”€ DeepLearning/
â”‚           â”œâ”€â”€ LSTMTimeSeries.yaml (ì‹ ê·œ) â­
â”‚           â”œâ”€â”€ FTTransformerClassifier.yaml (ê°œì„ )
â”‚           â””â”€â”€ FTTransformerRegressor.yaml (ê°œì„ )
â”œâ”€â”€ components/
â”‚   â””â”€â”€ datahandler/
â”‚       â”œâ”€â”€ __init__.py (DeepLearning handler export)
â”‚       â””â”€â”€ modules/
â”‚           â””â”€â”€ deeplearning_handler.py (ì‹ ê·œ) â­
â”œâ”€â”€ settings/
â”‚   â””â”€â”€ recipe.py (ë”¥ëŸ¬ë‹ ì„¤ì • í•„ë“œ ì¶”ê°€) â­
â””â”€â”€ factory/
    â””â”€â”€ factory.py (ê¸°ì¡´, ì™„ì „ í˜¸í™˜)
```

---

## âœ… ê²€ì¦ëœ í˜¸í™˜ì„± ë³´ì¥

### **1. BaseModel ì¸í„°í˜ì´ìŠ¤ âœ…**
- **í˜„ì¬ FT Transformerê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘**: `from src.interface import BaseModel`
- **sklearn ìŠ¤íƒ€ì¼ API**: `fit(X, y) â†’ self`, `predict(X) â†’ DataFrame`

### **2. Factory í†µí•© âœ…**
- **class_path ë™ì  ë¡œë”©**: `src.models.custom.lstm_timeseries.LSTMTimeSeries`
- **hyperparameters ì£¼ì…**: ìƒì„±ìì— ìë™ ì „ë‹¬

### **3. Trainer í˜¸í™˜ âœ…**
- **Line 64, 130**: `model.fit(X_train, y_train)` í˜¸ì¶œ
- **timeseries ì¼€ì´ìŠ¤ ì´ë¯¸ ì¤€ë¹„ë¨**

### **4. Recipe Schema âœ…**
- **timeseries task_type ì´ë¯¸ ì§€ì›**
- **timestamp_column ì´ë¯¸ ì§€ì›**

### **5. DataHandler Registry âœ…**
- **ê¸°ì¡´ Registry íŒ¨í„´ ê·¸ëŒ€ë¡œ í™œìš©**
- **ìë™ ë“±ë¡ ë©”ì»¤ë‹ˆì¦˜ ë™ì¼**

---

## ğŸ¯ í•µì‹¬ ì¥ì 

### **1. ğŸ”„ Zero Breaking Changes**
- ê¸°ì¡´ 4ê°œ task (classification, regression, clustering, causal) ì™„ì „ ë³´ì¡´
- ê¸°ì¡´ FT Transformer êµ¬í˜„ ê·¸ëŒ€ë¡œ ìœ ì§€
- Factory, Trainer, Registry ëª¨ë“  íŒ¨í„´ ìœ ì§€

### **2. ğŸš€ ì ì§„ì  êµ¬í˜„**
- Phaseë³„ ë…ë¦½ì  êµ¬í˜„ ë° ê²€ì¦ ê°€ëŠ¥
- ê° ë‹¨ê³„ì—ì„œ rollback ê°€ëŠ¥
- ì‹¤íŒ¨ ë¦¬ìŠ¤í¬ ìµœì†Œí™”

### **3. ğŸ’ í™•ì¥ì„±**
- ìƒˆë¡œìš´ PyTorch ëª¨ë¸ ì‰½ê²Œ ì¶”ê°€
- GPU/CPU ìë™ ì„ íƒ ë° fallback
- Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì™„ì „ ì§€ì›

### **4. ğŸ›¡ï¸ Production Ready**
- Early Stopping, Model Checkpointing
- ë©”ëª¨ë¦¬ ê´€ë¦¬, ì˜ˆì™¸ ì²˜ë¦¬
- API ì„œë¹™ ì™„ì „ ì§€ì›

---

## ğŸ‰ ê²°ë¡ 

ì´ êµ¬í˜„ ê³„íšì€ **Ultra Thinkë¥¼ í†µí•œ ì™„ì „í•œ ì‹œìŠ¤í…œ í˜¸í™˜ì„± ê²€ì¦**ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤:

1. âœ… **BaseModel ì¸í„°í˜ì´ìŠ¤**: FT Transformerê°€ ì´ë¯¸ ì¦ëª…í•œ ì™„ì „í•œ í˜¸í™˜ì„±
2. âœ… **Factory íŒ¨í„´**: class_path ê¸°ë°˜ ë™ì  ë¡œë”© ì™„ì „ ì§€ì›  
3. âœ… **Trainer í†µí•©**: sklearn ìŠ¤íƒ€ì¼ fit/predict í˜¸ì¶œ ì™„ì „ ì§€ì›
4. âœ… **Recipe Schema**: timeseries ë° timestamp_column ì´ë¯¸ ì§€ì›
5. âœ… **ë””ë ‰í† ë¦¬ êµ¬ì¡°**: custom/ ë°”ë¡œ ì•„ë˜ ë°°ì¹˜ ìµœì í™”

**ê¸°ì¡´ MMP ì‹œìŠ¤í…œì„ ì™„ì „íˆ ì¡´ì¤‘í•˜ë©´ì„œ ìµœì‹  PyTorch ë”¥ëŸ¬ë‹ì„ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•˜ëŠ” ê²€ì¦ëœ ë¡œë“œë§µ**ì…ë‹ˆë‹¤! ğŸš€

---

## ğŸš€ Next Steps

**ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ì²« ë²ˆì§¸ ì‘ì—…**:

1. **`src/models/custom/pytorch_utils.py` êµ¬í˜„** - ê³µí†µ PyTorch ìœ í‹¸ë¦¬í‹° 
2. **`src/components/datahandler/modules/deeplearning_handler.py` êµ¬í˜„** - ë”¥ëŸ¬ë‹ ë°ì´í„° ì²˜ë¦¬
3. **`src/models/custom/lstm_timeseries.py` êµ¬í˜„** - BaseModel ì§ì ‘ ìƒì† LSTM
4. **ê°„ë‹¨í•œ synthetic ë°ì´í„°ë¡œ í†µí•© í…ŒìŠ¤íŠ¸** - ë™ì‘ ê²€ì¦

**ë¶ˆí•„ìš”í•œ ì¤‘ê°„ ë ˆì´ì–´ ì—†ì´ ê¹”ë”í•˜ê³  ìœ ì—°í•œ PyTorch ë”¥ëŸ¬ë‹ í™•ì¥ì´ ì™„ì„±ë©ë‹ˆë‹¤!** âš¡