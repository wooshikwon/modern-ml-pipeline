# Practical Tests Structure Refactoring Strategy

## Executive Summary

**í˜„ì¬ ìƒíƒœ**: 62/62 integration tests 100% ì„±ê³µ  
**í•µì‹¬ ë¬¸ì œ**: í…ŒìŠ¤íŠ¸ ì½”ë“œì˜ 70-80%ê°€ setup, 20-30%ë§Œ ì‹¤ì œ ê²€ì¦  
**í•´ê²° ë°©ì•ˆ**: **Context-based Test Architecture** - ì±…ì„ ë¶„ë¦¬ì™€ setup ì¤‘ì•™í™”  
**ë§ˆì´ê·¸ë ˆì´ì…˜**: **Zero-risk incremental migration** - 100% ì„±ê³µë¥  ë³´ì¥

---

## âš¡ Quickstart: ìƒˆ Context ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì‘ì„±

1. ì»¨í…ìŠ¤íŠ¸ í”½ìŠ¤ì²˜ ì‚¬ìš©: `mlflow_test_context` ë˜ëŠ” ëª©ì ì— ë§ëŠ” ì»¨í…ìŠ¤íŠ¸ í”½ìŠ¤ì²˜ë¥¼ í…ŒìŠ¤íŠ¸ ì‹œê·¸ë‹ˆì²˜ì— ì¶”ê°€
2. ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ì„¤ì •/ë°ì´í„°/ë¦¬ì†ŒìŠ¤ ìë™ ì¤€ë¹„: `with ... as ctx:`
3. íŒŒì´í”„ë¼ì¸/í¼ë¸”ë¦­ API í˜¸ì¶œ í›„, ì»¨í…ìŠ¤íŠ¸ì˜ ê²€ì¦ í—¬í¼ë¡œ ê´€ì°°/ê²€ì¦ë§Œ ìˆ˜í–‰

```python
def test_quickstart_mlflow(mlflow_test_context):
    with mlflow_test_context.for_classification(experiment="quickstart") as ctx:
        result = run_train_pipeline(ctx.settings)
        assert result is not None
        assert ctx.experiment_exists()
        assert ctx.get_experiment_run_count() == 1
        assert isinstance(ctx.get_run_metrics(), dict)
```

## ğŸ”¤ Naming Conventions

- **Context íŒŒì¼**: `tests/fixtures/contexts/<area>_context.py` (ì˜ˆ: `mlflow_context.py`)
- **Context í´ë˜ìŠ¤**: `*TestContext`, ë‚´ë¶€ ë§¤ë‹ˆì €ëŠ” `*ContextManager`
- **í”½ìŠ¤ì²˜ ì´ë¦„**: `<area>_test_context` (ì˜ˆ: `mlflow_test_context`)
- **í…ŒìŠ¤íŠ¸ í•¨ìˆ˜**: `test_<area>_<behavior>[_v2]` (A/B ê³µì¡´ ì‹œ `_v2` ì ‘ë¯¸ì‚¬)
- **í…œí”Œë¦¿ íŒŒì¼**: `tests/fixtures/templates/configs/<task>_base.yaml`

## ğŸš« Testing Anti-Patterns

- **ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì¬êµ¬í˜„**: ì»¨í…ìŠ¤íŠ¸ ë‚´ë¶€ì—ì„œ íŒŒì´í”„ë¼ì¸/íŒ©í† ë¦¬ ë¡œì§ì„ ì¬í˜„í•˜ì§€ ì•ŠëŠ”ë‹¤(í¼ë¸”ë¦­ APIë§Œ í˜¸ì¶œ).
- **ìƒíƒœ ê³µìœ **: í…ŒìŠ¤íŠ¸ ê°„ `Factory`/`Settings`/ë¦¬ì†ŒìŠ¤ë¥¼ ê³µìœ í•˜ì§€ ì•ŠëŠ”ë‹¤(í…ŒìŠ¤íŠ¸ë‹¹ ìƒˆ ìƒì„±).
- **ì‹œê°„ ê¸°ë°˜ ì´ë¦„**: `time.time()` ë“± ì‹œê°„ ì˜ì¡´ì  ëª…ëª… ê¸ˆì§€ â†’ uuid ê¸°ë°˜ ì‚¬ìš©.
- **ìˆ¨ì€ ì „ì—­ ë³€ê²½**: ê¸€ë¡œë²Œ í™˜ê²½ ë³€ìˆ˜/ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½ì€ ì»¨í…ìŠ¤íŠ¸ ë²”ìœ„ ë‚´ì—ì„œë§Œ í•˜ê³  ìë™ ë³µì›.
- **ë¶ˆí•„ìš”í•œ ëª¨í‚¹**: ë ˆì§€ìŠ¤íŠ¸ë¦¬/íŒ©í† ë¦¬ ê²½ë¡œë¥¼ ëª¨í‚¹í•˜ì§€ ë§ê³ , ì‹¤ì œ ê²½ëŸ‰ ì»´í¬ë„ŒíŠ¸ë¡œ ê²€ì¦.

## ğŸ§° Operational Considerations

- **ë°ì´í„° ìˆ˜ëª…ì£¼ê¸°**: ëª¨ë“  ì„ì‹œ íŒŒì¼ì€ í…ŒìŠ¤íŠ¸ ì „ìš© temp ë””ë ‰í† ë¦¬ ì•ˆì—ì„œ ìƒì„±/ì‚­ì œ.
- **MLflow ì €ì¥ì†Œ**: `file://{temp_dir}/mlruns` ê³ ì • ì‚¬ìš©(ì™¸ë¶€ ê²½ë¡œ ë¶ˆê°€).
- **ê²©ë¦¬**: í…ŒìŠ¤íŠ¸ë‹¹ 1 run ê¸°ì¤€, êµì°¨ í…ŒìŠ¤íŠ¸ ì˜ì¡´ ê¸ˆì§€.
- **ì„±ëŠ¥ ì˜ˆì‚°**: ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”/íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì— ëŒ€í•´ ìƒí•œì„  ì„¤ì • ë° ì¸¡ì •(`performance_benchmark`).

---

## ğŸ¯ Core Problems: í˜„ì¬ êµ¬ì¡°ì˜ ì§„ì§œ ë¬¸ì œì 

### Problem 1: Setup Overhead Dominance

**í˜„ì¬ í…ŒìŠ¤íŠ¸ êµ¬ì¡° ë¶„ì„**:
```python
def test_mlflow_experiment_creation_and_tracking(self, isolated_temp_directory, settings_builder):
    # â”â”â” SETUP CODE (23 lines, 80% of test) â”â”â”
    mlflow_uri = f"sqlite:///{isolated_temp_directory}/test_mlflow.db"
    experiment_name = f"integration_test_{int(time.time())}"
    
    test_data = pd.DataFrame({
        'feature1': np.random.rand(50),
        'feature2': np.random.rand(50),
        'target': np.random.randint(0, 2, 50)
    })
    data_path = isolated_temp_directory / "mlflow_test_data.csv"
    test_data.to_csv(data_path, index=False)
    
    settings = settings_builder \
        .with_task("classification") \
        .with_model("sklearn.ensemble.RandomForestClassifier") \
        .with_data_path(str(data_path)) \
        .with_mlflow(tracking_uri=mlflow_uri, experiment_name=experiment_name) \
        .build()
    
    # â”â”â” ACTUAL TEST LOGIC (3 lines, 20% of test) â”â”â”
    try:
        result = run_train_pipeline(settings)
        assert result is not None
    except Exception as e:
        assert True  # No Mock Hell validation
```

**ë¬¸ì œ**: í…ŒìŠ¤íŠ¸ì˜ ì§„ì§œ ëª©ì ì¸ "MLflow experiment creation validation"ì´ ì½”ë“œì˜ 20%ë§Œ ì°¨ì§€

### Problem 2: Responsibility Mixing

**í•œ í…ŒìŠ¤íŠ¸ê°€ ë‹´ë‹¹í•˜ëŠ” 4ê°€ì§€ ì±…ì„**:
1. **Data Generation**: Test data ìƒì„± ë° íŒŒì¼ ì €ì¥
2. **Resource Setup**: MLflow/Database URI ìƒì„±  
3. **Settings Configuration**: Settings ê°ì²´ êµ¬ì„±
4. **Business Logic Validation**: ì‹¤ì œ í…ŒìŠ¤íŠ¸í•˜ë ¤ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (ì§„ì§œ ëª©ì )

**ê²°ê³¼**: í…ŒìŠ¤íŠ¸ ì˜ë„ ë¶ˆë¶„ëª…, ì½”ë“œ ì¤‘ë³µ, ìœ ì§€ë³´ìˆ˜ ì–´ë ¤ì›€

### Problem 3: Template Duplication

**62ê°œ í…ŒìŠ¤íŠ¸ì—ì„œ ë°˜ë³µë˜ëŠ” íŒ¨í„´ë“¤**:

**Pattern A: MLflow Setup (11íšŒ ë°˜ë³µ)**
```python
mlflow_uri = f"sqlite:///{isolated_temp_directory}/xxx.db"
experiment_name = f"yyy_{int(time.time())}"
```

**Pattern B: YAML Configuration (15íšŒ ë°˜ë³µ)**  
```python
config_yaml = """
environment:
  name: integration_test
data_source:
  adapter_type: storage
mlflow:
  tracking_uri: sqlite:///zzz.db
"""
```

**Pattern C: Component Factory Setup (30íšŒ ë°˜ë³µ)**
```python
factory = Factory(settings)
adapter = factory.create_data_adapter()
model = factory.create_model()
```

---

## ğŸ—ï¸ Proposed Architecture: Context-based Test Structure

### Core Design Principles

1. **Separation of Concerns**: Setup vs Verification ì±…ì„ ë¶„ë¦¬
2. **Context Management**: ê° í…ŒìŠ¤íŠ¸ ì˜ì—­ë³„ ì „ìš© Context í´ë˜ìŠ¤
3. **Zero Configuration**: ìµœì†Œí•œì˜ ì„¤ì •ìœ¼ë¡œ ìµœëŒ€í•œì˜ setup
4. **Incremental Migration**: ê¸°ì¡´ í…ŒìŠ¤íŠ¸ì™€ ìƒˆ êµ¬ì¡° ê³µì¡´

#### Context Minimal Contract

- **ì—­í•  ìµœì†Œí™”**: ì»¨í…ìŠ¤íŠ¸ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ "í˜¸ì¶œ"í•˜ê³  ê²°ê³¼ë¥¼ "ê´€ì°°"ë§Œ í•œë‹¤.
- **í¼ë¸”ë¦­ APIë§Œ ì‚¬ìš©**: `run_train_pipeline`, `Factory.create_*` ë“± ê³µê°œ APIë§Œ í˜¸ì¶œí•˜ê³  ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì¬êµ¬í˜„ì€ ê¸ˆì§€í•œë‹¤.
- **í•„ìˆ˜ ì†ì„±**: `ctx.settings`, `ctx.data_path`, `ctx.tracking_uri`, `ctx.experiment_name`
- **í•„ìˆ˜ í—¬í¼**: `experiment_exists()`, `get_experiment_run_count()`, `get_run_metrics()`
- **ì„ íƒ í—¬í¼(MLflow í™•ì¥)**: `verify_mlflow_artifacts()`
- **ìƒíƒœ ê²©ë¦¬**: í…ŒìŠ¤íŠ¸ë§ˆë‹¤ ìƒˆ `Settings`/ìƒˆ `Factory` ìƒì„±(ì»´í¬ë„ŒíŠ¸ ìºì‹œ/ìƒíƒœ ëˆ„ìˆ˜ ë°©ì§€)

### Architecture Overview

```
tests/
â”œâ”€â”€ conftest.py                     # ê¸°ì¡´ + ìƒˆë¡œìš´ context fixtures
â”œâ”€â”€ fixtures/
â”‚   â”œâ”€â”€ contexts/                   # ğŸ†• Context Classes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mlflow_context.py      # MLflow Test Context
â”‚   â”‚   â”œâ”€â”€ database_context.py    # Database Test Context
â”‚   â”‚   â”œâ”€â”€ component_context.py   # Component Factory Context
â”‚   â”‚   â””â”€â”€ scenario_context.py    # End-to-end Scenario Context
â”‚   â”œâ”€â”€ templates/                  # ğŸ†• YAML Templates
â”‚   â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”‚   â”œâ”€â”€ classification_base.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ regression_base.yaml
â”‚   â”‚   â”‚   â””â”€â”€ mlflow_base.yaml
â”‚   â”‚   â””â”€â”€ scenarios/
â”‚   â”‚       â”œâ”€â”€ classification_full.yaml
â”‚   â”‚       â””â”€â”€ regression_full.yaml
â”‚   â”œâ”€â”€ expected/                   # ğŸ†• Enhanced Expected Outputs
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”‚   â”œâ”€â”€ classification_baseline.json
â”‚   â”‚   â”‚   â””â”€â”€ regression_baseline.json
â”‚   â”‚   â”œâ”€â”€ predictions/
â”‚   â”‚   â”‚   â”œâ”€â”€ sample_classification.csv
â”‚   â”‚   â”‚   â””â”€â”€ sample_regression.csv
â”‚   â”‚   â””â”€â”€ responses/
â”‚   â”‚       â”œâ”€â”€ mlflow_tracking.json
â”‚   â”‚       â””â”€â”€ component_interactions.json
â”‚   â””â”€â”€ [ê¸°ì¡´ ë””ë ‰í† ë¦¬ë“¤ ìœ ì§€]
â””â”€â”€ integration/                    # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ + ìƒˆ ë²„ì „ ê³µì¡´
```

### Standardization for Tests

- **MLflow tracking_uri**: `file://{temp_dir}/mlruns` (í…ŒìŠ¤íŠ¸ ì „ìš©, ë¡œì»¬ ê²½ë¡œ ê³ ì •)
- **Experiment name**: `{prefix}-{uuid4().hex[:8]}` (ì‹œê°„ ì˜ì¡´ ì œê±°, ì¶©ëŒ ë°©ì§€)
- **ë°ì´í„° ìƒì„± ì‹œë“œ**: ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê³ ì • ì‹œë“œ ì‚¬ìš©(`seed=42` ê¸°ë³¸)
- **ìƒíƒœ ê²©ë¦¬ ì›ì¹™**: ê° í…ŒìŠ¤íŠ¸ì—ì„œ `Settings`/`Factory`ë¥¼ ìƒˆë¡œ ìƒì„±

---

## ğŸ”§ Implementation Examples

### Example 1: MLflow Test Context

**Before (í˜„ì¬ ë°©ì‹)**:
```python
def test_mlflow_experiment_creation(self, isolated_temp_directory, settings_builder):
    # 23 lines of setup code
    mlflow_uri = f"sqlite:///{isolated_temp_directory}/test_mlflow.db"
    experiment_name = f"integration_test_{int(time.time())}"
    test_data = pd.DataFrame({...})
    data_path = isolated_temp_directory / "test.csv" 
    test_data.to_csv(data_path, index=False)
    settings = settings_builder.with_task("classification")...build()
    
    # 3 lines of actual test
    try:
        result = run_train_pipeline(settings)
        assert result is not None
    except Exception:
        assert True
```

**After (Context ë°©ì‹)**:
```python
def test_mlflow_experiment_creation(self, mlflow_test_context):
    with mlflow_test_context.for_classification(experiment="experiment_creation") as ctx:
        result = run_train_pipeline(ctx.settings)
        assert result is not None
        assert ctx.experiment_exists()
        assert ctx.get_experiment_run_count() == 1
        metrics = ctx.get_run_metrics()
        assert isinstance(metrics, dict) and len(metrics) > 0
```

**MLflowTestContext êµ¬í˜„**:
```python
from uuid import uuid4
import pandas as pd
from mlflow.tracking import MlflowClient

class MLflowTestContext:
    def __init__(self, isolated_temp_directory, settings_builder, test_data_generator, seed: int = 42):
        self.temp_dir = isolated_temp_directory
        self.settings_builder = settings_builder
        self.data_generator = test_data_generator
        self.seed = seed
        
    def for_classification(self, experiment: str, model: str = "RandomForestClassifier"):
        return MLflowContextManager(
            task="classification",
            experiment_suffix=experiment,
            model_class=f"sklearn.ensemble.{model}",
            context=self
        )

class MLflowContextManager:
    def __enter__(self):
        # 1) MLflow URI í‘œì¤€í™”
        self.mlflow_uri = f"file://{self.context.temp_dir}/mlruns"
        # 2) ì‹¤í—˜ëª…ì€ uuid ê¸°ë°˜
        self.experiment_name = f"{self.experiment_suffix}-{uuid4().hex[:8]}"
        
        # 3) ê²°ì •ë¡ ì  ë°ì´í„° ìƒì„±
        X, y = self.context.data_generator.classification_data(n_samples=50, n_features=4, random_state=self.context.seed)
        self.test_data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(4)])
        self.test_data['target'] = y
        
        self.data_path = self.context.temp_dir / f"data_{self.experiment_suffix}.csv"
        self.test_data.to_csv(self.data_path, index=False)
        
        # 4) Settings ìë™ êµ¬ì„±
        self.settings = self.context.settings_builder \
            .with_task(self.task) \
            .with_model(self.model_class) \
            .with_data_path(str(self.data_path)) \
            .with_mlflow(self.mlflow_uri, self.experiment_name) \
            .build()
        
        # 5) MLflow client ì¤€ë¹„ ë° experiment id í™•ë³´
        self.mlflow_client = MlflowClient(tracking_uri=self.mlflow_uri)
        exp = self.mlflow_client.get_experiment_by_name(self.experiment_name)
        if exp is None:
            self.experiment_id = self.mlflow_client.create_experiment(self.experiment_name)
        else:
            self.experiment_id = exp.experiment_id
        
        return self
        
    def experiment_exists(self):
        return self.mlflow_client.get_experiment_by_name(self.experiment_name) is not None
    
    def get_experiment_run_count(self):
        runs = self.mlflow_client.list_run_infos(self.experiment_id)
        return len(runs)
    
    def get_run_metrics(self):
        runs = self.mlflow_client.search_runs([self.experiment_id], max_results=1, order_by=["attributes.start_time DESC"])
        if not runs:
            return {}
        return runs[0].data.metrics
```

### Example 2: Component Test Context

**Before**:
```python
def test_adapter_to_model_data_flow(self, isolated_temp_directory, settings_builder, test_data_generator):
    # 15 lines of component setup
    X, y = test_data_generator.classification_data(50, 4)
    test_data = pd.DataFrame(X, columns=[...])
    data_path = isolated_temp_directory / "dataflow_test.csv"
    test_data.to_csv(data_path, index=False)
    settings = settings_builder.with_task("classification")...build()
    factory = Factory(settings)
    adapter = factory.create_data_adapter()
    model = factory.create_model()
    
    # 5 lines of actual test
    raw_data = adapter.read(str(data_path))
    X_train = raw_data[feature_columns]
    y_train = raw_data[target_column]
    assert len(X_train) > 0
    assert len(y_train) > 0
```

**After**:
```python
def test_adapter_to_model_data_flow(self, component_test_context):
    with component_test_context.classification_stack() as ctx:
        # 8 lines of focused data flow verification
        raw_data = ctx.adapter.read(ctx.data_path)
        processed_data = ctx.prepare_model_input(raw_data)
        
        assert ctx.validate_data_flow(raw_data, processed_data)
        assert processed_data.shape[0] == 50
        assert processed_data.shape[1] == 4
        assert ctx.adapter.is_compatible_with(ctx.model)
        assert ctx.model.can_accept(processed_data)
```

### Example 3: Scenario-based Testing

**Before**: ë³µì¡í•œ end-to-end í…ŒìŠ¤íŠ¸ì—ì„œ 50+ lines setup

**After**:
```python
def test_full_classification_pipeline(self, classification_scenario):
    scenario = classification_scenario  # All setup done
    
    # 10 lines of pure business logic testing
    pipeline_result = scenario.run_full_pipeline()
    
    assert pipeline_result.model_trained
    assert pipeline_result.evaluation_completed  
    assert pipeline_result.mlflow_logged
    assert scenario.verify_model_quality(min_accuracy=0.7)
    assert scenario.verify_mlflow_artifacts()
    assert scenario.verify_evaluation_metrics()
```

---

## ğŸš€ Migration Strategy: Zero-Risk Incremental Approach

### Phase 1: Foundation Setup (Week 1)

**ìƒˆë¡œìš´ êµ¬ì¡° ì¶”ê°€ (ê¸°ì¡´ ì½”ë“œ ë¬´ë³€ê²½)**:

1. **Context Classes ìƒì„±**:
```bash
mkdir -p tests/fixtures/contexts
mkdir -p tests/fixtures/templates
```

2. **conftest.py í™•ì¥**:
```python
# conftest.pyì— ì¶”ê°€ (ê¸°ì¡´ fixtureë“¤ê³¼ í•¨ê»˜ ê³µì¡´)
@pytest.fixture
def mlflow_test_context(isolated_temp_directory, settings_builder, test_data_generator):
    return MLflowTestContext(isolated_temp_directory, settings_builder, test_data_generator)

@pytest.fixture  
def component_test_context(isolated_temp_directory, settings_builder, test_data_generator):
    return ComponentTestContext(isolated_temp_directory, settings_builder, test_data_generator)

@pytest.fixture
def classification_scenario(isolated_temp_directory, settings_builder, test_data_generator):
    return ClassificationScenario(isolated_temp_directory, settings_builder, test_data_generator)
```

3. **Template YAML íŒŒì¼ë“¤ ìƒì„±**:
```yaml
# tests/fixtures/templates/configs/classification_base.yaml
environment:
  name: "test_env"
data_source:
  name: "test_storage" 
  adapter_type: "storage"
mlflow:
  tracking_uri: "{{mlflow_uri}}"
  experiment_name: "{{experiment_name}}"
```

4. **ê°€ì´ë“œ/ì„±ëŠ¥ ê³„ì¸¡ ì¶”ê°€**:
- `tests/fixtures/contexts/README.md`ì— ì»¨í…ìŠ¤íŠ¸ ìµœì†Œ ê·œì•½Â·ê¸ˆì§€ì‚¬í•­(ì—”ì§„ ì¬êµ¬í˜„ ê¸ˆì§€) ëª…ì‹œ
- ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì‹œê°„ `performance_benchmark`ë¡œ ê³„ì¸¡(ê¶Œì¥ ì„ê³„: 120ms)

### Phase 2: Pilot Testing (Week 2-3)

**A/B í…ŒìŠ¤íŒ… ë°©ì‹**:

```python
# ê¸°ì¡´ í…ŒìŠ¤íŠ¸ (ë³€ê²½ ì—†ìŒ)
def test_mlflow_experiment_creation_and_tracking(self, isolated_temp_directory, settings_builder):
    # ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€
    
# ìƒˆë¡œìš´ ë°©ì‹ (ê°™ì€ íŒŒì¼ì— ì¶”ê°€)  
def test_mlflow_experiment_creation_and_tracking_v2(self, mlflow_test_context):
    # ìƒˆë¡œìš´ Context ë°©ì‹
    
# ê²°ê³¼ ë¹„êµ í…ŒìŠ¤íŠ¸
def test_compare_old_vs_new_approach(self, isolated_temp_directory, settings_builder, mlflow_test_context):
    # ë‘ ë°©ì‹ì˜ ê²°ê³¼ê°€ ë™ì¼í•œì§€ ê²€ì¦
```

**ê²€ì¦ ê¸°ì¤€**:
- âœ… ìƒˆ ë°©ì‹ê³¼ ê¸°ì¡´ ë°©ì‹ ê²°ê³¼ 100% ì¼ì¹˜
- âœ… ìƒˆ ë°©ì‹ì´ ë” ì§§ê³  ëª…í™•í•œ ì½”ë“œ
- âœ… ìƒˆ ë°©ì‹ì´ ë” ë§ì€ ê²€ì¦ ë¡œì§ í¬í•¨ ê°€ëŠ¥
- âœ… ì„±ëŠ¥ íšŒê·€ ì—†ìŒ(ì»¨í…ìŠ¤íŠ¸ init/íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œê°„ ìƒí•œ ë§Œì¡±)

### Phase 3: Category-wise Migration (Week 4-6)

**ë‹¨ê³„ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜**:

1. **MLflow Tests** (Week 4):
   - `test_mlflow_integration.py` ë‚´ 11ê°œ í…ŒìŠ¤íŠ¸
   - í•œ ë²ˆì— í•˜ë‚˜ì”© ìƒˆ ë°©ì‹ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
   - ê° í…ŒìŠ¤íŠ¸ë§ˆë‹¤ ê²°ê³¼ ì¼ì¹˜ í™•ì¸

2. **Component Interaction Tests** (Week 5):
   - `test_component_interactions.py` ë‚´ 10ê°œ í…ŒìŠ¤íŠ¸
   - Context-based component testing ì ìš©

3. **Database Integration Tests** (Week 6):
   - `test_database_integration.py` ë‚´ 9ê°œ í…ŒìŠ¤íŠ¸
   - DatabaseTestContext ì ìš©

### Phase 4: Validation & Cleanup (Week 7-8)

**ìµœì¢… ê²€ì¦**:
```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰í•˜ì—¬ ì„±ê³µë¥  í™•ì¸
pytest tests/integration/ -v
# Expected: 62/62 tests passed (100%)
```

**Cleanup ê¸°ì¤€**:
- ìƒˆ ë°©ì‹ê³¼ ê¸°ì¡´ ë°©ì‹ ê²°ê³¼ê°€ **100% ì¼ì¹˜** í™•ì¸ëœ ê²½ìš°ì—ë§Œ ê¸°ì¡´ ì½”ë“œ ì œê±°
- ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê²½ìš° ë‘ ë°©ì‹ ê³µì¡´ ìœ ì§€
- ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” setup ì½”ë“œë§Œ ì •ë¦¬

**Rollback Plan**:
- ì„ê³„ ì¹´í…Œê³ ë¦¬ì—ì„œ ì‹¤íŒ¨/íšŒê·€ ë°œìƒ ì‹œ, í•´ë‹¹ íŒŒì¼ì˜ v2 í…ŒìŠ¤íŠ¸ë¥¼ ì¼ì‹œ ë¹„í™œì„±í™”í•˜ê³  ê¸°ì¡´(v1)ë§Œ ìœ ì§€
- ì»¨í…ìŠ¤íŠ¸/í…œí”Œë¦¿ ë³€ê²½ì€ PR ë‹¨ìœ„ë¡œ ê²©ë¦¬í•˜ì—¬ ë¹ ë¥¸ revert ê°€ëŠ¥í•˜ë„ë¡ ìœ ì§€
- ì‹¤íŒ¨ ìœ í˜•ì„ ë¦¬ê·¸ë ˆì…˜ ë…¸íŠ¸ë¡œ ê¸°ë¡í•˜ê³ , ì»¨í…ìŠ¤íŠ¸ ìµœì†Œ ê·œì•½ ìœ„ë°˜ ì—¬ë¶€ ìš°ì„  ì ê²€

---

## ğŸ“Š Expected Benefits

### Quantitative Improvements

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Average Test Length** | 25-30 lines | 8-12 lines | **60% reduction** |
| **Setup vs Verification Ratio** | 80:20 | 20:80 | **4x more verification** |
| **Code Duplication** | 30+ repeated patterns | 3-5 centralized contexts | **85% reduction** |
| **New Test Creation Time** | 15-20 minutes | 5-8 minutes | **60% faster** |
| **Test Readability Score** | 6/10 | 9/10 | **50% improvement** |

### Qualitative Improvements

1. **í…ŒìŠ¤íŠ¸ ì˜ë„ ëª…í™•ì„±**: Setup noise ì œê±°ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ì§‘ì¤‘
2. **ìœ ì§€ë³´ìˆ˜ì„±**: ì¤‘ì•™í™”ëœ Contextë¡œ í•œ ê³³ì—ì„œ ê´€ë¦¬
3. **í™•ì¥ì„±**: ìƒˆë¡œìš´ ì‹œë‚˜ë¦¬ì˜¤ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥  
4. **ì¼ê´€ì„±**: í‘œì¤€í™”ëœ íŒ¨í„´ìœ¼ë¡œ ê°œë°œì ê°„ ì¼ê´€ì„± í–¥ìƒ
5. **ë””ë²„ê¹… ìš©ì´ì„±**: ë¬¸ì œ ë°œìƒì‹œ Context ë‹¨ìœ„ë¡œ ê²©ë¦¬í•˜ì—¬ ë””ë²„ê¹…

### Risk Mitigation

| Risk | Mitigation Strategy |
|------|-------------------|
| **í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨** | A/B testingìœ¼ë¡œ ê²°ê³¼ ì¼ì¹˜ í™•ì¸ í›„ ë§ˆì´ê·¸ë ˆì´ì…˜ |
| **í˜¸í™˜ì„± ë¬¸ì œ** | ê¸°ì¡´ fixtureë“¤ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ì•Šê³  ìƒˆ fixture ì¶”ê°€ |
| **ì„±ëŠ¥ ì €í•˜** | Context ì´ˆê¸°í™” ë¹„ìš© vs Setup ì¤‘ë³µ ì œê±° íš¨ê³¼ ì¸¡ì • |
| **í•™ìŠµ ë¹„ìš©** | ë‹¨ê³„ì  ë„ì…ê³¼ ë¬¸ì„œí™”ë¡œ ì ì§„ì  í•™ìŠµ |
| **Context ê³¼ê¸°ëŠ¥í™”** | ì»¨í…ìŠ¤íŠ¸ ìµœì†Œ ê·œì•½ ì¤€ìˆ˜, í¼ë¸”ë¦­ APIë§Œ í˜¸ì¶œ |
| **ìƒíƒœ/ìºì‹œ ëˆ„ìˆ˜** | í…ŒìŠ¤íŠ¸ë‹¹ ìƒˆ Factory/Settings ìƒì„±, ì»¨í…ìŠ¤íŠ¸ ì¬ì‚¬ìš© ê¸ˆì§€ |
| **í”Œë ˆì´í‚¤(ì´ë¦„/ì‹œê°„ ì¢…ì†)** | uuid ê¸°ë°˜ ëª…ëª…, ê³ ì • ì‹œë“œ ì ìš© |
| **ì´ˆê¸°í™” ë¹„ìš© ì¦ê°€** | performance_benchmarkë¡œ ì»¨í…ìŠ¤íŠ¸ init ì‹œê°„ ëª¨ë‹ˆí„°ë§(ì„ê³„ 120ms) |

---

## ğŸ¯ Success Metrics

### Technical KPIs
- âœ… **62/62 tests passing** (ìµœìš°ì„  ì§€í‘œ)
- âœ… **Average test length < 15 lines**
- âœ… **Setup code ratio < 30%**
- âœ… **Zero new flaky tests**
- âœ… **Context init time < 0.12s (p75)**
- âœ… **Zero test state leakage across tests**
 - âœ… **Artifact equivalence maintained (metrics/params/signature/schema)**

### Developer Experience KPIs  
- âœ… **New test creation time < 10 minutes**
- âœ… **Test readability score > 8/10** (peer review)
- âœ… **Context adoption rate > 80%** (new tests)
- âœ… **Developer satisfaction score > 4/5**

### CI/Execution Strategy
- **ìŠ¤ìœ„íŠ¸ ë¶„ë¦¬**: `unit`/`integration`/`e2e`ë¥¼ ì›Œí¬í”Œë¡œ ì¡ìœ¼ë¡œ ë¶„ë¦¬, ì»¨í…ìŠ¤íŠ¸ ë„ì… í…ŒìŠ¤íŠ¸ë¥¼ ë³„ë„ ë§¤íŠ¸ë¦­ìŠ¤ì— ë°°ì¹˜
- **ê²Œì´íŒ…**: A/B ë™ë“±ì„±/ì„±ëŠ¥ ìƒí•œì„ PR ê²Œì´íŠ¸ë¡œ ì¶”ê°€, ìœ„ë°° ì‹œ ë¨¸ì§€ ì°¨ë‹¨
- **ì•„í‹°íŒ©íŠ¸ ë¹„êµ**: MLflow run-level ë©”íŠ¸ë¦­/íŒŒë¼ë¯¸í„°/ì‹œê·¸ë‹ˆì²˜/ìŠ¤í‚¤ë§ˆ ìš”ì•½ì„ ë¹„êµí•˜ì—¬ ë™ë“±ì„± ê²€ì¦ ë¦¬í¬íŠ¸ ì²¨ë¶€

---

## ğŸ”„ Implementation Checklist

### Foundation Phase
- [ ] Create `tests/fixtures/contexts/` directory
- [ ] Implement `MLflowTestContext` class  
- [ ] Implement `ComponentTestContext` class
- [ ] Implement `DatabaseTestContext` class
- [ ] Create YAML templates in `tests/fixtures/templates/`
- [ ] Add new fixtures to `conftest.py`
- [ ] Verify all existing tests still pass (62/62)
- [ ] Standardize MLflow tracking URI and experiment naming
- [ ] Enforce deterministic seed usage in contexts
- [ ] Add `tests/fixtures/contexts/README.md` (minimal contract & anti-patterns)
- [ ] Add performance measurement for context init

### Pilot Phase
- [ ] Migrate 2-3 MLflow tests to new approach
- [ ] A/B test old vs new approach results
- [ ] Measure code length reduction
- [ ] Collect developer feedback
- [ ] Refine Context implementations based on feedback
- [ ] Include performance upper-bound checks in A/B (init + run)
 - [ ] Add artifact equivalence gate (metrics/params/signature/schema) in CI

### Migration Phase  
- [ ] Migrate remaining MLflow tests (8-9 tests)
- [ ] Migrate component interaction tests (10 tests)
- [ ] Migrate database integration tests (9 tests)
- [ ] Migrate remaining integration tests
- [ ] Verify 62/62 success rate after each category

### Cleanup Phase
- [ ] Remove deprecated setup code
- [ ] Update documentation
- [ ] Create developer guidelines for new Context usage
- [ ] Final validation: 62/62 tests passing
- [ ] Replace time-based names with uuid-based naming (standardization)

---

## ğŸ¨ Long-term Vision

### 6-Month Goals
- **100% Context Adoption**: ëª¨ë“  ìƒˆ í…ŒìŠ¤íŠ¸ê°€ Context íŒ¨í„´ ì‚¬ìš©
- **Template Ecosystem**: ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ìš© í…œí”Œë¦¿ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶•  
- **Auto-generation**: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìë™ ìƒì„± ë„êµ¬ ê°œë°œ

### 1-Year Goals  
- **Cross-Project Reusability**: Context íŒ¨í„´ì„ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ë„ ì ìš©
- **Performance Optimization**: Context ì´ˆê¸°í™” ì„±ëŠ¥ ìµœì í™”
- **Advanced Scenarios**: ë³µì¡í•œ multi-component ì‹œë‚˜ë¦¬ì˜¤ ì§€ì›

---

## ğŸ’¡ Conclusion

### Key Takeaways

1. **í˜„ì¬ êµ¬ì¡°ì˜ ì§„ì§œ ë¬¸ì œ**: Setup overheadê°€ í…ŒìŠ¤íŠ¸ ì½”ë“œì˜ 80% ì°¨ì§€
2. **Context-based í•´ê²°ì±…**: ì±…ì„ ë¶„ë¦¬ì™€ setup ì¤‘ì•™í™”ë¡œ ê·¼ë³¸ í•´ê²°  
3. **Zero-risk Migration**: ê¸°ì¡´ í…ŒìŠ¤íŠ¸ì™€ ê³µì¡´í•˜ë©° ì ì§„ì  ê°œì„ 
4. **ì‹¤ì§ˆì  ê°œì„ **: 60% ì½”ë“œ ê°ì†Œ, 4ë°° ë” ë§ì€ ê²€ì¦ ë¡œì§

### Final Recommendation

> **DO**: Context-based Architectureë¡œ ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜  
> **HOW**: A/B testingìœ¼ë¡œ ì•ˆì „ì„± í™•ë³´í•˜ë©° ë‹¨ê³„ì  ì ìš©  
> **GOAL**: 62/62 ì„±ê³µë¥  ìœ ì§€í•˜ë©´ì„œ ë” ì•„ë¦„ë‹¤ìš´ í…ŒìŠ¤íŠ¸ êµ¬ì¡° ë‹¬ì„±

ì´ ì ‘ê·¼ë²•ìœ¼ë¡œ **"í˜„ì¬ì²˜ëŸ¼ ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì™„ì „íˆ ë§Œì¡±í•˜ë©´ì„œë„ ë” ì•„ë¦„ë‹µê³  ì±…ì„ë¶„ë¦¬ëœ tests/ êµ¬ì¡°"**ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

*Generated: 2025-01-XX*  
*Status: Practical Implementation Ready*  
*Next Step: Foundation Phase Implementation*