# Practical Tests Structure Refactoring Strategy

## Executive Summary

**í˜„ì¬ ìƒíƒœ**: 62/62 integration tests 100% ì„±ê³µ  
**í•µì‹¬ ë¬¸ì œ**: í…ŒìŠ¤íŠ¸ ì½”ë“œì˜ 70-80%ê°€ setup, 20-30%ë§Œ ì‹¤ì œ ê²€ì¦  
**í•´ê²° ë°©ì•ˆ**: **Context-based Test Architecture** - ì±…ì„ ë¶„ë¦¬ì™€ setup ì¤‘ì•™í™”  
**ë§ˆì´ê·¸ë ˆì´ì…˜**: **Zero-risk incremental migration** - 100% ì„±ê³µë¥  ë³´ì¥

---

## ğŸ¯ Core Problems: í˜„ì¬ êµ¬ì¡°ì˜ ì§„ì§œ ë¬¸ì œì 

### Problem 1: Setup Overhead Dominance

**í˜„ì¬ í…ŒìŠ¤íŠ¸ êµ¬ì¡° ë¶„ì„**:
```python
def test_mlflow_experiment_creation_and_tracking(self, isolated_temp_directory, settings_builder):
    # â”â”â” SETUP CODE (23 lines, 80% of test) â”â”â”
    mlflow_uri = f"sqlite:///{isolated_temp_directory}/test_mlflow.db"
    experiment_name = f"integration_test_{int(time.time())}"
    
    test_data = pd.DataFrame({
        'feature1': np.random.rand(50),
        'feature2': np.random.rand(50),
        'target': np.random.randint(0, 2, 50)
    })
    data_path = isolated_temp_directory / "mlflow_test_data.csv"
    test_data.to_csv(data_path, index=False)
    
    settings = settings_builder \
        .with_task("classification") \
        .with_model("sklearn.ensemble.RandomForestClassifier") \
        .with_data_path(str(data_path)) \
        .with_mlflow(tracking_uri=mlflow_uri, experiment_name=experiment_name) \
        .build()
    
    # â”â”â” ACTUAL TEST LOGIC (3 lines, 20% of test) â”â”â”
    try:
        result = run_train_pipeline(settings)
        assert result is not None
    except Exception as e:
        assert True  # No Mock Hell validation
```

**ë¬¸ì œ**: í…ŒìŠ¤íŠ¸ì˜ ì§„ì§œ ëª©ì ì¸ "MLflow experiment creation validation"ì´ ì½”ë“œì˜ 20%ë§Œ ì°¨ì§€

### Problem 2: Responsibility Mixing

**í•œ í…ŒìŠ¤íŠ¸ê°€ ë‹´ë‹¹í•˜ëŠ” 4ê°€ì§€ ì±…ì„**:
1. **Data Generation**: Test data ìƒì„± ë° íŒŒì¼ ì €ì¥
2. **Resource Setup**: MLflow/Database URI ìƒì„±  
3. **Settings Configuration**: Settings ê°ì²´ êµ¬ì„±
4. **Business Logic Validation**: ì‹¤ì œ í…ŒìŠ¤íŠ¸í•˜ë ¤ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (ì§„ì§œ ëª©ì )

**ê²°ê³¼**: í…ŒìŠ¤íŠ¸ ì˜ë„ ë¶ˆë¶„ëª…, ì½”ë“œ ì¤‘ë³µ, ìœ ì§€ë³´ìˆ˜ ì–´ë ¤ì›€

### Problem 3: Template Duplication

**62ê°œ í…ŒìŠ¤íŠ¸ì—ì„œ ë°˜ë³µë˜ëŠ” íŒ¨í„´ë“¤**:

**Pattern A: MLflow Setup (11íšŒ ë°˜ë³µ)**
```python
mlflow_uri = f"sqlite:///{isolated_temp_directory}/xxx.db"
experiment_name = f"yyy_{int(time.time())}"
```

**Pattern B: YAML Configuration (15íšŒ ë°˜ë³µ)**  
```python
config_yaml = """
environment:
  name: integration_test
data_source:
  adapter_type: storage
mlflow:
  tracking_uri: sqlite:///zzz.db
"""
```

**Pattern C: Component Factory Setup (30íšŒ ë°˜ë³µ)**
```python
factory = Factory(settings)
adapter = factory.create_data_adapter()
model = factory.create_model()
```

---

## ğŸ—ï¸ Proposed Architecture: Context-based Test Structure

### Core Design Principles

1. **Separation of Concerns**: Setup vs Verification ì±…ì„ ë¶„ë¦¬
2. **Context Management**: ê° í…ŒìŠ¤íŠ¸ ì˜ì—­ë³„ ì „ìš© Context í´ë˜ìŠ¤
3. **Zero Configuration**: ìµœì†Œí•œì˜ ì„¤ì •ìœ¼ë¡œ ìµœëŒ€í•œì˜ setup
4. **Incremental Migration**: ê¸°ì¡´ í…ŒìŠ¤íŠ¸ì™€ ìƒˆ êµ¬ì¡° ê³µì¡´

### Architecture Overview

```
tests/
â”œâ”€â”€ conftest.py                     # ê¸°ì¡´ + ìƒˆë¡œìš´ context fixtures
â”œâ”€â”€ fixtures/
â”‚   â”œâ”€â”€ contexts/                   # ğŸ†• Context Classes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mlflow_context.py      # MLflow Test Context
â”‚   â”‚   â”œâ”€â”€ database_context.py    # Database Test Context
â”‚   â”‚   â”œâ”€â”€ component_context.py   # Component Factory Context
â”‚   â”‚   â””â”€â”€ scenario_context.py    # End-to-end Scenario Context
â”‚   â”œâ”€â”€ templates/                  # ğŸ†• YAML Templates
â”‚   â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”‚   â”œâ”€â”€ classification_base.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ regression_base.yaml
â”‚   â”‚   â”‚   â””â”€â”€ mlflow_base.yaml
â”‚   â”‚   â””â”€â”€ scenarios/
â”‚   â”‚       â”œâ”€â”€ classification_full.yaml
â”‚   â”‚       â””â”€â”€ regression_full.yaml
â”‚   â”œâ”€â”€ expected/                   # ğŸ†• Enhanced Expected Outputs
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”‚   â”œâ”€â”€ classification_baseline.json
â”‚   â”‚   â”‚   â””â”€â”€ regression_baseline.json
â”‚   â”‚   â”œâ”€â”€ predictions/
â”‚   â”‚   â”‚   â”œâ”€â”€ sample_classification.csv
â”‚   â”‚   â”‚   â””â”€â”€ sample_regression.csv
â”‚   â”‚   â””â”€â”€ responses/
â”‚   â”‚       â”œâ”€â”€ mlflow_tracking.json
â”‚   â”‚       â””â”€â”€ component_interactions.json
â”‚   â””â”€â”€ [ê¸°ì¡´ ë””ë ‰í† ë¦¬ë“¤ ìœ ì§€]
â””â”€â”€ integration/                    # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ + ìƒˆ ë²„ì „ ê³µì¡´
```

---

## ğŸ”§ Implementation Examples

### Example 1: MLflow Test Context

**Before (í˜„ì¬ ë°©ì‹)**:
```python
def test_mlflow_experiment_creation(self, isolated_temp_directory, settings_builder):
    # 23 lines of setup code
    mlflow_uri = f"sqlite:///{isolated_temp_directory}/test_mlflow.db"
    experiment_name = f"integration_test_{int(time.time())}"
    test_data = pd.DataFrame({...})
    data_path = isolated_temp_directory / "test.csv" 
    test_data.to_csv(data_path, index=False)
    settings = settings_builder.with_task("classification")...build()
    
    # 3 lines of actual test
    try:
        result = run_train_pipeline(settings)
        assert result is not None
    except Exception:
        assert True
```

**After (Context ë°©ì‹)**:
```python
def test_mlflow_experiment_creation(self, mlflow_test_context):
    with mlflow_test_context.for_classification(experiment="experiment_creation") as ctx:
        # 8 lines of focused verification
        result = run_train_pipeline(ctx.settings)
        
        assert result is not None
        assert ctx.experiment_exists()
        assert ctx.has_active_run()
        assert len(ctx.get_run_metrics()) > 0
        assert ctx.get_experiment_run_count() == 1
```

**MLflowTestContext êµ¬í˜„**:
```python
class MLflowTestContext:
    def __init__(self, isolated_temp_directory, settings_builder, test_data_generator):
        self.temp_dir = isolated_temp_directory
        self.settings_builder = settings_builder
        self.data_generator = test_data_generator
        
    def for_classification(self, experiment: str, model: str = "RandomForestClassifier"):
        return MLflowContextManager(
            task="classification",
            experiment_suffix=experiment,
            model_class=f"sklearn.ensemble.{model}",
            context=self
        )

class MLflowContextManager:
    def __enter__(self):
        # All setup automation
        self.mlflow_uri = f"sqlite:///{self.context.temp_dir}/mlflow_{self.experiment_suffix}.db"
        self.experiment_name = f"{self.experiment_suffix}_{int(time.time())}"
        
        # Auto-generate test data
        X, y = self.context.data_generator.classification_data(50, 4)
        self.test_data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(4)])
        self.test_data['target'] = y
        
        self.data_path = self.context.temp_dir / f"data_{self.experiment_suffix}.csv"
        self.test_data.to_csv(self.data_path, index=False)
        
        # Auto-configure settings
        self.settings = self.context.settings_builder \
            .with_task(self.task) \
            .with_model(self.model_class) \
            .with_data_path(str(self.data_path)) \
            .with_mlflow(self.mlflow_uri, self.experiment_name) \
            .build()
        
        # Setup MLflow client
        self.mlflow_client = MlflowClient(tracking_uri=self.mlflow_uri)
        
        return self
        
    def experiment_exists(self):
        try:
            exp = self.mlflow_client.get_experiment_by_name(self.experiment_name)
            return exp is not None
        except:
            return False
    
    def has_active_run(self):
        runs = self.mlflow_client.list_run_infos(self.experiment_id)
        return len(runs) > 0
```

### Example 2: Component Test Context

**Before**:
```python
def test_adapter_to_model_data_flow(self, isolated_temp_directory, settings_builder, test_data_generator):
    # 15 lines of component setup
    X, y = test_data_generator.classification_data(50, 4)
    test_data = pd.DataFrame(X, columns=[...])
    data_path = isolated_temp_directory / "dataflow_test.csv"
    test_data.to_csv(data_path, index=False)
    settings = settings_builder.with_task("classification")...build()
    factory = Factory(settings)
    adapter = factory.create_data_adapter()
    model = factory.create_model()
    
    # 5 lines of actual test
    raw_data = adapter.read(str(data_path))
    X_train = raw_data[feature_columns]
    y_train = raw_data[target_column]
    assert len(X_train) > 0
    assert len(y_train) > 0
```

**After**:
```python
def test_adapter_to_model_data_flow(self, component_test_context):
    with component_test_context.classification_stack() as ctx:
        # 8 lines of focused data flow verification
        raw_data = ctx.adapter.read(ctx.data_path)
        processed_data = ctx.prepare_model_input(raw_data)
        
        assert ctx.validate_data_flow(raw_data, processed_data)
        assert processed_data.shape[0] == 50
        assert processed_data.shape[1] == 4
        assert ctx.adapter.is_compatible_with(ctx.model)
        assert ctx.model.can_accept(processed_data)
```

### Example 3: Scenario-based Testing

**Before**: ë³µì¡í•œ end-to-end í…ŒìŠ¤íŠ¸ì—ì„œ 50+ lines setup

**After**:
```python
def test_full_classification_pipeline(self, classification_scenario):
    scenario = classification_scenario  # All setup done
    
    # 10 lines of pure business logic testing
    pipeline_result = scenario.run_full_pipeline()
    
    assert pipeline_result.model_trained
    assert pipeline_result.evaluation_completed  
    assert pipeline_result.mlflow_logged
    assert scenario.verify_model_quality(min_accuracy=0.7)
    assert scenario.verify_mlflow_artifacts()
    assert scenario.verify_evaluation_metrics()
```

---

## ğŸš€ Migration Strategy: Zero-Risk Incremental Approach

### Phase 1: Foundation Setup (Week 1)

**ìƒˆë¡œìš´ êµ¬ì¡° ì¶”ê°€ (ê¸°ì¡´ ì½”ë“œ ë¬´ë³€ê²½)**:

1. **Context Classes ìƒì„±**:
```bash
mkdir -p tests/fixtures/contexts
mkdir -p tests/fixtures/templates
```

2. **conftest.py í™•ì¥**:
```python
# conftest.pyì— ì¶”ê°€ (ê¸°ì¡´ fixtureë“¤ê³¼ í•¨ê»˜ ê³µì¡´)
@pytest.fixture
def mlflow_test_context(isolated_temp_directory, settings_builder, test_data_generator):
    return MLflowTestContext(isolated_temp_directory, settings_builder, test_data_generator)

@pytest.fixture  
def component_test_context(isolated_temp_directory, settings_builder, test_data_generator):
    return ComponentTestContext(isolated_temp_directory, settings_builder, test_data_generator)

@pytest.fixture
def classification_scenario(isolated_temp_directory, settings_builder, test_data_generator):
    return ClassificationScenario(isolated_temp_directory, settings_builder, test_data_generator)
```

3. **Template YAML íŒŒì¼ë“¤ ìƒì„±**:
```yaml
# tests/fixtures/templates/configs/classification_base.yaml
environment:
  name: "test_env"
data_source:
  name: "test_storage" 
  adapter_type: "storage"
mlflow:
  tracking_uri: "{{mlflow_uri}}"
  experiment_name: "{{experiment_name}}"
```

### Phase 2: Pilot Testing (Week 2-3)

**A/B í…ŒìŠ¤íŒ… ë°©ì‹**:

```python
# ê¸°ì¡´ í…ŒìŠ¤íŠ¸ (ë³€ê²½ ì—†ìŒ)
def test_mlflow_experiment_creation_and_tracking(self, isolated_temp_directory, settings_builder):
    # ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€
    
# ìƒˆë¡œìš´ ë°©ì‹ (ê°™ì€ íŒŒì¼ì— ì¶”ê°€)  
def test_mlflow_experiment_creation_and_tracking_v2(self, mlflow_test_context):
    # ìƒˆë¡œìš´ Context ë°©ì‹
    
# ê²°ê³¼ ë¹„êµ í…ŒìŠ¤íŠ¸
def test_compare_old_vs_new_approach(self, isolated_temp_directory, settings_builder, mlflow_test_context):
    # ë‘ ë°©ì‹ì˜ ê²°ê³¼ê°€ ë™ì¼í•œì§€ ê²€ì¦
```

**ê²€ì¦ ê¸°ì¤€**:
- âœ… ìƒˆ ë°©ì‹ê³¼ ê¸°ì¡´ ë°©ì‹ ê²°ê³¼ 100% ì¼ì¹˜
- âœ… ìƒˆ ë°©ì‹ì´ ë” ì§§ê³  ëª…í™•í•œ ì½”ë“œ
- âœ… ìƒˆ ë°©ì‹ì´ ë” ë§ì€ ê²€ì¦ ë¡œì§ í¬í•¨ ê°€ëŠ¥

### Phase 3: Category-wise Migration (Week 4-6)

**ë‹¨ê³„ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜**:

1. **MLflow Tests** (Week 4):
   - `test_mlflow_integration.py` ë‚´ 11ê°œ í…ŒìŠ¤íŠ¸
   - í•œ ë²ˆì— í•˜ë‚˜ì”© ìƒˆ ë°©ì‹ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
   - ê° í…ŒìŠ¤íŠ¸ë§ˆë‹¤ ê²°ê³¼ ì¼ì¹˜ í™•ì¸

2. **Component Interaction Tests** (Week 5):
   - `test_component_interactions.py` ë‚´ 10ê°œ í…ŒìŠ¤íŠ¸
   - Context-based component testing ì ìš©

3. **Database Integration Tests** (Week 6):
   - `test_database_integration.py` ë‚´ 9ê°œ í…ŒìŠ¤íŠ¸
   - DatabaseTestContext ì ìš©

### Phase 4: Validation & Cleanup (Week 7-8)

**ìµœì¢… ê²€ì¦**:
```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰í•˜ì—¬ ì„±ê³µë¥  í™•ì¸
pytest tests/integration/ -v
# Expected: 62/62 tests passed (100%)
```

**Cleanup ê¸°ì¤€**:
- ìƒˆ ë°©ì‹ê³¼ ê¸°ì¡´ ë°©ì‹ ê²°ê³¼ê°€ **100% ì¼ì¹˜** í™•ì¸ëœ ê²½ìš°ì—ë§Œ ê¸°ì¡´ ì½”ë“œ ì œê±°
- ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê²½ìš° ë‘ ë°©ì‹ ê³µì¡´ ìœ ì§€
- ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” setup ì½”ë“œë§Œ ì •ë¦¬

---

## ğŸ“Š Expected Benefits

### Quantitative Improvements

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Average Test Length** | 25-30 lines | 8-12 lines | **60% reduction** |
| **Setup vs Verification Ratio** | 80:20 | 20:80 | **4x more verification** |
| **Code Duplication** | 30+ repeated patterns | 3-5 centralized contexts | **85% reduction** |
| **New Test Creation Time** | 15-20 minutes | 5-8 minutes | **60% faster** |
| **Test Readability Score** | 6/10 | 9/10 | **50% improvement** |

### Qualitative Improvements

1. **í…ŒìŠ¤íŠ¸ ì˜ë„ ëª…í™•ì„±**: Setup noise ì œê±°ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ì§‘ì¤‘
2. **ìœ ì§€ë³´ìˆ˜ì„±**: ì¤‘ì•™í™”ëœ Contextë¡œ í•œ ê³³ì—ì„œ ê´€ë¦¬
3. **í™•ì¥ì„±**: ìƒˆë¡œìš´ ì‹œë‚˜ë¦¬ì˜¤ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥  
4. **ì¼ê´€ì„±**: í‘œì¤€í™”ëœ íŒ¨í„´ìœ¼ë¡œ ê°œë°œì ê°„ ì¼ê´€ì„± í–¥ìƒ
5. **ë””ë²„ê¹… ìš©ì´ì„±**: ë¬¸ì œ ë°œìƒì‹œ Context ë‹¨ìœ„ë¡œ ê²©ë¦¬í•˜ì—¬ ë””ë²„ê¹…

### Risk Mitigation

| Risk | Mitigation Strategy |
|------|-------------------|
| **í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨** | A/B testingìœ¼ë¡œ ê²°ê³¼ ì¼ì¹˜ í™•ì¸ í›„ ë§ˆì´ê·¸ë ˆì´ì…˜ |
| **í˜¸í™˜ì„± ë¬¸ì œ** | ê¸°ì¡´ fixtureë“¤ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ì•Šê³  ìƒˆ fixture ì¶”ê°€ |
| **ì„±ëŠ¥ ì €í•˜** | Context ì´ˆê¸°í™” ë¹„ìš© vs Setup ì¤‘ë³µ ì œê±° íš¨ê³¼ ì¸¡ì • |
| **í•™ìŠµ ë¹„ìš©** | ë‹¨ê³„ì  ë„ì…ê³¼ ë¬¸ì„œí™”ë¡œ ì ì§„ì  í•™ìŠµ |

---

## ğŸ¯ Success Metrics

### Technical KPIs
- âœ… **62/62 tests passing** (ìµœìš°ì„  ì§€í‘œ)
- âœ… **Average test length < 15 lines**
- âœ… **Setup code ratio < 30%**
- âœ… **Zero new flaky tests**

### Developer Experience KPIs  
- âœ… **New test creation time < 10 minutes**
- âœ… **Test readability score > 8/10** (peer review)
- âœ… **Context adoption rate > 80%** (new tests)
- âœ… **Developer satisfaction score > 4/5**

---

## ğŸ”„ Implementation Checklist

### Foundation Phase
- [ ] Create `tests/fixtures/contexts/` directory
- [ ] Implement `MLflowTestContext` class  
- [ ] Implement `ComponentTestContext` class
- [ ] Implement `DatabaseTestContext` class
- [ ] Create YAML templates in `tests/fixtures/templates/`
- [ ] Add new fixtures to `conftest.py`
- [ ] Verify all existing tests still pass (62/62)

### Pilot Phase
- [ ] Migrate 2-3 MLflow tests to new approach
- [ ] A/B test old vs new approach results
- [ ] Measure code length reduction
- [ ] Collect developer feedback
- [ ] Refine Context implementations based on feedback

### Migration Phase  
- [ ] Migrate remaining MLflow tests (8-9 tests)
- [ ] Migrate component interaction tests (10 tests)
- [ ] Migrate database integration tests (9 tests)
- [ ] Migrate remaining integration tests
- [ ] Verify 62/62 success rate after each category

### Cleanup Phase
- [ ] Remove deprecated setup code
- [ ] Update documentation
- [ ] Create developer guidelines for new Context usage
- [ ] Final validation: 62/62 tests passing

---

## ğŸ¨ Long-term Vision

### 6-Month Goals
- **100% Context Adoption**: ëª¨ë“  ìƒˆ í…ŒìŠ¤íŠ¸ê°€ Context íŒ¨í„´ ì‚¬ìš©
- **Template Ecosystem**: ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ìš© í…œí”Œë¦¿ ë¼ì´ë¸ŒëŸ¬ë¦¬ êµ¬ì¶•  
- **Auto-generation**: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìë™ ìƒì„± ë„êµ¬ ê°œë°œ

### 1-Year Goals  
- **Cross-Project Reusability**: Context íŒ¨í„´ì„ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ë„ ì ìš©
- **Performance Optimization**: Context ì´ˆê¸°í™” ì„±ëŠ¥ ìµœì í™”
- **Advanced Scenarios**: ë³µì¡í•œ multi-component ì‹œë‚˜ë¦¬ì˜¤ ì§€ì›

---

## ğŸ’¡ Conclusion

### Key Takeaways

1. **í˜„ì¬ êµ¬ì¡°ì˜ ì§„ì§œ ë¬¸ì œ**: Setup overheadê°€ í…ŒìŠ¤íŠ¸ ì½”ë“œì˜ 80% ì°¨ì§€
2. **Context-based í•´ê²°ì±…**: ì±…ì„ ë¶„ë¦¬ì™€ setup ì¤‘ì•™í™”ë¡œ ê·¼ë³¸ í•´ê²°  
3. **Zero-risk Migration**: ê¸°ì¡´ í…ŒìŠ¤íŠ¸ì™€ ê³µì¡´í•˜ë©° ì ì§„ì  ê°œì„ 
4. **ì‹¤ì§ˆì  ê°œì„ **: 60% ì½”ë“œ ê°ì†Œ, 4ë°° ë” ë§ì€ ê²€ì¦ ë¡œì§

### Final Recommendation

> **DO**: Context-based Architectureë¡œ ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜  
> **HOW**: A/B testingìœ¼ë¡œ ì•ˆì „ì„± í™•ë³´í•˜ë©° ë‹¨ê³„ì  ì ìš©  
> **GOAL**: 62/62 ì„±ê³µë¥  ìœ ì§€í•˜ë©´ì„œ ë” ì•„ë¦„ë‹¤ìš´ í…ŒìŠ¤íŠ¸ êµ¬ì¡° ë‹¬ì„±

ì´ ì ‘ê·¼ë²•ìœ¼ë¡œ **"í˜„ì¬ì²˜ëŸ¼ ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ë¥¼ ì™„ì „íˆ ë§Œì¡±í•˜ë©´ì„œë„ ë” ì•„ë¦„ë‹µê³  ì±…ì„ë¶„ë¦¬ëœ tests/ êµ¬ì¡°"**ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

*Generated: 2025-01-XX*  
*Status: Practical Implementation Ready*  
*Next Step: Foundation Phase Implementation*