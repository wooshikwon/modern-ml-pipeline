# 🚀 Blueprint v17.0 Post-Implementation: 현실적 실행 기반 시스템 구축 계획

## 💎 **THE ULTIMATE MISSION: From Theory to Real Implementation**

Blueprint v17.0 "Automated Excellence Vision"의 **철학적 설계 완료** 후, **9대 핵심 설계 원칙에 기반한 환경별 차등적 기능 분리를 통한 실제 실행 가능한 시스템**으로 발전시키기 위한 **현실적 단계별 실행 로드맵**입니다.

**🎯 Blueprint의 환경별 운영 철학 구현:**
- **LOCAL**: "제약은 단순함을 낳고, 단순함은 집중을 낳는다" - uv sync → 3분 이내 즉시 실행
- **DEV**: "모든 기능이 완전히 작동하는 안전한 실험실" - 완전한 Feature Store + 15분 이내 setup
- **PROD**: "확장성과 안정성의 정점" - 클라우드 네이티브 (이 계획 범위 외)

---

## 🏗️ **현재 상황 분석: 이상향과 현실 간 Gap**

### **📊 9대 원칙 기반 현재 구현 상태**

| 원칙 | 설계 완성도 | 구현 완성도 | 실행 가능성 | Critical Gap |
|------|-------------|-------------|-------------|--------------|
| **1. 레시피는 논리, 설정은 인프라** | 100% | 95% | 90% | Recipe URI 스킴 잔존 |
| **2. 통합 데이터 어댑터** | 100% | 100% | 85% | 환경 호환성 이슈 |
| **3. URI 기반 동작 및 동적 팩토리** | 100% | 70% | 60% | Pipeline URI 파싱 잔존 |
| **4. 순수 로직 아티팩트** | 100% | 100% | 95% | 미미한 이슈 |
| **5. 단일 Augmenter, 컨텍스트 주입** | 100% | 100% | 90% | 환경별 테스트 필요 |
| **6. 자기 기술 API** | 100% | 100% | 85% | 환경별 검증 필요 |
| **7. 하이브리드 통합 인터페이스** | 100% | 100% | 90% | Feature Store 연동 |
| **8. 자동 HPO + Data Leakage 방지** | 100% | 100% | 85% | 환경별 검증 필요 |
| **9. 환경별 차등적 기능 분리** | 100% | 95% | 70% | 실제 환경 테스트 필요 |

**전체 달성도: 설계 100% | 구현 95% | 실행 83%**

### **🚨 Critical 실행 저해 요소**

#### **1. 개발 환경 불일치 (즉시 해결 필요)**
```yaml
문제: uv vs pip 혼재, Python 버전 불일치
현재 상태: Python 3.10.11, uv는 3.12.4에만 존재
모든 의존성 명령어: pip 기반으로 작성되어 있음
영향: 모든 setup 명령어 실행 불가
```

#### **2. 아키텍처 완전성 Gap (Blueprint 원칙 3 위반)**
```yaml
문제: Pipeline에서 Factory 역할 침범
구체적 위반:
- src/pipelines/train_pipeline.py: urlparse() 직접 사용
- 환경별 분기를 Pipeline에서 처리
- Factory 우회한 어댑터 생성
영향: 아키텍처 일관성 완전 파괴
```

#### **3. 테스트 실행 불가능**
```yaml
문제: 이상적 계획이지만 실제 실행 불가
구체적 문제:
- tests/recipes/ 디렉토리 존재하지 않음
- 기본 train 명령어 패키지 의존성 오류
- 환경별 실행 검증 불가
영향: 모든 개발 작업 중단
```

---

## 🎯 **Phase 0: 환경 정리 및 기반 구축 (Day 1-2)**
*모든 후속 작업의 전제 조건*

### **📋 Phase 0 Overview**
```yaml
목표: 실제 실행 가능한 기반 환경 구축
철학: Blueprint 2.6절 "현대적 개발 환경 철학" 구현
성공 기준: uv sync → python main.py train 즉시 실행
소요 시간: 2일
```

### **🔧 Phase 0.1: 개발 환경 표준화 (Day 1)**

#### **A. Python 환경 통일**
```bash
# 현재 상태 확인
python --version  # 3.10.11
pyenv versions   # 3.12.4 available

# Python 3.12.4로 전환
pyenv local 3.12.4
python --version  # 3.12.4 확인

# uv 환경 설정
uv --version     # 정상 동작 확인
uv venv          # 가상환경 생성
source .venv/bin/activate  # 환경 활성화
```

#### **B. uv 기반 의존성 설치**
```bash
# 기존 pip 설치물 완전 정리
pip freeze > old_requirements.txt  # 백업용
pip uninstall -r old_requirements.txt -y

# uv 기반 의존성 설치
uv sync  # pyproject.toml 기반 설치
uv add optuna>=3.4.0 catboost>=1.2.0 lightgbm>=4.1.0  # 누락 의존성 추가
```

#### **C. 환경 검증**
```bash
# 기본 import 테스트
python -c "import typer, mlflow, pandas; print('✅ 기본 의존성 OK')"
python -c "import optuna, catboost, lightgbm; print('✅ ML 라이브러리 OK')"

# Settings 로딩 테스트
python -c "
from src.settings import Settings
settings = Settings.load()
print(f'✅ Settings 로딩 OK: {settings.environment.app_env}')
"
```

### **🗂️ Phase 0.2: 최소 실행 환경 구축 (Day 1)**

#### **A. 테스트 데이터 준비**
```bash
# data/ 디렉토리 구조 확인 및 생성
mkdir -p data/{raw,processed,artifacts}
ls -la data/processed/  # 기존 테스트 데이터 확인

# 테스트 데이터 생성 (없을 경우)
python scripts/generate_local_test_data.py
ls -la data/processed/  # classification_test.parquet 등 확인
```

#### **B. 기본 Recipe 검증**
```bash
# 기존 Recipe 파일 확인
ls -la recipes/local_classification_test.yaml

# Recipe 내용 검증
python -c "
from src.settings import load_settings_by_file
settings = load_settings_by_file('local_classification_test')
print(f'✅ Recipe 로딩 OK: {settings.model.class_path}')
"
```

#### **C. 기본 워크플로우 검증**
```bash
# 최소 train 명령 실행
python main.py train --recipe-file "local_classification_test"

# 예상 결과: 
# - PassThroughAugmenter 동작 확인
# - 로컬 MLflow 저장 확인
# - 에러 없이 완료
```

### **✅ Phase 0 완료 기준**
```yaml
필수 조건:
- ✅ Python 3.12.4 환경 구성
- ✅ uv sync 완료 (모든 의존성 설치)
- ✅ python main.py train 정상 실행
- ✅ data/processed/ 테스트 데이터 존재
- ✅ MLflow 로컬 저장 확인

검증 명령어:
uv sync && python main.py train --recipe-file "local_classification_test"
```

---

## 🎯 **Phase 1: 아키텍처 완전성 달성 (Day 3-5)**
*Blueprint 원칙 3 "URI 기반 동작 및 동적 팩토리" 완전 구현*

### **📋 Phase 1 Overview**
```yaml
목표: Pipeline의 Factory 역할 침범 완전 제거
철학: "모든 데이터 접근은 Factory를 통해서만"
성공 기준: Pipeline에서 URI 파싱 로직 완전 제거
소요 시간: 3일
```

### **🏗️ Phase 1.1: Pipeline 아키텍처 정리 (Day 3-4)**

#### **A. train_pipeline.py 아키텍처 위반 수정**
```python
# 🚨 현재 잘못된 코드 (src/pipelines/train_pipeline.py:40-50)
loader_uri = settings.model.loader.source_uri
if settings.environment.app_env == "local" and settings.model.loader.local_override_uri:
    loader_uri = settings.model.loader.local_override_uri

scheme = urlparse(loader_uri).scheme or 'file'  # ❌ Blueprint 원칙 3 위반
data_adapter = factory.create_data_adapter(scheme)  # ❌ 잘못된 호출

# ✅ 올바른 코드 (수정 후)
data_adapter = factory.create_data_adapter("loader")  # ✅ Factory가 환경 처리
df = data_adapter.read(settings.model.loader.source_uri)  # ✅ 순수 논리 경로
```

#### **B. inference_pipeline.py 동일 수정**
```python
# 🚨 현재 잘못된 코드 (src/pipelines/inference_pipeline.py)
loader_uri = wrapper.loader_uri
scheme = urlparse(loader_uri).scheme  # ❌ Blueprint 원칙 3 위반
data_adapter = factory.create_data_adapter(scheme)  # ❌ 잘못된 호출

# ✅ 올바른 코드 (수정 후)
data_adapter = factory.create_data_adapter("loader")  # ✅ Factory가 환경 처리
input_df = data_adapter.read(wrapper.loader_uri, params=context_params)
```

#### **C. Factory 호출 방식 완전 통일**
```python
# 모든 Pipeline에서 통일된 Factory 호출
data_adapter = factory.create_data_adapter("loader")      # 데이터 로딩용
storage_adapter = factory.create_data_adapter("storage")  # 결과 저장용
feature_adapter = factory.create_data_adapter("feature_store")  # 피처 조회용
```

### **⚙️ Phase 1.2: Settings Import 완전 정리 (Day 4)**

#### **A. 테스트 파일 Import 패턴 수정**
```bash
# 현재 잘못된 패턴 (12개 파일)
grep -r "from src.settings.settings import" tests/

# 일괄 수정 명령어
find tests/ -name "*.py" -exec sed -i 's/from src\.settings\.settings import/from src.settings import/g' {} \;

# 수정 결과 확인
grep -r "from src.settings import" tests/ | wc -l  # 모든 파일 확인
```

#### **B. 기존 settings.py 제거**
```bash
# 백업 생성
cp src/settings/settings.py src/settings/settings.py.backup_$(date +%Y%m%d_%H%M%S)

# 기존 파일 제거
rm src/settings/settings.py

# 모든 import 동작 확인
python -c "from src.settings import Settings; print('✅ 분리된 Settings 구조 OK')"
```

#### **C. 전체 테스트 스위트 실행**
```bash
# 단위 테스트 실행
python -m pytest tests/settings/ -v
python -m pytest tests/core/test_factory.py -v

# 통합 테스트 실행
python -m pytest tests/integration/test_compatibility.py -v

# 전체 테스트 (선택적)
python -m pytest tests/ -v --tb=short
```

### **✅ Phase 1 완료 기준**
```yaml
필수 조건:
- ✅ Pipeline에서 urlparse() 완전 제거
- ✅ 모든 데이터 접근이 Factory 경유
- ✅ 환경별 분기 로직 Factory에서만 처리
- ✅ Settings import 패턴 완전 정리
- ✅ 전체 테스트 스위트 통과

검증 명령어:
grep -r "urlparse" src/pipelines/  # 결과 없어야 함
grep -r "from src.settings.settings import" tests/  # 결과 없어야 함
python -m pytest tests/core/test_factory.py -v
```

---

## 🎯 **Phase 2: 환경별 기능 검증 (Day 6-10)**
*Blueprint 원칙 9 "환경별 차등적 기능 분리" 완전 구현*

### **📋 Phase 2 Overview**
```yaml
목표: LOCAL/DEV 환경에서 실제 기능 완전 동작
철학: 환경별 특화된 가치 실현
성공 기준: 각 환경의 철학적 목표 달성
소요 시간: 5일
```

### **🏠 Phase 2.1: LOCAL 환경 완전 검증 (Day 6-7)**

#### **A. LOCAL 환경 철학 구현 확인**
```bash
# 환경 설정
export APP_ENV=local

# Blueprint 철학 "제약은 단순함을 낳는다" 검증
python main.py train --recipe-file "local_classification_test"
# 예상 결과: PassThroughAugmenter 동작 + 3분 이내 완료
```

#### **B. 의도적 제약 기능 검증**
```bash
# API Serving 시스템적 차단 확인
python main.py serve-api --run-id "latest"
# 예상 결과: Blueprint 철학 메시지와 함께 차단

# 지원 기능 확인
python main.py batch-inference --run-id "latest"  # ✅ 지원
python main.py evaluate --run-id "latest"        # ✅ 지원
```

#### **C. 완전 독립성 검증**
```bash
# 외부 서비스 의존성 없이 동작 확인
# (Redis, PostgreSQL 등 모든 외부 서비스 중지 상태에서)
python main.py train --recipe-file "local_classification_test"
# 예상 결과: 정상 동작 (외부 의존성 없음)
```

#### **D. 3분 이내 Setup 시간 달성**
```bash
# 시간 측정 스크립트
time (uv sync && python main.py train --recipe-file "local_classification_test")
# 목표: 3분 이내 완료
```

### **🔧 Phase 2.2: DEV 환경 통합 구축 (Day 8-10)**

#### **A. 외부 인프라 구축**
```bash
# mmp-local-dev 설정
cd ../mmp-local-dev
./setup.sh  # PostgreSQL + Redis + Feast 설치

# 연결 확인
psql -h localhost -U mluser -d mlpipeline -c "SELECT version();"
redis-cli ping  # PONG 응답 확인
```

#### **B. DEV 환경 설정**
```bash
# 환경 전환
export APP_ENV=dev
cd /path/to/modern-ml-pipeline

# 환경별 설정 확인
python -c "
from src.settings import Settings
settings = Settings.load()
print(f'환경: {settings.environment.app_env}')
print(f'DB 호스트: {settings.data_adapters.adapters[\"postgresql\"].config[\"host\"]}')
"
```

#### **C. 완전한 기능 검증**
```bash
# Feature Store 기반 학습
python main.py train --recipe-file "models/classification/random_forest_classifier"
# 예상 결과: FeatureStoreAugmenter 동작 + 완전한 피처 증강

# API 서빙 테스트
python main.py serve-api --run-id "latest" &
sleep 5
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"user_id": "test_user_123"}'
# 예상 결과: 동적 스키마 + 실시간 Feature Store 조회
```

#### **D. 15분 이내 Setup 시간 달성**
```bash
# 전체 DEV 환경 구축 시간 측정
time (cd ../mmp-local-dev && ./setup.sh && cd ../modern-ml-pipeline && 
      export APP_ENV=dev && python main.py train --recipe-file "models/classification/random_forest_classifier")
# 목표: 15분 이내 완료
```

### **✅ Phase 2 완료 기준**
```yaml
LOCAL 환경:
- ✅ 3분 이내 uv sync → train 완료
- ✅ PassThroughAugmenter 정상 동작
- ✅ API Serving 시스템적 차단 동작
- ✅ 외부 의존성 없이 완전 독립 동작

DEV 환경:
- ✅ 15분 이내 완전한 개발 환경 구축
- ✅ FeatureStoreAugmenter 정상 동작
- ✅ API 서빙 완전 기능 동작
- ✅ 모든 Blueprint 기능 동작

검증 명령어:
# LOCAL
APP_ENV=local python main.py train --recipe-file "local_classification_test"
# DEV  
APP_ENV=dev python main.py train --recipe-file "models/classification/random_forest_classifier"
```

---

## 🎯 **Phase 3: Blueprint 엑셀런스 완성 (Day 11-14)**
*9대 원칙 100% 달성*

### **📋 Phase 3 Overview**
```yaml
목표: Blueprint v17.0 "Automated Excellence Vision" 완전 구현
철학: 9대 원칙 모두 실코드로 구현
성공 기준: 환경별 전환 + 완전한 재현성 + 자동화된 최적화
소요 시간: 4일

접근 방식:
- Phase 3.1 & 3.2: 외부 인프라 없이 진행
- Phase 3.3: 간단한 Docker Compose로 최소 인프라 구성 후 실제 테스트
```

### **📄 Phase 3.1: Recipe 시스템 완전 정리 (Day 11-12)**
*외부 인프라 불필요*

#### **A. URI 스킴 제거 (Blueprint 원칙 1 완전 준수)**
```bash
# 현재 URI 스킴 사용 파일 확인
grep -r "bq://" recipes/
grep -r "file://" recipes/

# 수정 예시: xgboost_x_learner.yaml
# 🚨 현재 잘못된 내용
source_uri: "bq://recipes/sql/loader/user_features.sql"
local_override_uri: "file://local/data/sample_user_features.csv"

# ✅ 올바른 내용 (수정 후)
source_uri: "recipes/sql/loader/user_features.sql"  # 순수 논리 경로
```

#### **B. 우선순위 Recipe 파일 정리**
```bash
# 핵심 Recipe 파일 수정 (우선순위 순서)
1. local_classification_test.yaml  # 이미 정리됨
2. models/classification/random_forest_classifier.yaml
3. models/regression/lightgbm_regressor.yaml
4. xgboost_x_learner.yaml
5. causal_forest.yaml

# 각 파일에서 URI 스킴 제거 + 순수 논리 경로로 변경
```

#### **C. 레거시 호환성 유지**
```python
# Factory에서 하위 호환성 보장
# 기존 URI 스킴 방식도 일정 기간 지원 (deprecation warning)
def create_data_adapter_legacy(self, scheme: str) -> BaseAdapter:
    logger.warning(f"DEPRECATED: URI 스킴 기반 어댑터 생성 (scheme: {scheme})")
    # 기존 방식 지원
```

### **⚙️ Phase 3.2: 시스템 완전성 검증 (Day 13)**
*외부 인프라 불필요*

#### **A. 환경별 전환 테스트**
```bash
# 동일 Recipe로 환경별 테스트
RECIPE="models/classification/random_forest_classifier"

# LOCAL → DEV 전환
export APP_ENV=local
python main.py train --recipe-file "$RECIPE"
RUN_ID_LOCAL=$(python -c "import mlflow; print(mlflow.active_run().info.run_id)")

export APP_ENV=dev
python main.py train --recipe-file "$RECIPE"
RUN_ID_DEV=$(python -c "import mlflow; print(mlflow.active_run().info.run_id)")

# 두 환경에서 동일한 Wrapped Artifact 구조 확인
python -c "
import mlflow
local_model = mlflow.pyfunc.load_model(f'runs:/{RUN_ID_LOCAL}/model')
dev_model = mlflow.pyfunc.load_model(f'runs:/{RUN_ID_DEV}/model')
print('✅ 환경별 Wrapped Artifact 구조 동일')
"
```

#### **B. Trainer 이원적 지혜 검증**
```bash
# 하이퍼파라미터 자동 최적화 테스트
python main.py train --recipe-file "models/classification/xgboost_classifier"
# 예상 결과: Optuna 기반 자동 최적화 + 완전한 투명성 메타데이터

# 고정 하이퍼파라미터 테스트
python main.py train --recipe-file "local_classification_test"
# 예상 결과: 고정 파라미터 + 기존 워크플로우 유지

# 최적화 메타데이터 확인
python -c "
import mlflow
model = mlflow.pyfunc.load_model('runs:/latest/model')
print(model.unwrap_python_model().hyperparameter_optimization)
print(model.unwrap_python_model().training_methodology)
"
```

#### **C. 완전한 재현성 검증**
```bash
# 동일 Recipe로 다중 실행
for i in {1..3}; do
  python main.py train --recipe-file "local_classification_test"
done

# 모든 실행 결과 동일성 확인
python -c "
import mlflow
runs = mlflow.search_runs(experiment_ids=['0'], order_by=['start_time DESC'])
print('✅ 다중 실행 결과 완전 동일' if len(runs) >= 3 else '❌ 재현성 실패')
"
```

### **⚙️ Phase 3.3: MLflow 통합 완성 + 실제 Feature Store 연동 (Day 14)**
*Docker Compose 기반 최소 인프라 구성*

#### **A. 문제 상황 분석**
```yaml
현재 문제:
- DEV 환경에서 Mock 응답 사용 중
- MLflow가 params 전달 실패
- "model signature defines a params schema" 오류

원인:
- src/pipelines/train_pipeline.py:89에서 signature 미정의
- mlflow.pyfunc.log_model 호출 시 signature 파라미터 없음

해결 방향:
- MLflow signature 정의 수정 (코드 수정)
- 실제 Feature Store 연동 테스트 (간단한 Docker Compose 인프라)
```

#### **B. 단계별 실행 안내**

**Step 1: Docker 설치 확인 및 설치**
```bash
# Docker 설치 확인
docker --version

# 설치되지 않은 경우 (macOS 기준)
# 1. https://docs.docker.com/desktop/install/mac-install/ 접속
# 2. Docker Desktop for Mac 다운로드 및 설치
# 3. 설치 후 Docker Desktop 실행
# 4. 터미널에서 확인: docker --version
```

**Step 2: mmp-local-dev repo 클론 및 Docker Compose 파일 생성**
```bash
# 상위 디렉토리로 이동
cd ..

# mmp-local-dev repo 클론
git clone https://github.com/your-org/mmp-local-dev.git

# 디렉토리 이동
cd mmp-local-dev

# 간단한 Docker Compose 파일 생성
cat > docker-compose.yml << 'EOF'
version: '3.8'

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_DB: mlpipeline
      POSTGRES_USER: mluser
      POSTGRES_PASSWORD: mlpassword
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

volumes:
  postgres_data:
EOF
```

**Step 3: 인프라 실행 및 연결 테스트**
```bash
# Docker Compose 실행
docker-compose up -d

# 서비스 상태 확인
docker-compose ps

# PostgreSQL 연결 테스트
docker-compose exec postgres psql -U mluser -d mlpipeline -c "SELECT version();"

# Redis 연결 테스트
docker-compose exec redis redis-cli ping
```

**Step 4: ML Pipeline 프로젝트로 돌아가서 코드 수정**
```bash
# ML Pipeline 프로젝트로 돌아가기
cd ../modern-ml-pipeline

# 이제 코드 수정 진행 (assistant가 안내)
# 1. src/pipelines/train_pipeline.py - signature 추가
# 2. serving/api.py - Mock 제거
```

**Step 5: 실제 Feature Store 연동 테스트**
```bash
# DEV 환경에서 PostgreSQL + Redis 연동 테스트
APP_ENV=dev python main.py train --recipe-file "dev_classification_test"

# API 서빙 실제 Feature Store 연동 테스트
APP_ENV=dev python main.py serve-api --run-id "latest"

# 다른 터미널에서 API 테스트
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"user_id": "test_user_123"}'
```

**Step 6: 정리 및 종료**
```bash
# 테스트 완료 후 인프라 종료
cd ../mmp-local-dev
docker-compose down

# 필요시 데이터 완전 삭제
docker-compose down -v
```

#### **C. 완료 기준**
```yaml
인프라 구성:
- ✅ Docker 설치 완료
- ✅ PostgreSQL, Redis 정상 실행
- ✅ 연결 테스트 성공

코드 수정:
- ✅ MLflow model signature 정의 완료
- ✅ params 전달 정상 동작 확인
- ✅ Mock 코드 완전 제거

실제 연동 테스트:
- ✅ DEV 환경에서 실제 Feature Store 연동
- ✅ API 서빙 완전 기능 동작
- ✅ PostgreSQL, Redis 실제 연결 확인
```

#### **D. 안전 장치**
```yaml
문제 발생 시 롤백:
- Docker 문제: docker-compose down → 재시작
- 연결 문제: PostgreSQL/Redis 상태 확인
- 코드 문제: Git으로 이전 상태 복원

완료 후 정리:
- docker-compose down으로 인프라 종료
- 필요시 docker-compose down -v로 데이터 완전 삭제
```

### **✅ Phase 3 완료 기준**
```yaml
Phase 3.1 - Recipe 시스템:
- ✅ 모든 핵심 Recipe URI 스킴 제거
- ✅ 순수 논리 경로만 사용
- ✅ 레거시 호환성 유지

Phase 3.2 - 시스템 완전성:
- ✅ 환경별 전환 완벽 동작
- ✅ Trainer 이원적 지혜 완전 구현
- ✅ 완전한 재현성 보장
- ✅ 9대 원칙 모두 실코드 구현

Phase 3.3 - MLflow 통합 + 실제 인프라:
- ✅ Docker 설치 및 PostgreSQL/Redis 실행
- ✅ MLflow model signature 정의 완료
- ✅ Params 전달 정상 동작
- ✅ Mock 코드 완전 제거
- ✅ 실제 Feature Store 연동 테스트

전체 검증 명령어:
# Phase 3.1 & 3.2 (외부 인프라 없음)
APP_ENV=local python main.py train --recipe-file "models/classification/random_forest_classifier"
APP_ENV=dev python main.py train --recipe-file "models/classification/random_forest_classifier"

# Phase 3.3 (Docker Compose 인프라 포함)
cd ../mmp-local-dev && docker-compose up -d
cd ../modern-ml-pipeline
APP_ENV=dev python main.py train --recipe-file "dev_classification_test"
APP_ENV=dev python main.py serve-api --run-id "latest"
```

---

## 📊 **최종 성공 지표 (Final Success Metrics)**

### **🎯 Blueprint v17.0 완성도 측정**
```yaml
9대 원칙 달성도:
1. 레시피는 논리, 설정은 인프라: 100% ✅
2. 통합 데이터 어댑터: 100% ✅
3. URI 기반 동작 및 동적 팩토리: 100% ✅
4. 순수 로직 아티팩트: 100% ✅
5. 단일 Augmenter, 컨텍스트 주입: 100% ✅
6. 자기 기술 API: 100% ✅
7. 하이브리드 통합 인터페이스: 100% ✅
8. 자동 HPO + Data Leakage 방지: 100% ✅
9. 환경별 차등적 기능 분리: 100% ✅

전체 달성도: 100% 🎉
```

### **⏱️ 환경별 실행 시간 보장**
```yaml
LOCAL 환경:
- Setup: uv sync (< 3분)
- Train: 즉시 실행 (< 2분)
- 총 시간: < 5분 ✅

DEV 환경:
- Setup: ./setup-dev-environment.sh (< 15분)
- Train: 완전한 기능 (< 10분)
- 총 시간: < 25분 ✅
```

### **🔄 실행 가능성 검증**
```yaml
필수 명령어 모두 정상 동작:
- ✅ uv sync
- ✅ python main.py train --recipe-file "local_classification_test"
- ✅ python main.py batch-inference --run-id "latest"
- ✅ python main.py evaluate --run-id "latest"
- ✅ APP_ENV=dev python main.py serve-api --run-id "latest"
```

---

## 🚨 **리스크 관리 및 Contingency Plan**

### **High Risk 요소**
```yaml
1. Python 환경 전환 (3.10 → 3.12):
   - 리스크: 의존성 호환성 문제
   - 대응: 단계적 전환 + 완전한 백업

2. 아키텍처 변경 (Pipeline 수정):
   - 리스크: 기존 기능 영향
   - 대응: 각 수정 후 즉시 테스트

3. 환경별 인프라 의존성:
   - 리스크: 외부 서비스 설정 실패
   - 대응: 각 환경별 독립적 검증
```

### **각 Phase별 롤백 계획**
```yaml
Phase 0 실패 시:
- Python 환경 롤백: pyenv local 3.10.11
- 기존 requirements.txt 복원

Phase 1 실패 시:
- Pipeline 코드 롤백: git checkout HEAD~1
- Settings 구조 복원: settings.py.backup 복원

Phase 2 실패 시:
- 환경별 독립적 롤백
- 각 환경 설정 개별 복원

Phase 3 실패 시:
- Recipe 파일 개별 롤백
- URI 스킴 방식 유지
```

---

## 💡 **최종 실행 권고사항**

### **실행 순서 (절대 변경 불가)**
1. **Phase 0 완료 후에만 Phase 1 시작**
2. **Phase 1 완료 후에만 Phase 2 시작**
3. **Phase 2 완료 후에만 Phase 3 시작**
4. **각 Phase 내에서도 순차적 실행 필수**

### **성공 보장 원칙**
```yaml
1. 실행 가능성 최우선:
   - 이론적 완성도 < 실제 실행 가능성
   - 매 단계 검증 후 다음 단계 진행

2. Blueprint 철학 준수:
   - 9대 원칙 위반 시 즉시 수정
   - 환경별 철학 완전 구현

3. 현실적 접근:
   - 이상향 추구하되 현실적 제약 고려
   - 단계적 개선을 통한 점진적 완성
```

### **최종 목표**
**"Blueprint v17.0 Automated Excellence Vision의 완전한 실현"**
- 9대 원칙 100% 실코드 구현
- 환경별 철학 완전 구현
- 실행 가능성 100% 보장
- 미래 확장성 완전 보장

이 계획을 통해 **이상향과 현실의 완벽한 조화**를 달성할 수 있을 것입니다. 🚀