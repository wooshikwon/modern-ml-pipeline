# ğŸš€ Blueprint v17.0 "Automated Excellence" ì™„ì „ êµ¬í˜„ ê³„íš (next_step.md) - v2.0 FIXED

## ğŸ’ **THE ULTIMATE MISSION: From Legacy to Excellence (í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ)**

í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ì™€ **Blueprint v17.0 "Automated Excellence Vision"** ì‚¬ì´ì˜ ì™„ì „í•œ gap ë¶„ì„ ë° **í˜¸í™˜ì„± ê²€ì¦**ì„ í†µí•´, **ì§„ì •í•œ MLOps ì—‘ì…€ëŸ°ìŠ¤**ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ **ì‹¤í–‰ ê°€ëŠ¥í•œ** ì²´ê³„ì  ë¡œë“œë§µì„ ì œì‹œí•©ë‹ˆë‹¤.

---

## ğŸ” **Phase 0: Gap Analysis & Compatibility Check (í˜„ì‹¤ ì§„ë‹¨ + í˜¸í™˜ì„± ê²€ì¦)**

### **ğŸš¨ Critical Gaps Identified (ìˆ˜ì •ëœ ë¶„ì„)**

**1. ğŸ”¥ Hyperparameter Optimization System ì™„ì „ ëˆ„ë½**
- âŒ í˜„ì¬: ê³ ì • hyperparametersë§Œ ì§€ì›
- âœ… ëª©í‘œ: **ê¸°ì¡´ Trainer ì¸í„°í˜ì´ìŠ¤ ìœ ì§€í•˜ë©´ì„œ** Optuna ê¸°ë°˜ ìë™ ìµœì í™” + Data Leakage ë°©ì§€

**2. ğŸ”¥ Settings êµ¬ì¡° í™•ì¥ í•„ìš”**  
- âŒ í˜„ì¬: `hyperparameter_tuning`, `feature_store` ì„¤ì • ì—†ìŒ
- âœ… ëª©í‘œ: **ê¸°ì¡´ Settingsì™€ í˜¸í™˜ë˜ëŠ” í™•ì¥**

**3. ğŸ”¥ Recipe êµ¬ì¡° Blueprint ë¶ˆì¼ì¹˜**
- âŒ í˜„ì¬: ê³ ì •ê°’ hyperparameters
- âœ… ëª©í‘œ: **í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€í•˜ë©´ì„œ** Dictionary í˜•ì‹ + hyperparameter_tuning ì„¹ì…˜

**4. ğŸ”¥ Config ì¸í”„ë¼ ì œì•½ ê´€ë¦¬ ëˆ„ë½**
- âŒ í˜„ì¬: hyperparameter_tuning config ì—†ìŒ
- âœ… ëª©í‘œ: **ê¸°ì¡´ config êµ¬ì¡° í™•ì¥**

**5. ğŸ”¥ Factory ë©”ì„œë“œ ëˆ„ë½**
- âŒ í˜„ì¬: feature_store_adapter, optuna_adapter ë©”ì„œë“œ ì—†ìŒ
- âœ… ëª©í‘œ: **ê¸°ì¡´ Factory íŒ¨í„´ í™•ì¥**

**6. ğŸ”¥ Data Leakage ë°©ì§€ ë©”ì»¤ë‹ˆì¦˜ ì—†ìŒ**
- âŒ í˜„ì¬: Preprocessorê°€ ì „ì²´ ë°ì´í„°ì— fit
- âœ… ëª©í‘œ: **ê¸°ì¡´ Trainer ë‚´ë¶€ì—ì„œ** Train-only fit + ê° trialë³„ ë…ë¦½ split

---

## ğŸ¯ **Phase 1: Core Architecture Revolution (Week 1-2) - í˜¸í™˜ì„± ì¤‘ì‹¬**

### **1.1 Settings êµ¬ì¡° í™•ì¥ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)**

**ğŸ“‹ ë³€ê²½ ì‘ì—…:**

**A. src/settings/settings.py ì ì§„ì  í™•ì¥**
```python
# ìƒˆë¡œìš´ Settings í´ë˜ìŠ¤ë“¤ ì¶”ê°€ (ê¸°ì¡´ ê²ƒì€ ìœ ì§€)
class HyperparameterTuningSettings(BaseModel):
    enabled: bool = False  # ê¸°ë³¸ê°’: ê¸°ì¡´ ë™ì‘ ìœ ì§€
    n_trials: int = 10
    metric: str = "accuracy"
    direction: str = "maximize"

class FeatureStoreSettings(BaseModel):
    provider: str = "dynamic"
    connection_timeout: int = 5000
    retry_attempts: int = 3
    connection_info: Dict[str, Any] = {}

# ê¸°ì¡´ ModelSettings í™•ì¥ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
class ModelSettings(BaseModel):
    class_path: str
    loader: LoaderSettings
    augmenter: Optional[AugmenterSettings] = None
    preprocessor: Optional[PreprocessorSettings] = None
    data_interface: DataInterfaceSettings
    hyperparameters: ModelHyperparametersSettings
    
    # ğŸ†• ìƒˆë¡œ ì¶”ê°€ (Optionalë¡œ í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥)
    hyperparameter_tuning: Optional[HyperparameterTuningSettings] = None
    computed: Optional[Dict[str, Any]] = None

# ê¸°ì¡´ Settings í™•ì¥ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
class Settings(BaseModel):
    environment: EnvironmentSettings
    mlflow: MlflowSettings
    serving: ServingSettings
    artifact_stores: Dict[str, ArtifactStoreSettings]
    model: ModelSettings
    
    # ğŸ†• ìƒˆë¡œ ì¶”ê°€ (Optionalë¡œ í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥)
    hyperparameter_tuning: Optional[HyperparameterTuningSettings] = None
    feature_store: Optional[FeatureStoreSettings] = None
```

**B. config/base.yaml ì ì§„ì  í™•ì¥**
```yaml
# ğŸ†• ê¸°ì¡´ ì„¤ì •ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  ìƒˆë¡œìš´ ì„¹ì…˜ ì¶”ê°€

# 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ìƒˆë¡œ ì¶”ê°€)
hyperparameter_tuning:
  enabled: false  # ê¸°ë³¸ê°’: ê¸°ì¡´ ë™ì‘ ìœ ì§€
  engine: "optuna"
  timeout: 1800  # 30ë¶„ (ì¸í”„ë¼ ì œì•½)
  pruning:
    enabled: true
    algorithm: "MedianPruner"
    n_startup_trials: 5
  parallelization:
    n_jobs: 1  # ê¸°ë³¸ê°’

# 6. Feature Store (ìƒˆë¡œ ì¶”ê°€)
feature_store:
  provider: "dynamic"
  connection_timeout: 5000
  retry_attempts: 3
  connection_info:
    redis_host: ${FEATURE_STORE_REDIS_HOST:localhost:6379}
    offline_store_uri: ${FEATURE_STORE_OFFLINE_URI:file://local/features}
```

---

### **1.2 Trainer ì•„í‚¤í…ì²˜ í™•ì¥ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)**

**ğŸª í˜¸í™˜ì„± ì¤‘ì‹¬ ì„¤ê³„**
```python
# âŒ ì˜ëª»ëœ ì ‘ê·¼ (ì¸í„°í˜ì´ìŠ¤ ë³€ê²½):
def train(self, augmented_data, recipe, config):

# âœ… ì˜¬ë°”ë¥¸ ì ‘ê·¼ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€):
def train(self, df, model, augmenter=None, preprocessor=None, context_params=None):
```

**ğŸ“‹ êµ¬ì²´ì  ì‘ì—…:**

**A. src/core/trainer.py ë‚´ë¶€ ë¡œì§ í™•ì¥ (ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)**
```python
import optuna
from typing import Optional, Dict, Any, Tuple

class Trainer(BaseTrainer):
    def __init__(self, settings: Settings):
        self.settings = settings
        logger.info("Trainerê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def train(
        self,
        df: pd.DataFrame,
        model,
        augmenter: Optional[BaseAugmenter] = None,
        preprocessor: Optional[BasePreprocessor] = None,
        context_params: Optional[Dict[str, Any]] = None,
    ) -> Tuple[Optional[BasePreprocessor], Any, Dict[str, Any]]:
        """
        ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€í•˜ë©´ì„œ ë‚´ë¶€ì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì²˜ë¦¬
        """
        logger.info("ëª¨ë¸ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
        context_params = context_params or {}

        # ğŸ†• í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì—¬ë¶€ í™•ì¸
        hyperparameter_tuning_config = self.settings.model.hyperparameter_tuning
        is_tuning_enabled = (
            hyperparameter_tuning_config and 
            hyperparameter_tuning_config.enabled and
            self.settings.hyperparameter_tuning and
            self.settings.hyperparameter_tuning.enabled
        )

        if is_tuning_enabled:
            return self._train_with_hyperparameter_optimization(
                df, model, augmenter, preprocessor, context_params
            )
        else:
            return self._train_with_fixed_hyperparameters(
                df, model, augmenter, preprocessor, context_params
            )
    
    def _train_with_hyperparameter_optimization(self, df, model, augmenter, preprocessor, context_params):
        """Optuna ê¸°ë°˜ ìë™ ìµœì í™” (ë‚´ë¶€ ë©”ì„œë“œ)"""
        
        # ê¸°ë³¸ ì„¤ì • ê²€ì¦ ë° ë°ì´í„° ë¶„í• 
        self.settings.model.data_interface.validate_required_fields()
        train_df, test_df = self._split_data(df)
        
        # í”¼ì²˜ ì¦ê°•
        if augmenter:
            logger.info("í”¼ì²˜ ì¦ê°•ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            train_df = augmenter.augment(train_df, run_mode="batch", context_params=context_params)
            test_df = augmenter.augment(test_df, run_mode="batch", context_params=context_params)
        
        # Optuna Study ìƒì„±
        study = optuna.create_study(
            direction=self.settings.model.hyperparameter_tuning.direction,
            pruner=optuna.pruners.MedianPruner()
        )
        
        def objective(trial):
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
            params = self._sample_hyperparameters(trial, self.settings.model.hyperparameters.root)
            
            # ë‹¨ì¼ í•™ìŠµ ì‹¤í–‰ (Data Leakage ë°©ì§€)
            result = self._single_training_iteration(
                train_df, params, seed=trial.number
            )
            
            # Pruning ì§€ì›
            trial.report(result['score'], step=trial.number)
            if trial.should_prune():
                raise optuna.TrialPruned()
                
            return result['score']
        
        # ìµœì í™” ì‹¤í–‰ (ì‹¤í—˜ ë…¼ë¦¬ + ì¸í”„ë¼ ì œì•½)
        study.optimize(
            objective,
            n_trials=self.settings.model.hyperparameter_tuning.n_trials,
            timeout=self.settings.hyperparameter_tuning.timeout
        )
        
        # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ
        best_params = study.best_params
        final_result = self._single_training_iteration(
            train_df, best_params, seed=42
        )
        
        # ğŸ†• ìµœì í™” ë©”íƒ€ë°ì´í„° í¬í•¨
        final_result['hyperparameter_optimization'] = {
            'enabled': True,
            'best_params': best_params,
            'best_score': study.best_value,
            'total_trials': len(study.trials),
            'optimization_time': str(study.trials[-1].datetime_complete - study.trials[0].datetime_start)
        }
        
        # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ì™€ í˜¸í™˜ë˜ëŠ” ë°˜í™˜ê°’
        return final_result['preprocessor'], final_result['model'], final_result
    
    def _train_with_fixed_hyperparameters(self, df, model, augmenter, preprocessor, context_params):
        """ê¸°ì¡´ ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°©ì‹ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)"""
        
        # ê¸°ì¡´ train ë©”ì„œë“œì˜ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        self.settings.model.data_interface.validate_required_fields()
        task_type = self.settings.model.data_interface.task_type
        
        # ë°ì´í„° ë¶„í• 
        train_df, test_df = self._split_data(df)
        
        # í”¼ì²˜ ì¦ê°•
        if augmenter:
            train_df = augmenter.augment(train_df, run_mode="batch", context_params=context_params)
            test_df = augmenter.augment(test_df, run_mode="batch", context_params=context_params)
        
        # ë°ì´í„° ì¤€ë¹„
        X_train, y_train, additional_data = self._prepare_training_data(train_df)
        X_test, y_test, _ = self._prepare_training_data(test_df)
        
        # ì „ì²˜ë¦¬ (Train-only fit for Data Leakage prevention)
        if preprocessor:
            preprocessor.fit(X_train)  # â† âœ… Data Leakage ë°©ì§€
            X_train_processed = preprocessor.transform(X_train)
            X_test_processed = preprocessor.transform(X_test)
        else:
            X_train_processed = X_train
            X_test_processed = X_test
        
        # ëª¨ë¸ í•™ìŠµ
        self._fit_model(model, X_train_processed, y_train, additional_data)
        
        # í‰ê°€
        from src.core.factory import Factory
        factory = Factory(self.settings)
        evaluator = factory.create_evaluator()
        metrics = evaluator.evaluate(model, X_test_processed, y_test, test_df)
        
        results = {
            "metrics": metrics,
            "hyperparameter_optimization": {"enabled": False}  # ğŸ†• ì¼ê´€ì„± ìœ ì§€
        }
        
        return preprocessor, model, results
    
    def _single_training_iteration(self, train_df, params, seed):
        """í•µì‹¬: Data Leakage ë°©ì§€ + ë‹¨ì¼ í•™ìŠµ ë¡œì§"""
        
        # 1. Train/Validation Split (Data Leakage ë°©ì§€)
        train_data, val_data = train_test_split(
            train_df, test_size=0.2, random_state=seed, 
            stratify=self._get_stratify_column(train_df)
        )
        
        # 2. ë™ì  ë°ì´í„° ì¤€ë¹„
        X_train, y_train, additional_data = self._prepare_training_data(train_data)
        X_val, y_val, _ = self._prepare_training_data(val_data)
        
        # 3. Preprocessor fit (Train only) â† âœ… Data Leakage ë°©ì§€
        from src.core.factory import Factory
        factory = Factory(self.settings)
        preprocessor = factory.create_preprocessor()
        
        if preprocessor:
            preprocessor.fit(X_train)  # Train ë°ì´í„°ì—ë§Œ fit
            X_train_processed = preprocessor.transform(X_train)
            X_val_processed = preprocessor.transform(X_val)
        else:
            X_train_processed = X_train
            X_val_processed = X_val
        
        # 4. Model ìƒì„± ë° í•™ìŠµ (ë™ì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©)
        model = self._create_model_with_params(self.settings.model.class_path, params)
        self._fit_model(model, X_train_processed, y_train, additional_data)
        
        # 5. í‰ê°€
        evaluator = factory.create_evaluator()
        metrics = evaluator.evaluate(model, X_val_processed, y_val, val_data)
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ (tuningì— ì‚¬ìš©)
        score = self._extract_optimization_score(metrics)
        
        return {
            'model': model,
            'preprocessor': preprocessor,
            'score': score,
            'metrics': metrics,
            'training_methodology': {
                'train_test_split_method': 'stratified',
                'preprocessing_fit_scope': 'train_only',  # Data Leakage ë°©ì§€ ì¦ëª…
                'random_state': seed
            }
        }
    
    def _create_model_with_params(self, class_path, params):
        """ë™ì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„±"""
        try:
            module_path, class_name = class_path.rsplit('.', 1)
            module = importlib.import_module(module_path)
            model_class = getattr(module, class_name)
            return model_class(**params)
        except Exception as e:
            logger.error(f"ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {class_path}, íŒŒë¼ë¯¸í„°: {params}, ì˜¤ë¥˜: {e}")
            raise ValueError(f"ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {class_path}") from e
    
    def _sample_hyperparameters(self, trial, hyperparams_config):
        """Optuna trialì„ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§"""
        sampled_params = {}
        
        for param_name, param_config in hyperparams_config.items():
            if isinstance(param_config, dict) and 'type' in param_config:
                # Dictionary í˜•ì‹ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì²˜ë¦¬
                param_type = param_config['type']
                
                if param_type == 'float':
                    low = param_config['low']
                    high = param_config['high']
                    log = param_config.get('log', False)
                    sampled_params[param_name] = trial.suggest_float(
                        param_name, low, high, log=log
                    )
                elif param_type == 'int':
                    low = param_config['low']
                    high = param_config['high']
                    sampled_params[param_name] = trial.suggest_int(
                        param_name, low, high
                    )
                elif param_type == 'categorical':
                    choices = param_config['choices']
                    sampled_params[param_name] = trial.suggest_categorical(
                        param_name, choices
                    )
            else:
                # ê³ ì •ê°’ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
                sampled_params[param_name] = param_config
        
        return sampled_params
    
    def _extract_optimization_score(self, metrics):
        """ë©”íŠ¸ë¦­ì—ì„œ ìµœì í™”ìš© ì ìˆ˜ ì¶”ì¶œ"""
        optimization_metric = self.settings.model.hyperparameter_tuning.metric
        
        if optimization_metric in metrics:
            return metrics[optimization_metric]
        else:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ì²« ë²ˆì§¸ ë©”íŠ¸ë¦­ ì‚¬ìš©
            return list(metrics.values())[0]
    
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (ë³€ê²½ ì—†ìŒ)
    def _prepare_training_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[pd.Series], Dict[str, Any]]:
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€
        pass
    
    def _fit_model(self, model, X: pd.DataFrame, y: Optional[pd.Series], additional_data: Dict[str, Any]):
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€
        pass
    
    def _split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€
        pass
```

---

### **1.3 Factory íŒ¨í„´ í™•ì¥ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)**

**ğŸ“‹ êµ¬ì²´ì  ì‘ì—…:**

**A. src/core/factory.py ë©”ì„œë“œ ì¶”ê°€**
```python
class Factory:
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ëª¨ë‘ ìœ ì§€
    
    # ğŸ†• ìƒˆë¡œìš´ ë©”ì„œë“œë“¤ ì¶”ê°€
    def create_feature_store_adapter(self):
        """í™˜ê²½ë³„ Feature Store ì–´ëŒ‘í„° ìƒì„±"""
        if not self.settings.feature_store:
            raise ValueError("Feature Store ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info("Feature Store ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        from src.utils.adapters.feature_store_adapter import FeatureStoreAdapter
        return FeatureStoreAdapter(self.settings)
    
    def create_optuna_adapter(self):
        """Optuna SDK ë˜í¼ ìƒì„±"""
        if not self.settings.hyperparameter_tuning:
            raise ValueError("Hyperparameter tuning ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info("Optuna ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        from src.utils.adapters.optuna_adapter import OptunaAdapter
        return OptunaAdapter(self.settings.hyperparameter_tuning)
    
    def create_tuning_utils(self):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìœ í‹¸ë¦¬í‹° ìƒì„±"""
        logger.info("Tuning ìœ í‹¸ë¦¬í‹°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        from src.utils.system.tuning_utils import TuningUtils
        return TuningUtils()
```

---

### **1.4 Recipe êµ¬ì¡° í™•ì¥ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)**

**ğŸ“‹ ë³€ê²½ ì‘ì—…:**

**A. recipes/*.yaml íŒŒì¼ ì ì§„ì  í™•ì¥**
```yaml
# recipes/xgboost_x_learner.yaml - v17.0 í˜¸í™˜ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
model:
  class_path: "causalml.inference.meta.XGBTRegressor"
  hyperparameters:
    # ğŸ†• Dictionary í˜•ì‹ê³¼ ê¸°ì¡´ ê³ ì •ê°’ ëª¨ë‘ ì§€ì›
    learning_rate: {type: "float", low: 0.01, high: 0.3, log: true}
    n_estimators: {type: "int", low: 50, high: 1000}
    max_depth: {type: "int", low: 3, high: 10}
    subsample: {type: "float", low: 0.5, high: 1.0}
    # ê³ ì •ê°’ë„ ê³„ì† ì§€ì›
    random_state: 42
    objective: "reg:squarederror"

# ğŸ†• í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì„¤ì • (Optional)
hyperparameter_tuning:
  enabled: true
  n_trials: 50
  metric: "roc_auc"
  direction: "maximize"

# ê¸°ì¡´ ì„¹ì…˜ë“¤ ëª¨ë‘ ìœ ì§€
loader:
  name: "campaign_users"
  source_uri: "bq://recipes/sql/loaders/user_features.sql"
  local_override_uri: "file://local/data/sample_user_features.csv"

augmenter:
  name: "point_in_time_features"
  source_uri: "bq://recipes/sql/features/user_summary.sql"
  local_override_uri: "file://local/data/sample_user_features.parquet"

preprocessor:
  name: "simple_scaler"
  params:
    criterion_col: null
    exclude_cols: ["member_id", "event_timestamp"]

data_interface:
  task_type: "causal"
  features:
    gender: "category"
    age_group: "category"
    days_since_last_visit: "numeric"
    lifetime_purchase_count: "numeric"
    avg_purchase_amount_90d: "numeric"
    avg_session_duration_30d: "numeric"
  target_col: "outcome"
  treatment_col: "grp"
  treatment_value: "treatment"
```

---

## ğŸ¯ **Phase 2: Feature Store Enhancement (Week 3-4) - ì ì§„ì  í™•ì¥**

### **2.1 ê¸°ì¡´ Augmenter í™•ì¥ (ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)**

**ğŸ“‹ êµ¬ì²´ì  ì‘ì—…:**

**A. src/utils/adapters/feature_store_adapter.py ìƒì„±**
```python
from src.interface.base_adapter import BaseAdapter
from src.settings.settings import Settings

class FeatureStoreAdapter(BaseAdapter):
    """í™˜ê²½ë³„ Feature Store í†µí•© ì–´ëŒ‘í„° (ê¸°ì¡´ Redis ì–´ëŒ‘í„° í™•ì¥)"""
    
    def __init__(self, settings: Settings):
        self.settings = settings
        self.feature_store_config = settings.feature_store
        self._init_connections()
    
    def _init_connections(self):
        """í™˜ê²½ë³„ ì—°ê²° ì´ˆê¸°í™”"""
        # ê¸°ì¡´ Redis ì–´ëŒ‘í„° í™œìš©
        from src.core.factory import Factory
        factory = Factory(self.settings)
        
        try:
            self.redis_adapter = factory.create_redis_adapter()
        except ImportError:
            logger.warning("Redis ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.redis_adapter = None
    
    def get_historical_features(self, entity_df, features):
        """ë°°ì¹˜ ëª¨ë“œ: ê¸°ì¡´ SQL ê¸°ë°˜ ë°©ì‹ ì‚¬ìš©"""
        # ê¸°ì¡´ ë°©ì‹ê³¼ í˜¸í™˜ì„± ìœ ì§€
        return entity_df  # ì„ì‹œ êµ¬í˜„
    
    def get_online_features(self, entity_keys, features):
        """ì‹¤ì‹œê°„ ëª¨ë“œ: ê¸°ì¡´ Redis ì–´ëŒ‘í„° í™œìš©"""
        if self.redis_adapter:
            return self.redis_adapter.get_features(entity_keys, features)
        else:
            return {}
    
    # BaseAdapter ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
    def read(self, source: str, params=None, **kwargs):
        return self.get_historical_features(params.get('entity_df'), params.get('features'))
    
    def write(self, df, target: str, options=None, **kwargs):
        pass
```

**B. ê¸°ì¡´ Augmenter ì ì§„ì  í™•ì¥ (ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)**
```python
# src/core/augmenter.py - ê¸°ì¡´ ì½”ë“œì— ê¸°ëŠ¥ ì¶”ê°€
class Augmenter(BaseAugmenter):
    def __init__(self, source_uri: str, settings: Settings):
        # ê¸°ì¡´ ì´ˆê¸°í™” ë¡œì§ ìœ ì§€
        self.source_uri = source_uri
        self.settings = settings
        self.sql_template_str = self._load_sql_template()
        
        # ê¸°ì¡´ ì–´ëŒ‘í„°ë“¤ ìœ ì§€
        from src.core.factory import Factory
        factory = Factory(settings)
        self.batch_adapter = factory.create_data_adapter('bq')
        
        try:
            self.redis_adapter = factory.create_redis_adapter()
        except ImportError:
            self.redis_adapter = None
        
        # ğŸ†• Feature Store ì–´ëŒ‘í„° ì¶”ê°€ (Optional)
        try:
            self.feature_store_adapter = factory.create_feature_store_adapter()
        except (ValueError, ImportError):
            self.feature_store_adapter = None
    
    # ê¸°ì¡´ augment ë©”ì„œë“œ ìœ ì§€ (ì¸í„°í˜ì´ìŠ¤ ë³€ê²½ ì—†ìŒ)
    def augment(
        self,
        data: pd.DataFrame,
        run_mode: str,
        context_params: Optional[Dict[str, Any]] = None,
        **kwargs,
    ) -> pd.DataFrame:
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€ (ë³€ê²½ ì—†ìŒ)
        if run_mode == "batch":
            return self._augment_batch(data, context_params)
        elif run_mode == "serving":
            return self._augment_realtime(data, kwargs.get("feature_store_config"))
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” Augmenter ì‹¤í–‰ ëª¨ë“œì…ë‹ˆë‹¤: {run_mode}")
    
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ëª¨ë‘ ìœ ì§€
    def _augment_batch(self, data, context_params):
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€
        logger.info(f"ë°°ì¹˜ ëª¨ë“œ í”¼ì²˜ ì¦ê°•ì„ ì‹œì‘í•©ë‹ˆë‹¤. (URI: {self.source_uri})")
        feature_df = self.batch_adapter.read(self.source_uri, params=context_params)
        return pd.merge(data, feature_df, on="member_id", how="left")
    
    def _augment_realtime(self, data, feature_store_config):
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€ (ë³€ê²½ ì—†ìŒ)
        # ... ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ
        pass
    
    # ê¸°ì¡´ augment_batch, augment_realtime ë©”ì„œë“œë“¤ë„ ëª¨ë‘ ìœ ì§€
    def augment_batch(self, data, sql_snapshot, context_params=None):
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€
        pass
    
    def augment_realtime(self, data, sql_snapshot, feature_store_config=None, feature_columns=None):
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€
        pass
```

---

## ğŸ¯ **Phase 3: Wrapped Artifact Enhancement (Week 5) - ì ì§„ì  í™•ì¥**

### **3.1 PyfuncWrapper ì ì§„ì  í™•ì¥ (í˜¸í™˜ì„± ìœ ì§€)**

**ğŸ“‹ êµ¬ì²´ì  ì‘ì—…:**

**A. src/core/factory.pyì˜ PyfuncWrapper í™•ì¥ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)**
```python
class PyfuncWrapper(mlflow.pyfunc.PythonModel):
    def __init__(
        self,
        trained_model,
        trained_preprocessor: Optional[BasePreprocessor],
        trained_augmenter: BaseAugmenter,
        loader_sql_snapshot: str,
        augmenter_sql_snapshot: str,  # ê¸°ì¡´ ì´ë¦„ ìœ ì§€
        recipe_yaml_snapshot: str,
        training_metadata: Dict[str, Any],
        # ğŸ†• ìƒˆë¡œìš´ ì¸ìë“¤ (Optionalë¡œ í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥)
        model_class_path: Optional[str] = None,
        hyperparameter_optimization: Optional[Dict[str, Any]] = None,
        training_methodology: Optional[Dict[str, Any]] = None,
    ):
        # ê¸°ì¡´ ì†ì„±ë“¤ ìœ ì§€
        self.trained_model = trained_model
        self.trained_preprocessor = trained_preprocessor
        self.trained_augmenter = trained_augmenter
        self.loader_sql_snapshot = loader_sql_snapshot
        self.augmenter_sql_snapshot = augmenter_sql_snapshot
        self.recipe_yaml_snapshot = recipe_yaml_snapshot
        self.training_metadata = training_metadata
        
        # ğŸ†• ìƒˆë¡œìš´ ì†ì„±ë“¤ (Optional)
        self.model_class_path = model_class_path
        self.hyperparameter_optimization = hyperparameter_optimization or {"enabled": False}
        self.training_methodology = training_methodology or {}
    
    # ê¸°ì¡´ predict ë©”ì„œë“œ ìœ ì§€ (ë³€ê²½ ìµœì†Œí™”)
    def predict(self, context, model_input, params=None):
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€í•˜ë˜ ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° í™œìš©
        params = params or {}
        run_mode = params.get("run_mode", "serving")
        return_intermediate = params.get("return_intermediate", False)

        logger.info(f"PyfuncWrapper.predict ì‹¤í–‰ ì‹œì‘ (ëª¨ë“œ: {run_mode})")

        # 1. í”¼ì²˜ ì¦ê°• (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        if run_mode == "batch":
            augmented_df = self.trained_augmenter.augment_batch(
                model_input, 
                sql_snapshot=self.augmenter_sql_snapshot,
                context_params=params.get("context_params", {})
            )
        else:
            augmented_df = self.trained_augmenter.augment_realtime(
                model_input,
                sql_snapshot=self.augmenter_sql_snapshot,
                feature_store_config=params.get("feature_store_config"),
                feature_columns=params.get("feature_columns")
            )

        # 2. ì „ì²˜ë¦¬ (Data Leakage ë°©ì§€ ë³´ì¥)
        if self.trained_preprocessor:
            preprocessed_df = self.trained_preprocessor.transform(augmented_df)
        else:
            preprocessed_df = augmented_df

        # 3. ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ëª¨ë¸ë¡œ ì˜ˆì¸¡
        predictions = self.trained_model.predict(preprocessed_df)

        # 4. ê²°ê³¼ ì •ë¦¬ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        results_df = model_input.merge(
            pd.DataFrame(predictions, index=model_input.index, columns=["uplift_score"]),
            left_index=True,
            right_index=True,
        )
        
        if return_intermediate:
            return {
                "final_predictions": results_df,
                "augmented_data": augmented_df,
                "preprocessed_data": preprocessed_df,
                "hyperparameter_optimization": self.hyperparameter_optimization,  # ğŸ†• ë©”íƒ€ë°ì´í„° í¬í•¨
            }
        
        return results_df
```

**B. Factoryì˜ create_pyfunc_wrapper í™•ì¥**
```python
def create_pyfunc_wrapper(
    self, 
    trained_model, 
    trained_preprocessor: Optional[BasePreprocessor],
    training_results: Optional[Dict[str, Any]] = None  # ğŸ†• Trainer ê²°ê³¼ ì „ë‹¬
) -> PyfuncWrapper:
    """
    ì™„ì „í•œ Wrapped Artifact ìƒì„± (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€í•˜ë©´ì„œ í™•ì¥)
    """
    logger.info("ì™„ì „í•œ Wrapped Artifact ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # ê¸°ì¡´ ë¡œì§ ìœ ì§€
    trained_augmenter = self.create_augmenter()
    loader_sql_snapshot = self._create_loader_sql_snapshot()
    augmenter_sql_snapshot = self._create_augmenter_sql_snapshot()
    recipe_yaml_snapshot = self._create_recipe_yaml_snapshot()
    training_metadata = self._create_training_metadata()
    
    # ğŸ†• ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° (Optional)
    model_class_path = self.settings.model.class_path
    hyperparameter_optimization = None
    training_methodology = None
    
    if training_results:
        hyperparameter_optimization = training_results.get('hyperparameter_optimization')
        training_methodology = training_results.get('training_methodology')
    
    # í™•ì¥ëœ Wrapper ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
    return PyfuncWrapper(
        trained_model=trained_model,
        trained_preprocessor=trained_preprocessor,
        trained_augmenter=trained_augmenter,
        loader_sql_snapshot=loader_sql_snapshot,
        augmenter_sql_snapshot=augmenter_sql_snapshot,
        recipe_yaml_snapshot=recipe_yaml_snapshot,
        training_metadata=training_metadata,
        # ğŸ†• ìƒˆë¡œìš´ ì¸ìë“¤
        model_class_path=model_class_path,
        hyperparameter_optimization=hyperparameter_optimization,
        training_methodology=training_methodology,
    )
```

**C. train_pipeline.py ìˆ˜ì • (ìµœì†Œ ë³€ê²½)**
```python
def run_training(settings: Settings, context_params: Optional[Dict[str, Any]] = None):
    # ê¸°ì¡´ ë¡œì§ ëŒ€ë¶€ë¶„ ìœ ì§€
    
    # 3. ëª¨ë¸ í•™ìŠµ (ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)
    trainer = Trainer(settings=settings)
    trained_preprocessor, trained_model, training_results = trainer.train(  # â† training_results í™œìš©
        df=df,
        model=model,
        augmenter=augmenter,
        preprocessor=preprocessor,
        context_params=context_params,
    )
    
    # 4. ê²°ê³¼ ë¡œê¹… (í™•ì¥)
    if 'metrics' in training_results:
        mlflow.log_metrics(training_results['metrics'])
    
    # ğŸ†• í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê²°ê³¼ ë¡œê¹…
    if 'hyperparameter_optimization' in training_results:
        hpo_result = training_results['hyperparameter_optimization']
        if hpo_result['enabled']:
            mlflow.log_params(hpo_result['best_params'])
            mlflow.log_metric('best_score', hpo_result['best_score'])
            mlflow.log_metric('total_trials', hpo_result['total_trials'])

    # 5. í™•ì¥ëœ PyfuncWrapper ìƒì„±
    pyfunc_wrapper = factory.create_pyfunc_wrapper(
        trained_model=trained_model,
        trained_preprocessor=trained_preprocessor,
        training_results=training_results,  # ğŸ†• ê²°ê³¼ ì „ë‹¬
    )
    
    # ê¸°ì¡´ ì €ì¥ ë¡œì§ ìœ ì§€
    mlflow.pyfunc.log_model(
        artifact_path="model",
        python_model=pyfunc_wrapper,
        description=f"ìë™ ìµœì í™” ëª¨ë¸ '{settings.model.computed['run_name']}'",
    )
```

---

## ğŸ¯ **Phase 4-6: ë‚˜ë¨¸ì§€ êµ¬í˜„ (Week 6-8) - ê¸°ì¡´ ê³„íš ìœ ì§€**

### **4.1 API Self-Description (Week 6)**
- ê¸°ì¡´ serving/api.py í™•ì¥ (ì¸í„°í˜ì´ìŠ¤ ìœ ì§€)
- SQL íŒŒì‹± ìœ í‹¸ë¦¬í‹° í™•ì¥

### **5.1 Testing & Documentation (Week 7)**
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í…ŒìŠ¤íŠ¸
- í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì¶”ê°€

### **6.1 Example Recipes & Documentation (Week 8)**
- 23ê°œ ëª¨ë¸ íŒ¨í‚¤ì§€ ì˜ˆì‹œ
- í•˜ìœ„ í˜¸í™˜ì„± ê°€ì´ë“œ

---

## ğŸ¯ **Final Validation: í˜¸í™˜ì„± ì¤‘ì‹¬ ì²´í¬ë¦¬ìŠ¤íŠ¸**

### **âœ… í˜¸í™˜ì„± ë³´ì¥**
1. **âœ… ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ 100% ìœ ì§€**
   - Trainer.train() ì‹œê·¸ë‹ˆì²˜ ë³€ê²½ ì—†ìŒ
   - Augmenter.augment() ì‹œê·¸ë‹ˆì²˜ ë³€ê²½ ì—†ìŒ
   - PyfuncWrapper ìƒì„±ì í•˜ìœ„ í˜¸í™˜ì„±

2. **âœ… ì ì§„ì  í™•ì¥**
   - ëª¨ë“  ìƒˆë¡œìš´ ê¸°ëŠ¥ì€ Optional
   - ê¸°ì¡´ ë™ì‘ì€ enabled=falseë¡œ ìœ ì§€
   - ì„¤ì • íŒŒì¼ í•˜ìœ„ í˜¸í™˜ì„±

3. **âœ… ì‹¤í–‰ ê°€ëŠ¥ì„±**
   - í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ì™€ 100% í˜¸í™˜
   - ë‹¨ê³„ë³„ ë…ë¦½ì  êµ¬í˜„ ê°€ëŠ¥
   - í…ŒìŠ¤íŠ¸ ì½”ë“œ ì˜í–¥ ìµœì†Œí™”

### **ğŸ¯ ì„±ê³¼ ì§€í‘œ (ìˆ˜ì •ë¨)**
- **í˜¸í™˜ì„±**: ê¸°ì¡´ ì½”ë“œ 100% ë™ì‘ ë³´ì¥
- **í™•ì¥ì„±**: ìƒˆë¡œìš´ ê¸°ëŠ¥ ì ì§„ì  í™œì„±í™” ê°€ëŠ¥
- **ì•ˆì •ì„±**: ì‹¤í—˜ì  ê¸°ëŠ¥ì€ opt-in ë°©ì‹

---

## ğŸš€ **Implementation Timeline (ìˆ˜ì •ë¨)**

| Week | Phase | í˜¸í™˜ì„± ì¤‘ì‹¬ Deliverables |
|------|-------|-------------|
| 1-2 | Settings & Trainer | ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€í•˜ë©´ì„œ ë‚´ë¶€ ë¡œì§ í™•ì¥ |
| 3-4 | Feature Store | ê¸°ì¡´ Augmenter ì ì§„ì  í™•ì¥ |
| 5 | Wrapped Artifact | ê¸°ì¡´ PyfuncWrapper í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€í•˜ë©´ì„œ í™•ì¥ |
| 6 | API Enhancement | ê¸°ì¡´ API ì ì§„ì  ê°œì„  |
| 7 | Testing | í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ + ìƒˆë¡œìš´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ |
| 8 | Documentation | í•˜ìœ„ í˜¸í™˜ì„± ê°€ì´ë“œ + ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ |

**ğŸ¯ ë§ˆì¼ìŠ¤í†¤ ê²€ì¦ (ìˆ˜ì •ë¨):**
- Week 2: ê¸°ì¡´ í…ŒìŠ¤íŠ¸ 100% í†µê³¼í•˜ë©´ì„œ ì²« ë²ˆì§¸ ìë™ ìµœì í™” ì„±ê³µ
- Week 4: ê¸°ì¡´ API 100% ë™ì‘í•˜ë©´ì„œ Feature Store í™•ì¥ ì„±ê³µ
- Week 8: ì™„ì „í•œ í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥í•˜ë©´ì„œ Blueprint v17.0 êµ¬í˜„ ì™„ë£Œ

---

**ğŸ† THE ULTIMATE RESULT: í˜¸í™˜ì„± ë³´ì¥í•˜ëŠ” ì ì§„ì  MLOps ì—‘ì…€ëŸ°ìŠ¤!**

ì´ **ìˆ˜ì •ëœ ê³„íš**ì„ í†µí•´:
- ğŸ”„ **ê¸°ì¡´ ì½”ë“œ 100% í˜¸í™˜ì„± ë³´ì¥**
- ğŸ¤– **ì ì§„ì  ìë™í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**
- ğŸ›¡ï¸ **ì•ˆì „í•œ Data Leakage ë°©ì§€**
- ğŸ¯ **ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¨ê³„ë³„ êµ¬í˜„**

**Blueprint v17.0 "Automated Excellence Vision" - í˜¸í™˜ì„± ë³´ì¥ ì™„ë£Œ! ğŸ‰**