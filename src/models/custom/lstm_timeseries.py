# src/models/custom/lstm_timeseries.py
"""
LSTM TimeSeries ëª¨ë¸ - BaseModel ì§ì ‘ ìƒì†

PyTorch ê¸°ë°˜ LSTMì„ ì‚¬ìš©í•œ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì…ë‹ˆë‹¤.
DeepLearning DataHandlerì™€ ì™„ì „ í†µí•©ë˜ì–´ 3D ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from typing import Any, Optional, Dict
from src.interface import BaseModel
from src.utils.system.logger import logger
from .pytorch_utils import get_device, create_dataloader, train_pytorch_model, predict_with_pytorch_model, set_seed


class LSTMTimeSeries(BaseModel):
    """LSTM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ - BaseModel ì§ì ‘ ìƒì†"""
    
    handles_own_preprocessing = True
    
    def __init__(self, 
                 hidden_dim: int = 64, 
                 num_layers: int = 2, 
                 dropout: float = 0.2,
                 epochs: int = 100, 
                 batch_size: int = 32, 
                 learning_rate: float = 0.001,
                 early_stopping_patience: int = 10,
                 bidirectional: bool = False,
                 **kwargs):
        """
        LSTM TimeSeries ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            hidden_dim: LSTM ì€ë‹‰ì¸µ ì°¨ì› ìˆ˜
            num_layers: LSTM ë ˆì´ì–´ ìˆ˜
            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
            learning_rate: í•™ìŠµë¥ 
            early_stopping_patience: ì¡°ê¸° ì¢…ë£Œ patience
            bidirectional: ì–‘ë°©í–¥ LSTM ì‚¬ìš© ì—¬ë¶€
        """
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.dropout = dropout
        self.epochs = epochs
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.early_stopping_patience = early_stopping_patience
        self.bidirectional = bidirectional
        
        # ë‚´ë¶€ ìƒíƒœ
        self.device = get_device()
        self.model = None
        self.is_fitted = False
        self.sequence_info = None  # ì‹œí€€ìŠ¤ ë©”íƒ€ë°ì´í„° ì €ì¥
        
        logger.info(f"ğŸ§  LSTM TimeSeries ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   Hidden Dim: {hidden_dim}, Layers: {num_layers}, Dropout: {dropout}")
        logger.info(f"   Epochs: {epochs}, Batch Size: {batch_size}, LR: {learning_rate}")
        logger.info(f"   Bidirectional: {bidirectional}, Device: {self.device}")
    
    def fit(self, X: pd.DataFrame, y: pd.Series = None, **kwargs: Any) -> 'LSTMTimeSeries':
        """
        LSTM ëª¨ë¸ í•™ìŠµ
        
        Args:
            X: ì‹œí€€ìŠ¤ ë°ì´í„°ê°€ flattenëœ DataFrame 
            y: íƒ€ê²Ÿ Series
            **kwargs: ë©”íƒ€ë°ì´í„° (original_sequence_shape ë“±)
            
        Returns:
            í•™ìŠµëœ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        """
        logger.info(f"ğŸ”¥ LSTM TimeSeries í•™ìŠµ ì‹œì‘")
        
        # ì‹œë“œ ì„¤ì • (ì¬í˜„ì„±)
        set_seed(42)
        
        # 1. ë©”íƒ€ë°ì´í„°ì—ì„œ ì‹œí€€ìŠ¤ ì •ë³´ ë³µì›
        self.sequence_info = self._extract_sequence_info(X, kwargs)
        X_3d = self._reconstruct_3d_sequences(X, self.sequence_info)
        
        logger.info(f"ğŸ“ ì‹œí€€ìŠ¤ ë³µì› ì™„ë£Œ: {X.shape} â†’ {X_3d.shape}")
        
        # 2. ë°ì´í„° ê²€ì¦
        self._validate_sequence_data(X_3d, y)
        
        # 3. Train/Validation ë¶„í•  (ì‹œê³„ì—´ì´ë¯€ë¡œ ì‹œê°„ ìˆœì„œ ìœ ì§€)
        X_train, X_val, y_train, y_val = self._time_based_split(X_3d, y)
        
        # 4. ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬ì¶•
        n_samples, seq_len, n_features = X_3d.shape
        self.model = self._build_lstm_model(n_features).to(self.device)
        
        logger.info(f"ğŸ—ï¸  LSTM ì•„í‚¤í…ì²˜ êµ¬ì¶• ì™„ë£Œ: {n_features} features â†’ LSTM({self.hidden_dim}) â†’ Linear(1)")
        
        # 5. DataLoader ìƒì„±  
        train_loader = create_dataloader(X_train, y_train, self.batch_size, shuffle=False)  # ì‹œê³„ì—´ì€ ìˆœì„œ ìœ ì§€
        val_loader = create_dataloader(X_val, y_val, self.batch_size, shuffle=False) if X_val is not None else None
        
        # 6. LSTM í•™ìŠµ
        history = train_pytorch_model(
            model=self.model,
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=self.epochs,
            learning_rate=self.learning_rate,
            device=self.device,
            early_stopping_patience=self.early_stopping_patience,
            log_interval=max(1, self.epochs // 10)  # 10ë²ˆ ì •ë„ ë¡œê·¸ ì¶œë ¥
        )
        
        logger.info(f"âœ… LSTM í•™ìŠµ ì™„ë£Œ - Best Epoch: {history.get('best_epoch', 0)}")
        logger.info(f"   Final Train Loss: {history['train_loss'][-1]:.4f}")
        if history['val_loss']:
            logger.info(f"   Final Val Loss: {history['val_loss'][-1]:.4f}")
        
        self.is_fitted = True
        return self
    
    def predict(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        LSTM ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            X: ì˜ˆì¸¡í•  ì‹œí€€ìŠ¤ ë°ì´í„° (DataFrame)
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ DataFrame
        """
        if not self.is_fitted:
            raise RuntimeError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        logger.info(f"ğŸ”® LSTM ì˜ˆì¸¡ ì‹œì‘: {X.shape}")
        
        # 1. 3D ì‹œí€€ìŠ¤ë¡œ ë³µì›
        X_3d = self._reconstruct_3d_sequences(X, self.sequence_info)
        logger.info(f"ğŸ“ ì‹œí€€ìŠ¤ ë³µì›: {X.shape} â†’ {X_3d.shape}")
        
        # 2. DataLoader ìƒì„± (y ì—†ì´)
        test_loader = create_dataloader(X_3d, y=None, batch_size=self.batch_size, shuffle=False)
        
        # 3. ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = predict_with_pytorch_model(self.model, test_loader, self.device)
        
        # 4. DataFrameìœ¼ë¡œ ë°˜í™˜ (BaseModel ì¸í„°í˜ì´ìŠ¤ ì¤€ìˆ˜)
        result = pd.DataFrame(predictions, index=X.index, columns=['prediction'])
        
        logger.info(f"âœ… LSTM ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ ì˜ˆì¸¡ê°’")
        
        return result
    
    def _extract_sequence_info(self, X: pd.DataFrame, kwargs: Dict) -> Dict:
        """ë©”íƒ€ë°ì´í„°ì—ì„œ ì‹œí€€ìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        if 'original_sequence_shape' in kwargs:
            # DeepLearning DataHandlerì—ì„œ ì „ë‹¬ëœ ë©”íƒ€ë°ì´í„° ì‚¬ìš©
            original_shape = kwargs['original_sequence_shape']
            return {
                'original_shape': original_shape,
                'sequence_length': original_shape[1],
                'n_features': original_shape[2],
                'from_datahandler': True
            }
        else:
            # Fallback: DataFrameì—ì„œ ì§ì ‘ ì¶”ë¡  (í…ŒìŠ¤íŠ¸ë‚˜ ë‹¤ë¥¸ ê²½ë¡œì—ì„œ ì‚¬ìš©)
            logger.warning("âš ï¸  ë©”íƒ€ë°ì´í„°ê°€ ì—†ì–´ DataFrameì—ì„œ ì§ì ‘ ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤")
            return self._infer_sequence_info_from_dataframe(X)
    
    def _infer_sequence_info_from_dataframe(self, X: pd.DataFrame) -> Dict:
        """DataFrameì—ì„œ ì‹œí€€ìŠ¤ ì •ë³´ ì¶”ë¡  (Fallback)"""
        # ì»¬ëŸ¼ëª… íŒ¨í„´ì—ì„œ ì¶”ë¡ : seq0_feat0, seq0_feat1, ..., seq1_feat0, ...
        if X.columns[0].startswith('seq') and '_feat' in X.columns[0]:
            # ì‹œí€€ìŠ¤ ê¸¸ì´ì™€ íŠ¹ì„± ê°œìˆ˜ ì¶”ë¡ 
            max_seq = max(int(col.split('_')[0][3:]) for col in X.columns) + 1  # seq0, seq1, ...
            max_feat = max(int(col.split('_')[1][4:]) for col in X.columns) + 1  # feat0, feat1, ...
            
            return {
                'original_shape': (len(X), max_seq, max_feat),
                'sequence_length': max_seq,
                'n_features': max_feat,
                'from_datahandler': False
            }
        else:
            # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ê¸°ë³¸ê°’ ì‚¬ìš©
            logger.warning("âš ï¸  ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ ì¶”ë¡ í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤")
            n_cols = len(X.columns)
            # ê°€ì •: sequence_length=10, ë‚˜ë¨¸ì§€ëŠ” features
            seq_len = 10
            n_feat = n_cols // seq_len
            return {
                'original_shape': (len(X), seq_len, n_feat),
                'sequence_length': seq_len,
                'n_features': n_feat,
                'from_datahandler': False
            }
    
    def _reconstruct_3d_sequences(self, X: pd.DataFrame, seq_info: Dict) -> np.ndarray:
        """DataFrameì„ 3D ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ ë³µì›"""
        original_shape = seq_info['original_shape']
        stored_n_samples, seq_len, n_features = original_shape
        
        # ì‹¤ì œ ì…ë ¥ ë°ì´í„°ì˜ í¬ê¸° ì‚¬ìš© (trainê³¼ predictì—ì„œ ìƒ˜í”Œ ìˆ˜ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        actual_n_samples = len(X)
        expected_features = seq_len * n_features
        
        # DataFrame shape ê²€ì¦
        if X.shape[1] != expected_features:
            raise ValueError(f"ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì„± ìˆ˜ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                           f"ê¸°ëŒ€ê°’: {expected_features} (seq_len={seq_len} Ã— n_features={n_features}), "
                           f"ì‹¤ì œê°’: {X.shape[1]}")
        
        # DataFrameì„ numpy arrayë¡œ ë³€í™˜ í›„ reshape
        X_flat = X.values  # (actual_n_samples, seq_len * n_features)
        X_3d = X_flat.reshape(actual_n_samples, seq_len, n_features)
        
        return X_3d.astype(np.float32)
    
    def _validate_sequence_data(self, X: np.ndarray, y: pd.Series):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ê²€ì¦"""
        if len(X.shape) != 3:
            raise ValueError(f"3D ì‹œí€€ìŠ¤ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {X.shape}")
        
        if len(X) != len(y):
            raise ValueError(f"Xì™€ yì˜ ê¸¸ì´ê°€ ë‹¤ë¦…ë‹ˆë‹¤: {len(X)} vs {len(y)}")
        
        if X.shape[0] < 10:
            logger.warning(f"âš ï¸  í•™ìŠµ ë°ì´í„°ê°€ ì ìŠµë‹ˆë‹¤: {X.shape[0]}ê°œ. ìµœì†Œ 10ê°œ ì´ìƒ ê¶Œì¥")
    
    def _time_based_split(self, X: np.ndarray, y: pd.Series, val_ratio: float = 0.2):
        """ì‹œê³„ì—´ ë°ì´í„°ì˜ ì‹œê°„ ê¸°ì¤€ ë¶„í• """
        n_samples = len(X)
        split_idx = int(n_samples * (1 - val_ratio))
        
        if split_idx < 5:  # ë„ˆë¬´ ì ìœ¼ë©´ validation ìƒëµ
            logger.warning(f"âš ï¸  ë°ì´í„°ê°€ ì ì–´ validation splitì„ ìƒëµí•©ë‹ˆë‹¤: {n_samples}ê°œ")
            return X, None, y.values, None
        
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y.iloc[:split_idx].values, y.iloc[split_idx:].values
        
        logger.info(f"ğŸ“Š ì‹œê°„ ê¸°ì¤€ ë¶„í• : Train({len(X_train)}) / Val({len(X_val)})")
        
        return X_train, X_val, y_train, y_val
    
    def _build_lstm_model(self, input_size: int) -> nn.Module:
        """LSTM ì•„í‚¤í…ì²˜ ì •ì˜"""
        class LSTMNet(nn.Module):
            def __init__(self, input_size, hidden_dim, num_layers, dropout, bidirectional):
                super().__init__()
                self.lstm = nn.LSTM(
                    input_size=input_size,
                    hidden_size=hidden_dim,
                    num_layers=num_layers,
                    dropout=dropout if num_layers > 1 else 0,  # Single layerë©´ dropout=0
                    batch_first=True,
                    bidirectional=bidirectional
                )
                
                # Linear layer input size
                lstm_output_size = hidden_dim * 2 if bidirectional else hidden_dim
                self.fc = nn.Linear(lstm_output_size, 1)
                self.dropout = nn.Dropout(dropout)
                
            def forward(self, x):
                # LSTM forward
                lstm_out, (hidden, cell) = self.lstm(x)
                # ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ ì‚¬ìš©
                last_output = lstm_out[:, -1, :]  # (batch_size, hidden_dim)
                # Dropout + Linear
                output = self.dropout(last_output)
                output = self.fc(output)
                return output
        
        return LSTMNet(
            input_size=input_size,
            hidden_dim=self.hidden_dim,
            num_layers=self.num_layers,
            dropout=self.dropout,
            bidirectional=self.bidirectional
        )
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if not self.is_fitted:
            return {"status": "not_fitted"}
        
        from .pytorch_utils import count_parameters
        param_info = count_parameters(self.model)
        
        return {
            "status": "fitted",
            "architecture": "LSTM",
            "hyperparameters": {
                "hidden_dim": self.hidden_dim,
                "num_layers": self.num_layers,
                "dropout": self.dropout,
                "bidirectional": self.bidirectional,
                "learning_rate": self.learning_rate
            },
            "sequence_info": self.sequence_info,
            "model_parameters": param_info,
            "device": str(self.device)
        }