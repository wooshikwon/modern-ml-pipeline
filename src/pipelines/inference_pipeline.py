"""
Inference Pipeline - Batch and Realtime Inference
"""
import pandas as pd
import mlflow
from datetime import datetime
from typing import Dict, Any, Optional

from src.factory import Factory
from src.utils.integrations.mlflow_integration import start_run
from src.utils.system.logger import logger
from src.settings import Settings
from src.utils.system.reproducibility import set_global_seeds


def run_batch_inference(settings: Settings, run_id: str, context_params: dict = None):
    """
    ì§€ì •ëœ Run IDì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    context_params = context_params or {}

    # ì¬í˜„ì„±ì„ ìœ„í•œ ì „ì—­ ì‹œë“œ ì„¤ì • (ë ˆì‹œí”¼ ì‹œë“œê°€ ì—†ìœ¼ë©´ 42)
    seed = getattr(settings.recipe.model, 'computed', {}).get('seed', 42)
    set_global_seeds(seed)

    # 1. MLflow ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ì‹œì‘
    with start_run(settings, run_name=f"batch_inference_{run_id}") as run:
        # 2. ëª¨ë¸ ë¡œë“œ
        model_uri = f"runs:/{run_id}/model"
        logger.info(f"MLflow ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_uri}")
        model = mlflow.pyfunc.load_model(model_uri)
        
        # 3. ë°ì´í„° ë¡œë”© - ğŸ†• Phase 3: ë³´ì•ˆ ê°•í™” Dynamic SQL ì²˜ë¦¬
        # Wrapperì— ë‚´ì¥ëœ loader_sql_snapshotì„ ì‚¬ìš©
        wrapped_model = model.unwrap_python_model()
        loader_sql_template = wrapped_model.loader_sql_snapshot
        
        # Factoryë¥¼ í†µí•´ í˜„ì¬ í™˜ê²½ì— ë§ëŠ” ë°ì´í„° ì–´ëŒ‘í„° ìƒì„±
        factory = Factory(settings)
        
        # ğŸ†• Phase 3: Template SQL ë³´ì•ˆ ë Œë”ë§
        if _is_jinja_template(loader_sql_template) and context_params:
            # Jinja template + context_params â†’ ë³´ì•ˆ ê°•í™” ë™ì  ë Œë”ë§
            from src.utils.system.templating_utils import render_template_from_string
            try:
                rendered_sql = render_template_from_string(loader_sql_template, context_params)
                logger.info("âœ… ë™ì  SQL ë Œë”ë§ ì„±ê³µ (ë³´ì•ˆ ê²€ì¦ ì™„ë£Œ)")
            except ValueError as e:
                # ë³´ì•ˆ ìœ„ë°˜ ë˜ëŠ” ì˜ëª»ëœ íŒŒë¼ë¯¸í„° â†’ ëª…í™•í•œ ì—ëŸ¬
                raise ValueError(f"ë™ì  SQL ë Œë”ë§ ì‹¤íŒ¨: {e}")
                
        elif context_params:
            # ì •ì  SQL + context_params â†’ ë³´ì•ˆ ì—ëŸ¬ (ëª…í™•í•œ ì•ˆë‚´)
            raise ValueError(
                "ğŸš¨ ë³´ì•ˆ ìœ„ë°˜: ì´ ëª¨ë¸ì€ ì •ì  SQLë¡œ í•™ìŠµë˜ì–´ ë™ì  ì‹œì  ë³€ê²½ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
                "ë™ì  Batch Inferenceë¥¼ ì›í•œë‹¤ë©´ Jinja template (.sql.j2)ë¡œ í•™ìŠµí•˜ì„¸ìš”.\n"
                f"í˜„ì¬ SQL: {loader_sql_template[:100]}..."
            )
        else:
            # ì •ì  SQL + context_params ì—†ìŒ â†’ ì •ìƒ ì²˜ë¦¬
            rendered_sql = loader_sql_template
        
        data_adapter = factory.create_data_adapter(factory.model_config.loader.adapter)
        df = data_adapter.read(rendered_sql)
        
        # 4. ì˜ˆì¸¡ ì‹¤í–‰ (PyfuncWrapperê°€ ë‚´ë¶€ì ìœ¼ë¡œ ìŠ¤í‚¤ë§ˆ ê²€ì¦ì„ ìˆ˜í–‰)
        predictions_df = model.predict(df)
        
        # 5. í•µì‹¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì¶”ì ì„± ë³´ì¥)
        predictions_df['model_run_id'] = run_id  # ì‚¬ìš©ëœ ëª¨ë¸ì˜ MLflow Run ID
        predictions_df['inference_run_id'] = run.info.run_id  # í˜„ì¬ ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰ ID
        predictions_df['inference_timestamp'] = datetime.now()  # ì˜ˆì¸¡ ìˆ˜í–‰ ì‹œê°
        
        # 6. ê²°ê³¼ ì €ì¥
        storage_adapter = factory.create_data_adapter("storage")
        target_path = f"{settings.artifact_stores['prediction_results'].base_uri}/preds_{run.info.run_id}.parquet"
        storage_adapter.write(predictions_df, target_path)

        # 7. PostgreSQL ì €ì¥ (ì„¤ì •ì´ í™œì„±í™”ëœ ê²½ìš°)
        prediction_config = settings.artifact_stores['prediction_results']
        
        if hasattr(prediction_config, 'postgres_storage') and prediction_config.postgres_storage:
            postgres_config = prediction_config.postgres_storage
            
            if postgres_config.enabled:
                try:
                    # SQL ì–´ëŒ‘í„°ë¡œ PostgreSQLì— ì €ì¥
                    sql_adapter = factory.create_data_adapter("sql") 
                    table_name = postgres_config.table_name
                    
                    # DataFrameì„ PostgreSQL í…Œì´ë¸”ì— ì €ì¥ (append ëª¨ë“œ)
                    sql_adapter.write(predictions_df, table_name, if_exists='append', index=False)
                    logger.info(f"ë°°ì¹˜ ì¶”ë¡  ê²°ê³¼ë¥¼ PostgreSQL í…Œì´ë¸” '{table_name}'ì— ì €ì¥ ì™„ë£Œ ({len(predictions_df)}í–‰)")
                    
                    mlflow.log_metric("postgres_rows_saved", len(predictions_df))
                except Exception as e:
                    logger.error(f"PostgreSQL ì €ì¥ ì‹¤íŒ¨: {e}")
                    # PostgreSQL ì €ì¥ ì‹¤íŒ¨í•´ë„ íŒŒì¼ ì €ì¥ì€ ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰

        mlflow.log_artifact(target_path.replace("file://", ""))
        mlflow.log_metric("inference_row_count", len(predictions_df))


def _save_dataset(
    factory: Factory,
    df: pd.DataFrame,
    store_name: str,
    settings: Settings,
    options: Optional[Dict[str, Any]] = None,
):
    """
    Factoryë¥¼ í†µí•´ ì ì ˆí•œ ë°ì´í„° ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•˜ê³ , DataFrameì„ ì €ì¥í•©ë‹ˆë‹¤.
    (ê¸°ì¡´ artifact_utils.save_dataset ë¡œì§ì„ ì§ì ‘ êµ¬í˜„)
    """
    if df.empty:
        logger.warning(f"DataFrameì´ ë¹„ì–´ìˆì–´, '{store_name}' ì•„í‹°íŒ©íŠ¸ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    try:
        # ì˜¬ë°”ë¥¸ ì ‘ê·¼ ë°©ì‹ ì ìš©: dict['í‚¤'] -> ê²°ê³¼ëŠ” Pydantic ëª¨ë¸
        store_config = settings.artifact_stores[store_name]
    except KeyError:
        logger.error(f"'{store_name}'ì— í•´ë‹¹í•˜ëŠ” ì•„í‹°íŒ©íŠ¸ ìŠ¤í† ì–´ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        raise

    if not store_config.enabled:
        logger.info(f"'{store_name}' ì•„í‹°íŒ©íŠ¸ ìŠ¤í† ì–´ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    base_uri = store_config.base_uri
    
    # âœ… Blueprint ì›ì¹™ 3: URI ê¸°ë°˜ ë™ì‘ ë° ë™ì  íŒ©í† ë¦¬ ì™„ì „ êµ¬í˜„
    # Factoryê°€ í™˜ê²½ë³„ ë¶„ê¸°ì™€ ì–´ëŒ‘í„° ì„ íƒì„ ì „ë‹´
    adapter = factory.create_data_adapter("storage")

    # ì €ì¥ë  ìµœì¢… ê²½ë¡œ(í…Œì´ë¸”ëª… ë˜ëŠ” íŒŒì¼ëª…) ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # inferenceì—ì„œëŠ” model.nameì´ ì—†ìœ¼ë¯€ë¡œ run_id ê¸°ë°˜ìœ¼ë¡œ ì‹ë³„ì ìƒì„±
    model_identifier = "batch_inference"
    artifact_name = f"{model_identifier}_{timestamp}"
    
    # âœ… Blueprint ì›ì¹™ 3: Factoryê°€ URI í•´ì„ ì²˜ë¦¬ - ë‹¨ìˆœí•œ artifact ì´ë¦„ë§Œ ì „ë‹¬
    final_target = f"{base_uri.rstrip('/')}/{artifact_name}"

    logger.info(f"'{store_name}' ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì‹œì‘: {final_target}")
    adapter.write(df, final_target, options)
    logger.info(f"'{store_name}' ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì™„ë£Œ: {final_target}")


def _is_jinja_template(sql: str) -> bool:
    """
    ğŸ†• Phase 3: SQL ë¬¸ìì—´ì´ Jinja2 í…œí”Œë¦¿ì¸ì§€ ê°ì§€
    
    Args:
        sql: ê²€ì‚¬í•  SQL ë¬¸ìì—´
        
    Returns:
        Jinja2 í…œí”Œë¦¿ íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ True, ì•„ë‹ˆë©´ False
    """
    jinja_patterns = ['{{', '}}', '{%', '%}']
    return any(pattern in sql for pattern in jinja_patterns)