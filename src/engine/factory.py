from __future__ import annotations
import importlib
from pathlib import Path
from typing import Optional, Dict, Any, TYPE_CHECKING

import mlflow
import pandas as pd

from src.components.augmenter import Augmenter, LocalFileAugmenter, BaseAugmenter, PassThroughAugmenter
from src.components.preprocessor import BasePreprocessor, Preprocessor
from src.interface.base_adapter import BaseAdapter
from src.settings import Settings
from src.utils.system.logger import logger
from src.engine.registry import AdapterRegistry

if TYPE_CHECKING:
    from src.engine.artifact import PyfuncWrapper


class Factory:
    """
    í˜„ëŒ€í™”ëœ Recipe ì„¤ì •(settings.recipe)ì— ê¸°ë°˜í•˜ì—¬ ëª¨ë“  í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì•™ íŒ©í† ë¦¬ í´ë˜ìŠ¤.
    """
    def __init__(self, settings: Settings):
        self.settings = settings
        
        # í˜„ëŒ€í™”ëœ Recipe êµ¬ì¡° ê²€ì¦
        if not self.settings.recipe:
            raise ValueError("í˜„ëŒ€í™”ëœ Recipe êµ¬ì¡°ê°€ í•„ìš”í•©ë‹ˆë‹¤. settings.recipeê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info(f"Factory initialized with Recipe: {self.settings.recipe.name}")

    @property 
    def model_config(self):
        """í˜„ì¬ ëª¨ë¸ ì„¤ì • ë°˜í™˜ (í˜„ëŒ€í™”ëœ Recipe.model)"""
        return self.settings.recipe.model

    def create_data_adapter(self, adapter_type: str) -> BaseAdapter:
        """ë°ì´í„° ì–´ëŒ‘í„° ìƒì„±"""
        logger.debug(f"Requesting to create data adapter of type: {adapter_type}")
        return AdapterRegistry.create(adapter_type, self.settings)

    def create_augmenter(self) -> "BaseAugmenter":
        """Augmenter ìƒì„± (í™˜ê²½ë³„ ì°¨ë“± ê¸°ëŠ¥)"""
        # Blueprint ì›ì¹™ 9: í™˜ê²½ë³„ ì°¨ë“±ì  ê¸°ëŠ¥ ë¶„ë¦¬
        if self.settings.environment.app_env == "local":
            logger.info("LOCAL env: Creating PassThroughAugmenter.")
            return PassThroughAugmenter(settings=self.settings)
        else:
            logger.info("DEV/PROD env: Creating standard Augmenter.")
            return Augmenter(settings=self.settings, factory=self)

    def create_preprocessor(self) -> "BasePreprocessor":
        """ì „ì²˜ë¦¬ê¸° ìƒì„±"""
        if not self.model_config.preprocessor: 
            return None
        return Preprocessor(settings=self.settings)

    def create_model(self):
        """ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ ë™ì  ëª¨ë¸ ìƒì„±"""
        class_path = self.model_config.class_path
        try:
            module_path, class_name = class_path.rsplit('.', 1)
            module = importlib.import_module(module_path)
            model_class = getattr(module, class_name)
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì²˜ë¦¬ (í˜„ëŒ€í™”ëœ êµ¬ì¡°)
            hyperparams = self._extract_hyperparameters()
            
            logger.info(f"Creating model instance from: {class_path}")
            return model_class(**hyperparams)
        except Exception as e:
            logger.error(f"Failed to load model: {class_path}", exc_info=True)
            raise ValueError(f"Could not load model class: {class_path}") from e

    def _extract_hyperparameters(self) -> Dict[str, Any]:
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì¶œ (í˜„ëŒ€í™”ëœ í˜•ì‹ ì „ìš©)"""
        hyperparams_config = self.model_config.hyperparameters
        
        # í˜„ëŒ€í™”ëœ ModernHyperparametersSettings í˜•ì‹
        if hasattr(hyperparams_config, 'get_fixed_params'):
            return hyperparams_config.get_fixed_params()
        
        # Dict í˜•ì‹ ì§ì ‘ ì‚¬ìš©
        elif isinstance(hyperparams_config, dict):
            return hyperparams_config
        
        # RootModel í˜•ì‹ (ë ˆê±°ì‹œ í˜¸í™˜)
        elif hasattr(hyperparams_config, 'root'):
            return hyperparams_config.root
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° í˜•ì‹: {type(hyperparams_config)}")

    def create_evaluator(self):
        """task_typeì— ë”°ë¥¸ ë™ì  evaluator ìƒì„±"""
        from src.components.evaluator import (
            ClassificationEvaluator,
            RegressionEvaluator,
            ClusteringEvaluator,
            CausalEvaluator,
        )
        
        task_type = self.model_config.data_interface.task_type
        evaluator_map = {
            "classification": ClassificationEvaluator,
            "regression": RegressionEvaluator,
            "clustering": ClusteringEvaluator,
            "causal": CausalEvaluator,
        }
        
        evaluator_class = evaluator_map.get(task_type)
        if not evaluator_class:
            raise ValueError(f"Unsupported task_type for evaluator: '{task_type}'")
        
        logger.info(f"Creating evaluator for task: {task_type}")
        return evaluator_class(self.model_config.data_interface)

    def create_feature_store_adapter(self):
        """í™˜ê²½ë³„ Feature Store ì–´ëŒ‘í„° ìƒì„±"""
        if not self.settings.feature_store:
            raise ValueError("Feature Store settings are not configured.")
        logger.info("Creating Feature Store adapter.")
        return AdapterRegistry.create('feature_store', self.settings)
    
    def create_optuna_integration(self):
        """Optuna Integration ìƒì„±"""
        tuning_config = self.model_config.hyperparameter_tuning
        if not tuning_config:
            raise ValueError("Hyperparameter tuning settings are not configured.")
        
        from src.utils.integrations.optuna_integration import OptunaIntegration
        logger.info("Creating Optuna integration.")
        return OptunaIntegration(tuning_config)

    def create_tuning_utils(self):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìœ í‹¸ë¦¬í‹° ìƒì„±"""
        logger.info("Creating Tuning utils.")
        from src.utils.system.tuning_utils import TuningUtils
        return TuningUtils()

    def create_pyfunc_wrapper(
        self, 
        trained_model, 
        trained_preprocessor: Optional[BasePreprocessor],
        training_df: Optional[pd.DataFrame] = None,  # ğŸ†• Phase 5: ìŠ¤í‚¤ë§ˆ ìƒì„±ìš©
        training_results: Optional[Dict[str, Any]] = None
    ) -> PyfuncWrapper:
        """ğŸ”„ Phase 5: ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ìº¡ìŠí™”ëœ Enhanced Artifact ìƒì„±"""
        from src.engine.artifact import PyfuncWrapper
        logger.info("Creating Enhanced PyfuncWrapper artifact with Phase 5 capabilities...")
        
        # ğŸ†• Phase 5: Enhanced Signature + Schema ìƒì„± (training_dfê°€ ìˆëŠ” ê²½ìš°)
        signature = None
        data_schema = None
        schema_validator = None
        
        if training_df is not None:
            logger.info("ğŸ†• Phase 5: Enhanced Model Signature + ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„° ìƒì„± ì¤‘...")
            from src.utils.integrations.mlflow_integration import create_enhanced_model_signature_with_schema
            
            # Phase 1 Schema ì •ë³´ í†µí•© (27ê°œ Recipe ëŒ€ì‘)
            entity_schema = self.model_config.loader.entity_schema
            data_interface = self.model_config.data_interface
            
            data_interface_config = {
                # Entity + Timestamp ì •ë³´
                'entity_columns': entity_schema.entity_columns,
                'timestamp_column': entity_schema.timestamp_column,
                # ML ì‘ì—… ì •ë³´
                'task_type': data_interface.task_type,
                'target_column': data_interface.target_column,
                'treatment_column': getattr(data_interface, 'treatment_column', None),
            }
            
            # Enhanced Signature + Schema ìƒì„±
            signature, data_schema = create_enhanced_model_signature_with_schema(
                training_df, 
                data_interface_config
            )
            
            # ğŸ†• Phase 4 SchemaConsistencyValidator ìƒì„±
            from src.utils.system.schema_utils import SchemaConsistencyValidator
            schema_validator = SchemaConsistencyValidator(data_schema)
            
            logger.info("âœ… Enhanced Signature + ìŠ¤í‚¤ë§ˆ ê²€ì¦ê¸° ìƒì„± ì™„ë£Œ")
        else:
            logger.warning("âš ï¸ training_dfê°€ ì œê³µë˜ì§€ ì•Šì•„ ê¸°ë³¸ PyfuncWrapper ìƒì„± (Phase 5 ê¸°ëŠ¥ ì œí•œ)")
        
        # ğŸ”„ Enhanced PyfuncWrapper ìƒì„± (Phase 4, 5 í†µí•©)
        return PyfuncWrapper(
            # ê¸°ì¡´ ë§¤ê°œë³€ìˆ˜ë“¤ ë³´ì¡´
            trained_model=trained_model,
            trained_preprocessor=trained_preprocessor,
            trained_augmenter=self.create_augmenter(),
            loader_sql_snapshot=self._create_loader_sql_snapshot(),
            augmenter_config_snapshot=self._create_augmenter_config_snapshot(),
            recipe_yaml_snapshot=self._create_recipe_yaml_snapshot(),
            model_class_path=self.model_config.class_path,
            hyperparameter_optimization=training_results.get('hyperparameter_optimization') if training_results else None,
            training_methodology=training_results.get('training_methodology') if training_results else {},
            
            # ğŸ†• Phase 4, 5 í†µí•©: ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ì •ë³´
            data_schema=data_schema,
            schema_validator=schema_validator,
            # ğŸ†• Phase 5: Enhanced Signature ì €ì¥
            signature=signature,
        )
    
    def _create_loader_sql_snapshot(self) -> str:
        """Loader SQL ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        return self.model_config.loader.source_uri
    
    def _create_augmenter_config_snapshot(self) -> Dict[str, Any]:
        """Augmenter ì„¤ì • ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        if self.model_config.augmenter:
            return self.model_config.augmenter.model_dump()
        return {}
    
    def _create_recipe_yaml_snapshot(self) -> str:
        """Recipe YAML ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        computed_fields = self.model_config.computed
        if computed_fields:
            recipe_file = computed_fields.get("recipe_file")
            if recipe_file:
                recipe_path = Path(f"recipes/{recipe_file}.yaml")
                if recipe_path.exists():
                    return recipe_path.read_text(encoding="utf-8")
        
        logger.warning("Recipe YAML ìŠ¤ëƒ…ìƒ·ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return ""
