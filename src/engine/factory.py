from __future__ import annotations
import importlib
from typing import Optional, Dict, Any, TYPE_CHECKING

import pandas as pd

from src.components._fetcher import FetcherRegistry
from src.components._preprocessor import Preprocessor, BasePreprocessor
from src.components._adapter import AdapterRegistry
from src.components._evaluator import EvaluatorRegistry
from src.interface import BaseAdapter
from src.settings import Settings
from src.utils.system.logger import logger

if TYPE_CHECKING:
    from src.engine._artifact import PyfuncWrapper
    from src.interface import BaseFetcher


class Factory:
    """
    í˜„ëŒ€í™”ëœ Recipe ì„¤ì •(settings.recipe)ì— ê¸°ë°˜í•˜ì—¬ ëª¨ë“  í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì•™ íŒ©í† ë¦¬ í´ë˜ìŠ¤.
    """
    def __init__(self, settings: Settings):
        self.settings = settings
        
        # í˜„ëŒ€í™”ëœ Recipe êµ¬ì¡° ê²€ì¦
        if not self.settings.recipe:
            raise ValueError("í˜„ëŒ€í™”ëœ Recipe êµ¬ì¡°ê°€ í•„ìš”í•©ë‹ˆë‹¤. settings.recipeê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info(f"Factory initialized with Recipe: {self.settings.recipe.name}")

    @property 
    def model_config(self):
        """í˜„ì¬ ëª¨ë¸ ì„¤ì • ë°˜í™˜ (í˜„ëŒ€í™”ëœ Recipe.model)"""
        return self.settings.recipe.model

    def _detect_adapter_type_from_uri(self, source_uri: str) -> str:
        """
        source_uri íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì–´ëŒ‘í„° íƒ€ì…ì„ ìë™ìœ¼ë¡œ ê²°ì •í•©ë‹ˆë‹¤.
        
        íŒ¨í„´:
        - .sql íŒŒì¼ ë˜ëŠ” SQL ì¿¼ë¦¬ â†’ 'sql'
        - .csv, .parquet, .json â†’ 'storage'
        - s3://, gs://, az:// â†’ 'storage'
        - bigquery:// â†’ 'bigquery'
        """
        uri_lower = source_uri.lower()
        
        # SQL íŒ¨í„´
        if uri_lower.endswith('.sql') or 'select' in uri_lower or 'from' in uri_lower:
            return 'sql'
        
        # BigQuery íŒ¨í„´
        if uri_lower.startswith('bigquery://'):
            return 'bigquery'
        
        # Cloud Storage íŒ¨í„´
        if any(uri_lower.startswith(prefix) for prefix in ['s3://', 'gs://', 'az://']):
            return 'storage'
        
        # File íŒ¨í„´
        if any(uri_lower.endswith(ext) for ext in ['.csv', '.parquet', '.json', '.tsv']):
            return 'storage'
        
        # ê¸°ë³¸ê°’
        logger.warning(f"source_uri íŒ¨í„´ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_uri}. 'storage' ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return 'storage'
    
    def create_data_adapter(self, adapter_type: Optional[str] = None) -> "BaseAdapter":
        """
        ë°ì´í„° ì–´ëŒ‘í„° ìƒì„±. 
        1. ì¸ìë¡œ ì „ë‹¬ëœ adapter_type ìš°ì„ 
        2. Recipeì— adapter í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš© (backward compatibility)
        3. ì—†ìœ¼ë©´ source_uri íŒ¨í„´ìœ¼ë¡œ ìë™ ê°ì§€
        """
        # ìš°ì„ ìˆœìœ„ 1: ëª…ì‹œì  ì¸ì
        if adapter_type:
            target_type = adapter_type
        else:
            # Recipeì—ì„œ loader ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            loader = self.settings.recipe.data.loader
            
            # ìš°ì„ ìˆœìœ„ 2: Recipeì— adapter í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš© (backward compatibility)
            if hasattr(loader, 'adapter') and loader.adapter:
                target_type = loader.adapter
                logger.debug(f"Recipeì—ì„œ adapter íƒ€ì… ì‚¬ìš©: '{target_type}'")
            else:
                # ìš°ì„ ìˆœìœ„ 3: source_uri íŒ¨í„´ìœ¼ë¡œ ìë™ ê°ì§€
                source_uri = loader.source_uri
                target_type = self._detect_adapter_type_from_uri(source_uri)
                logger.info(f"source_uri '{source_uri}'ì—ì„œ adapter íƒ€ì… ìë™ ê°ì§€: '{target_type}'")
        
        logger.debug(f"DataAdapter ìƒì„±: type='{target_type}'")
        try:
            return AdapterRegistry.create_adapter(target_type, self.settings)
        except Exception as e:
            available = list(AdapterRegistry.list_adapters().keys())
            raise ValueError(
                f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DataAdapter íƒ€ì…ì…ë‹ˆë‹¤: '{target_type}'. ì‚¬ìš© ê°€ëŠ¥í•œ ì–´ëŒ‘í„°: {available}"
            ) from e
    
    def create_fetcher(self, run_mode: Optional[str] = None):
        """
        ì„¤ì •ê³¼ ì‹¤í–‰ ëª¨ë“œì— ë”°ë¼ ì ì ˆí•œ fetcher ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        mode = (run_mode or "batch").lower()
        env = self.settings.environment.env_name if hasattr(self.settings, "environment") else "local"
        provider = (self.settings.feature_store.provider if getattr(self.settings, "feature_store", None) else "none")
        aug_conf = getattr(self.settings.recipe.model, "fetcher", None)
        aug_type = getattr(aug_conf, "type", None) if aug_conf else None

        # servingì—ì„œëŠ” PassThrough/SqlFallback ê¸ˆì§€
        if mode == "serving":
            if aug_type in (None, "pass_through") or provider in (None, "none"):
                raise TypeError("Servingì—ì„œëŠ” pass_through/feature_store ë¯¸êµ¬ì„± ì‚¬ìš©ì´ ê¸ˆì§€ë©ë‹ˆë‹¤. Feature Store ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # local ë˜ëŠ” provider=none ë˜ëŠ” ëª…ì‹œì  pass_through â†’ PassThrough
        if env == "local" or provider == "none" or aug_type == "pass_through" or not aug_conf:
            return FetcherRegistry.create(aug_type)

        # Feature Store ìš”ì²­ + provider OK â†’ FeatureStorefetcher
        if aug_type == "feature_store" and provider in {"feast", "mock", "dynamic"}:
            return FetcherRegistry.create(aug_type, settings=self.settings, factory=self)


        raise ValueError(
            f"ì ì ˆí•œ fetcherë¥¼ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. env={env}, provider={provider}, aug_type={aug_type}, mode={mode}"
        )

    def create_preprocessor(self) -> Optional[BasePreprocessor]:
        """ì „ì²˜ë¦¬ê¸° ìƒì„±"""
        if not self.model_config.preprocessor: 
            return None
        return Preprocessor(settings=self.settings)

    def create_model(self):
        """ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ ë™ì  ëª¨ë¸ ìƒì„±"""
        class_path = self.model_config.class_path
        try:
            module_path, class_name = class_path.rsplit('.', 1)
            module = importlib.import_module(module_path)
            model_class = getattr(module, class_name)
            
            hyperparams = self.model_config.hyperparameters.get_fixed_params() if hasattr(self.model_config.hyperparameters, 'get_fixed_params') else self.model_config.hyperparameters.copy()

            for key, value in hyperparams.items():
                if isinstance(value, str) and "." in value and ("_fn" in key or "_class" in key):
                    try:
                        module_path, class_name = value.rsplit('.', 1)
                        obj_module = importlib.import_module(module_path)
                        hyperparams[key] = getattr(obj_module, class_name)
                        logger.info(f"Hyperparameter '{key}' converted to object: {value}")
                    except (ImportError, AttributeError):
                        logger.warning(f"Could not convert hyperparameter '{key}' to object: {value}. Keeping as string.")

            logger.info(f"Creating model instance from: {class_path}")
            return model_class(**hyperparams)
        except Exception as e:
            logger.error(f"Failed to load model: {class_path}", exc_info=True)
            raise ValueError(f"Could not load model class: {class_path}") from e

    def create_evaluator(self):
        """task_typeì— ë”°ë¼ ë™ì  evaluator ìƒì„±"""
        task_type = self.model_config.data_interface.task_type
        logger.info(f"Creating evaluator for task: {task_type}")
        return EvaluatorRegistry.create(task_type, self.model_config.data_interface)

    def create_feature_store_adapter(self):
        """í™˜ê²½ë³„ Feature Store ì–´ëŒ‘í„° ìƒì„±"""
        if not self.settings.feature_store:
            raise ValueError("Feature Store settings are not configured.")
        logger.info("Creating Feature Store adapter.")
        return AdapterRegistry.create_adapter('feature_store', self.settings)
    
    def create_optuna_integration(self):
        """Optuna Integration ìƒì„±"""
        tuning_config = self.model_config.hyperparameter_tuning
        if not tuning_config:
            raise ValueError("Hyperparameter tuning settings are not configured.")
        
        from src.utils.integrations.optuna_integration import OptunaIntegration
        logger.info("Creating Optuna integration.")
        return OptunaIntegration(tuning_config)

    def create_pyfunc_wrapper(
        self, 
        trained_model: Any, 
        trained_preprocessor: Optional[BasePreprocessor],
        trained_fetcher: Optional['BaseFetcher'],
        training_df: Optional[pd.DataFrame] = None,
        training_results: Optional[Dict[str, Any]] = None
    ) -> PyfuncWrapper:
        """ğŸ”„ Phase 5: ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ ìº¡ìŠí™”ëœ Enhanced Artifact ìƒì„±"""
        from src.engine._artifact import PyfuncWrapper
        logger.info("Creating PyfuncWrapper artifact...")
        
        signature, data_schema = None, None
        if training_df is not None:
            logger.info("Generating model signature and data schema from training_df...")
            from src.utils.integrations.mlflow_integration import create_enhanced_model_signature_with_schema
            
            entity_schema = self.model_config.loader.entity_schema
            data_interface = self.model_config.data_interface
            
            # í•™ìŠµ ì‹œì ì— timestamp ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„± í™•ë³´
            try:
                ts_col = entity_schema.timestamp_column
                if ts_col and ts_col in training_df.columns:
                    import pandas as pd
                    if not pd.api.types.is_datetime64_any_dtype(training_df[ts_col]):
                        training_df = training_df.copy()
                        training_df[ts_col] = pd.to_datetime(training_df[ts_col], errors='coerce')
            except Exception:
                pass

            data_interface_config = {
                'entity_columns': entity_schema.entity_columns,
                'timestamp_column': entity_schema.timestamp_column,
                'task_type': data_interface.task_type,
                'target_column': data_interface.target_column,
                'treatment_column': getattr(data_interface, 'treatment_column', None),
            }
            
            signature, data_schema = create_enhanced_model_signature_with_schema(
                training_df, 
                data_interface_config
            )
            logger.info("âœ… Signature and data schema created successfully.")
        
        return PyfuncWrapper(
            settings=self.settings,
            trained_model=trained_model,
            trained_preprocessor=trained_preprocessor,
            trained_fetcher=trained_fetcher,
            training_results=training_results,
            signature=signature,
            data_schema=data_schema,
        )
