"""
DataInterface ê¸°ë°˜ ë°ì´í„° ê²€ì¦ ëª¨ë“ˆ

Phase 5.1ì—ì„œ ë„ì…ëœ DataInterface ê¸°ë°˜ ì»¬ëŸ¼ ê²€ì¦ ë¡œì§ì„ ì œê³µí•©ë‹ˆë‹¤.
ê¸°ì¡´ schema_utils.pyë³´ë‹¤ ë‹¨ìˆœí•˜ê³  ëª…í™•í•œ DataInterface í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.

CLAUDE.md ì›ì¹™ ì¤€ìˆ˜:
- íƒ€ì… íŒíŠ¸ í•„ìˆ˜
- Google Style Docstring
- ë‹¨ì¼ ì±…ì„ ì›ì¹™
"""

from typing import List, Set, Dict, Any
import pandas as pd

from src.settings.recipe import DataInterface
from src.utils.system.logger import logger


def get_required_columns_from_data_interface(
    data_interface: DataInterface, 
    actual_training_df: pd.DataFrame = None
) -> List[str]:
    """
    DataInterfaceì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    DataInterface ê²€ì¦ ê·œì¹™ (ì¶”ë¡ ìš©):
    - entity_columns: í•­ìƒ í•„ìˆ˜ (ì–´ë–¤ entityì— ëŒ€í•œ ì˜ˆì¸¡ì¸ì§€ ì‹ë³„)
    - timestamp_column: timeseries taskì—ì„œ í•„ìˆ˜ (ì–¸ì œ ì‹œì  ì˜ˆì¸¡ì¸ì§€)
    - treatment_column: causal taskì—ì„œ í•„ìˆ˜ (ì²˜ì¹˜ ë³€ìˆ˜)
    - feature_columns: ëª…ì‹œëœ ê²½ìš° í¬í•¨, nullì¸ ê²½ìš° ì‹¤ì œ í•™ìŠµ ë°ì´í„°ì—ì„œ ìë™ ì¶”ì¶œ
    - target_column: ì¶”ë¡ ì‹œ ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ì œì™¸
    
    Args:
        data_interface: Recipeì˜ DataInterface ì„¤ì •
        actual_training_df: ì‹¤ì œ í•™ìŠµ ë°ì´í„°í”„ë ˆì„ (feature_columns=nullì¸ ê²½ìš° í•„ìš”)
        
    Returns:
        List[str]: í•„ìˆ˜ ì»¬ëŸ¼ ëª©ë¡ (ì¤‘ë³µ ì œê±°ë¨)
        
    Examples:
        >>> interface = DataInterface(
        ...     task_type="regression",
        ...     target_column="price", 
        ...     entity_columns=["user_id"],
        ...     feature_columns=["age", "income"]
        ... )
        >>> get_required_columns_from_data_interface(interface)
        ['user_id', 'age', 'income']  # target_column ì œì™¸
        
        >>> # feature_columns=nullì¸ ê²½ìš° ì‹¤ì œ ë°ì´í„°ì—ì„œ ì¶”ì¶œ
        >>> interface_null = DataInterface(
        ...     task_type="regression",
        ...     target_column="price",
        ...     entity_columns=["user_id"], 
        ...     feature_columns=None
        ... )
        >>> df = pd.DataFrame({'user_id': [1,2], 'price': [100,200], 'age': [25,30], 'income': [5000,6000]})
        >>> get_required_columns_from_data_interface(interface_null, df)
        ['user_id', 'age', 'income']  # target ì œì™¸, entity + ë‚˜ë¨¸ì§€ feature ìë™ ì¶”ì¶œ
    """
    required = []
    
    # 1. Entity ì»¬ëŸ¼ë“¤ (í•­ìƒ í•„ìˆ˜ - ì–´ë–¤ entityì— ëŒ€í•œ ì˜ˆì¸¡ì¸ì§€ ì‹ë³„)
    required.extend(data_interface.entity_columns)
    
    # 2. Taskë³„ íŠ¹ìˆ˜ ì»¬ëŸ¼ (ì¶”ë¡ ì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤)
    if data_interface.task_type == "timeseries" and data_interface.timestamp_column:
        required.append(data_interface.timestamp_column)
    elif data_interface.task_type == "causal" and data_interface.treatment_column:
        required.append(data_interface.treatment_column)
    
    # ì°¸ê³ : target_columnì€ ì¶”ë¡ ì‹œ ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ì œì™¸
    
    # 4. Feature columns ì²˜ë¦¬
    if data_interface.feature_columns:
        # ëª…ì‹œì  feature_columns ì¶”ê°€
        required.extend(data_interface.feature_columns)
    elif actual_training_df is not None:
        # feature_columns=nullì¸ ê²½ìš°: ì‹¤ì œ í•™ìŠµ ë°ì´í„°ì—ì„œ ìë™ ì¶”ì¶œ
        # target, entity, timestamp, treatment ì œì™¸í•œ ë‚˜ë¨¸ì§€ ëª¨ë“  ì»¬ëŸ¼ ì‚¬ìš©
        exclude_columns = set()
        
        # ì œì™¸í•  ì»¬ëŸ¼ë“¤ ìˆ˜ì§‘
        if data_interface.target_column:
            exclude_columns.add(data_interface.target_column)
        exclude_columns.update(data_interface.entity_columns)
        if data_interface.timestamp_column:
            exclude_columns.add(data_interface.timestamp_column)
        if data_interface.treatment_column:
            exclude_columns.add(data_interface.treatment_column)
        
        # ì‹¤ì œ ë°ì´í„°ì—ì„œ ì œì™¸ ëŒ€ìƒ ì»¬ëŸ¼ë“¤ì„ ë¹¼ê³  ë‚˜ë¨¸ì§€ ì¶”ê°€
        feature_columns_from_data = [
            col for col in actual_training_df.columns 
            if col not in exclude_columns
        ]
        required.extend(feature_columns_from_data)
        
        logger.info(
            f"feature_columns=null ê°ì§€ - ì‹¤ì œ í•™ìŠµ ë°ì´í„°ì—ì„œ {len(feature_columns_from_data)}ê°œ "
            f"feature ì»¬ëŸ¼ ìë™ ì¶”ì¶œ: {feature_columns_from_data}"
        )
    else:
        # feature_columns=nullì´ì§€ë§Œ actual_training_dfê°€ ì—†ëŠ” ê²½ìš° (ì¶”ë¡  ì‹œì )
        logger.warning(
            "feature_columns=nullì´ì§€ë§Œ ì‹¤ì œ í•™ìŠµ ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
            "ì €ì¥ëœ data_interface_schemaë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤."
        )
    
    # ì¤‘ë³µ ì œê±°í•˜ì—¬ ë°˜í™˜
    unique_required = list(set(required))
    
    logger.debug(
        f"DataInterface í•„ìˆ˜ ì»¬ëŸ¼ ì¶”ì¶œ ì™„ë£Œ - "
        f"Task: {data_interface.task_type}, "
        f"ì»¬ëŸ¼ ìˆ˜: {len(unique_required)}, "
        f"ì»¬ëŸ¼ë“¤: {unique_required}"
    )
    
    return unique_required


def validate_data_interface_columns(
    df: pd.DataFrame, 
    data_interface: DataInterface, 
    stored_required_columns: List[str] = None
) -> None:
    """
    DataFrameì´ DataInterface í•„ìˆ˜ ì»¬ëŸ¼ì„ ëª¨ë‘ í¬í•¨í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Phase 5 ìˆ˜ì •ëœ ê²€ì¦ ì •ì±…:
    - í•™ìŠµì‹œ: data.data_interface + ì‹¤ì œ í•™ìŠµ ë°ì´í„°ì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ì¶”ì¶œ
    - ì¶”ë¡ ì‹œ: ì €ì¥ëœ í•„ìˆ˜ ì»¬ëŸ¼ ëª©ë¡ê³¼ --data-path ë°ì´í„° ë¹„êµ
    - ì™„ì „í•œ ì»¬ëŸ¼ ì¼ì¹˜ê°€ ì•„ë‹Œ, í•„ìˆ˜ ì»¬ëŸ¼ í¬í•¨ ì—¬ë¶€ë§Œ ê²€ì¦
    - ì¶”ê°€ ì»¬ëŸ¼ ì¡´ì¬ëŠ” í—ˆìš© (ë¬´ì‹œë¨)
    
    Args:
        df: ê²€ì¦í•  ë°ì´í„°í”„ë ˆì„
        data_interface: Recipeì˜ DataInterface ì„¤ì •
        stored_required_columns: í•™ìŠµì‹œ ì €ì¥ëœ í•„ìˆ˜ ì»¬ëŸ¼ ëª©ë¡ (ì¶”ë¡ ì‹œ ì‚¬ìš©)
        
    Raises:
        ValueError: í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ëœ ê²½ìš° ìƒì„¸í•œ ì§„ë‹¨ ë©”ì‹œì§€ì™€ í•¨ê»˜ ë°œìƒ
        
    Examples:
        >>> # ì¶”ë¡ ìš© ê²€ì¦ (target_column ë¶ˆí•„ìš”)
        >>> df = pd.DataFrame({
        ...     'user_id': [1, 2, 3],
        ...     'age': [25, 30, 35],
        ...     'income': [5000, 6000, 7000]
        ... })
        >>> interface = DataInterface(...)
        >>> validate_data_interface_columns(df, interface)  # í†µê³¼ (target_column ì—†ì–´ë„ ë¨)
        
        >>> # ì¶”ë¡ ì‹œ ê²€ì¦ (ì €ì¥ëœ ì»¬ëŸ¼ ëª©ë¡ ì‚¬ìš©)
        >>> inference_df = pd.DataFrame({'user_id': [4, 5], 'age': [40, 45], 'income': [8000, 9000]})
        >>> validate_data_interface_columns(inference_df, interface, stored_required_columns=['user_id', 'age', 'income'])  # í†µê³¼
    """
    # ì¶”ë¡ ì‹œì—ëŠ” ì €ì¥ëœ ì»¬ëŸ¼ ëª©ë¡ ìš°ì„  ì‚¬ìš©
    if stored_required_columns:
        required_columns = stored_required_columns
        logger.debug(f"ì €ì¥ëœ í•„ìˆ˜ ì»¬ëŸ¼ ëª©ë¡ ì‚¬ìš©: {required_columns}")
    else:
        # í•™ìŠµì‹œì—ëŠ” DataInterfaceì—ì„œ ì¶”ì¶œ (ì‹¤ì œ ë°ì´í„° í•„ìš”í•  ìˆ˜ ìˆìŒ)
        required_columns = get_required_columns_from_data_interface(data_interface, df)
        logger.debug(f"DataInterfaceì—ì„œ í•„ìˆ˜ ì»¬ëŸ¼ ì¶”ì¶œ: {required_columns}")
    
    actual_columns = set(df.columns.tolist())
    missing_columns = set(required_columns) - actual_columns
    
    if missing_columns:
        # ìƒì„¸í•œ ì§„ë‹¨ ë©”ì‹œì§€ ìƒì„±
        error_message = (
            f"DataInterface í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½ ê°ì§€:\n\n"
            f"ğŸ“‹ Task Type: {data_interface.task_type}\n"
            f"âŒ ëˆ„ë½ëœ ì»¬ëŸ¼: {sorted(missing_columns)}\n"
            f"âœ… í•„ìš”í•œ ì „ì²´ ì»¬ëŸ¼: {sorted(required_columns)}\n"
            f"ğŸ“Š ì‹¤ì œ ë°ì´í„° ì»¬ëŸ¼: {sorted(actual_columns)}\n\n"
            f"ğŸ’¡ í•´ê²°ë°©ì•ˆ:\n"
            f"1. ë°ì´í„° ì†ŒìŠ¤ì— ëˆ„ë½ëœ ì»¬ëŸ¼ì„ ì¶”ê°€í•˜ì„¸ìš”\n"
            f"2. Recipeì˜ DataInterface ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”\n"
            f"3. Fetcher ì„¤ì •ì—ì„œ ì»¬ëŸ¼ ë§¤í•‘ì„ í™•ì¸í•˜ì„¸ìš”"
        )
        
        logger.error(f"DataInterface ì»¬ëŸ¼ ê²€ì¦ ì‹¤íŒ¨: {missing_columns}")
        raise ValueError(error_message)
    
    # ì¶”ê°€ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš° ì •ë³´ì„± ë¡œê·¸ (ì—ëŸ¬ ì•„ë‹˜)
    extra_columns = actual_columns - set(required_columns)
    if extra_columns:
        logger.info(
            f"DataInterfaceì— ì •ì˜ë˜ì§€ ì•Šì€ ì¶”ê°€ ì»¬ëŸ¼ ë°œê²¬: {sorted(extra_columns)} "
            f"(í—ˆìš©ë¨, í•™ìŠµ/ì¶”ë¡ ì—ì„œ ë¬´ì‹œë¨)"
        )
    
    logger.info(
        f"DataInterface ì»¬ëŸ¼ ê²€ì¦ í†µê³¼ - "
        f"Task: {data_interface.task_type}, "
        f"í•„ìˆ˜ ì»¬ëŸ¼: {len(required_columns)}ê°œ, "
        f"ì‹¤ì œ ì»¬ëŸ¼: {len(actual_columns)}ê°œ"
    )


def create_data_interface_schema_for_storage(
    data_interface: DataInterface, 
    df: pd.DataFrame
) -> Dict[str, Any]:
    """
    PyfuncWrapper ì €ì¥ìš© DataInterface ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ì´ ìŠ¤í‚¤ë§ˆëŠ” ì¶”ë¡  ì‹œì ì— ë°ì´í„° ê²€ì¦ì„ ìœ„í•´ MLflow ëª¨ë¸ê³¼ í•¨ê»˜ ì €ì¥ë©ë‹ˆë‹¤.
    í•™ìŠµ ì‹œì ì˜ DataInterface ì„¤ì •ê³¼ ì‹¤ì œ ë°ì´í„° íƒ€ì… ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
    
    **í•µì‹¬ ê¸°ëŠ¥:**
    - feature_columns=nullì¸ ê²½ìš° ì‹¤ì œ í•™ìŠµ ë°ì´í„°ì—ì„œ ì‚¬ìš©ëœ ëª¨ë“  ì»¬ëŸ¼ ì €ì¥
    - ì¶”ë¡ ì‹œ ì´ ì €ì¥ëœ ì»¬ëŸ¼ë“¤ê³¼ --data-path ë°ì´í„°ì˜ ì¼ì¹˜ì„± ê²€ì¦ì— ì‚¬ìš©
    
    Args:
        data_interface: Recipeì˜ DataInterface ì„¤ì •
        df: ì‹¤ì œ í•™ìŠµ ë°ì´í„°í”„ë ˆì„
        
    Returns:
        Dict[str, Any]: PyfuncWrapper ì €ì¥ìš© ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„°
        
    Examples:
        >>> # feature_columnsê°€ ëª…ì‹œëœ ê²½ìš°
        >>> schema = create_data_interface_schema_for_storage(interface, train_df)
        >>> schema['required_columns']
        ['price', 'user_id', 'age', 'income']
        
        >>> # feature_columns=nullì¸ ê²½ìš° ì‹¤ì œ ë°ì´í„°ì—ì„œ ì¶”ì¶œ
        >>> schema_null = create_data_interface_schema_for_storage(interface_null, train_df)
        >>> schema_null['required_columns']  # target, entity ì œì™¸í•œ ëª¨ë“  ì»¬ëŸ¼
        ['price', 'user_id', 'age', 'income', 'location', 'category']
    """
    # ğŸ†• í•µì‹¬: ì‹¤ì œ í•™ìŠµ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•„ìˆ˜ ì»¬ëŸ¼ ì¶”ì¶œ
    required_columns = get_required_columns_from_data_interface(data_interface, df)
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë“¤ì˜ ë°ì´í„° íƒ€ì… ìˆ˜ì§‘
    column_dtypes = {}
    for col in required_columns:
        if col in df.columns:
            column_dtypes[col] = str(df[col].dtype)
        else:
            # ì´ë¡ ì ìœ¼ë¡œëŠ” validate_data_interface_columns()ë¥¼ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•˜ë¯€ë¡œ
            # ì´ ìƒí™©ì€ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨
            logger.warning(f"í•„ìˆ˜ ì»¬ëŸ¼ '{col}'ì´ DataFrameì— ì—†ìŠµë‹ˆë‹¤. íƒ€ì… ì •ë³´ë¥¼ 'unknown'ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            column_dtypes[col] = 'unknown'
    
    # ì¶”ë¡  ì‹œ ê²€ì¦ì„ ìœ„í•œ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„° ìƒì„±
    schema_metadata = {
        # DataInterface ì „ì²´ ì„¤ì • (Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜)
        'data_interface': data_interface.model_dump(),
        
        # ğŸ†• í•µì‹¬: ì‹¤ì œ í•™ìŠµì‹œ ì‚¬ìš©ëœ í•„ìˆ˜ ì»¬ëŸ¼ë“¤ (feature_columns=null ì²˜ë¦¬ í¬í•¨)
        'required_columns': required_columns,
        'column_dtypes': column_dtypes,
        
        # feature_columns=null ì—¬ë¶€ ë©”íƒ€ë°ì´í„° (ë””ë²„ê¹…ìš©)
        'feature_columns_was_null': data_interface.feature_columns is None,
        'original_dataframe_columns': df.columns.tolist(),  # ì „ì²´ ì»¬ëŸ¼ ê¸°ë¡
        
        # ë©”íƒ€ë°ì´í„°
        'schema_version': '5.1',  # Phase 5.1 ë²„ì „
        'validation_timestamp': pd.Timestamp.now().isoformat(),
        'total_required_columns': len(required_columns),
        'validation_policy': 'data_interface_based',  # ìƒˆë¡œìš´ ì •ì±… ëª…ì‹œ
        
        # ë””ë²„ê¹… ì •ë³´
        'actual_dataframe_shape': list(df.shape)
    }
    
    logger.info(
        f"DataInterface ì €ì¥ìš© ìŠ¤í‚¤ë§ˆ ìƒì„± ì™„ë£Œ - "
        f"Task: {data_interface.task_type}, "
        f"í•„ìˆ˜ ì»¬ëŸ¼: {len(required_columns)}ê°œ, "
        f"feature_columns_was_null: {data_interface.feature_columns is None}, "
        f"ìŠ¤í‚¤ë§ˆ ë²„ì „: 5.1"
    )
    
    return schema_metadata