import pandas as pd
from datetime import datetime
from typing import Dict, List, Any
from src.settings import Settings
from src.utils.system.logger import logger

def validate_schema(df: pd.DataFrame, settings: Settings, for_training: bool = False):
    """
    ì…ë ¥ ë°ì´í„°í”„ë ˆì„ì´ Recipe ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤. (27ê°œ Recipe ëŒ€ì‘)

    Args:
        df (pd.DataFrame): ê²€ì¦í•  ë°ì´í„°í”„ë ˆì„.
        settings (Settings): ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ í¬í•¨ëœ ì„¤ì • ê°ì²´.
        for_training (bool): Trueë©´ ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ê²€ì¦ (entity, timestamp ì»¬ëŸ¼ ì œì™¸)
                            Falseë©´ ì›ë³¸ ë°ì´í„° ê²€ì¦ (ëª¨ë“  ì»¬ëŸ¼ ìš”êµ¬)

    Raises:
        TypeError: ìŠ¤í‚¤ë§ˆ ê²€ì¦ì— ì‹¤íŒ¨í•  ê²½ìš° ë°œìƒí•©ë‹ˆë‹¤.
    """
    logger.info(f"ëª¨ë¸ ì…ë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆë¥¼ ê²€ì¦í•©ë‹ˆë‹¤... (for_training: {for_training})")

    # 27ê°œ Recipe êµ¬ì¡°: í•„ìˆ˜ ì»¬ëŸ¼ë“¤ í™•ì¸
    entity_schema = settings.recipe.model.loader.entity_schema
    data_interface = settings.recipe.model.data_interface
    
    errors = []
    required_columns = []
    
    if not for_training:
        # ì›ë³¸ ë°ì´í„° ê²€ì¦: ëª¨ë“  ì»¬ëŸ¼ ìš”êµ¬
        # 1. Entity + Timestamp ì»¬ëŸ¼ ê²€ì¦
        required_columns = entity_schema.entity_columns + [entity_schema.timestamp_column]
        
        # 2. Target ì»¬ëŸ¼ ê²€ì¦ (clustering ì œì™¸)
        if data_interface.task_type != "clustering" and data_interface.target_column:
            required_columns.append(data_interface.target_column)
    else:
        # ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ê²€ì¦: entity_schema ì»¬ëŸ¼ë“¤ ì œì™¸
        logger.info("ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ê²€ì¦: entity_columns, timestamp_column ì œì™¸")
        required_columns = []
        
        # Target ì»¬ëŸ¼ì€ ì´ë¯¸ ë¶„ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ ê²€ì¦í•˜ì§€ ì•ŠìŒ
        
    # 3. Treatment ì»¬ëŸ¼ ê²€ì¦ (causal ì „ìš©) - í•™ìŠµ ì‹œì—ë„ í•„ìš”
    if data_interface.task_type == "causal" and data_interface.treatment_column:
        required_columns.append(data_interface.treatment_column)
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦
    for col in required_columns:
        if col not in df.columns:
            errors.append(f"- í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: '{col}' (task_type: {data_interface.task_type})")
    
    # ê¸°ë³¸ ë°ì´í„° íƒ€ì… ê²€ì¦ (timestamp ì»¬ëŸ¼)
    if entity_schema.timestamp_column in df.columns:
        timestamp_col = entity_schema.timestamp_column
        if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):
            try:
                # ìë™ ë³€í™˜ ì‹œë„
                pd.to_datetime(df[timestamp_col])
                logger.info(f"Timestamp ì»¬ëŸ¼ '{timestamp_col}' ìë™ ë³€í™˜ ê°€ëŠ¥")
            except:
                errors.append(f"- Timestamp ì»¬ëŸ¼ '{timestamp_col}' íƒ€ì… ì˜¤ë¥˜: datetime ë³€í™˜ ë¶ˆê°€")

    if errors:
        error_message = "ëª¨ë¸ ì…ë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì‹¤íŒ¨:\n" + "\n".join(errors)
        error_message += f"\n\ní•„ìˆ˜ ì»¬ëŸ¼: {required_columns}"
        error_message += f"\nì‹¤ì œ ì»¬ëŸ¼: {list(df.columns)}"
        raise TypeError(error_message)
    
    logger.info(f"ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì„±ê³µ (task_type: {data_interface.task_type})")


def convert_schema(df: pd.DataFrame, expected_schema: dict) -> pd.DataFrame:
    """
    ë°ì´í„°í”„ë ˆì„ì„ ì˜ˆìƒ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        df (pd.DataFrame): ë³€í™˜í•  ë°ì´í„°í”„ë ˆì„
        expected_schema (dict): ì˜ˆìƒ ìŠ¤í‚¤ë§ˆ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        pd.DataFrame: ë³€í™˜ëœ ë°ì´í„°í”„ë ˆì„
    """
    logger.info("ë°ì´í„° íƒ€ì… ë³€í™˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    converted_df = df.copy()
    
    for col, expected_type in expected_schema.items():
        if col not in converted_df.columns:
            continue
            
        if expected_type == "numeric":
            converted_df[col] = pd.to_numeric(converted_df[col], errors='coerce')
        elif expected_type == "category":
            converted_df[col] = converted_df[col].astype('category')
    
    logger.info("ë°ì´í„° íƒ€ì… ë³€í™˜ ì™„ë£Œ.")
    return converted_df 

class SchemaConsistencyValidator:
    """
    ğŸ†• Phase 4: Training/Inference ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„± ìë™ ê²€ì¦ê¸°
    ê¸°ì¡´ validate_schema í•¨ìˆ˜ë¥¼ í™•ì¥í•œ ì°¨ì„¸ëŒ€ ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„± ë³´ì¥ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, training_schema: dict):
        """
        Training ì‹œì ì— ìƒì„±ëœ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì´ˆê¸°í™”
        
        Args:
            training_schema (dict): Training ì‹œì  ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„°
                - entity_columns: Phase 1 EntitySchema ì •ë³´
                - timestamp_column: Point-in-Time ê¸°ì¤€ ì»¬ëŸ¼
                - inference_columns: Inferenceì— í•„ìš”í•œ ì»¬ëŸ¼ ëª©ë¡
                - column_types: ê° ì»¬ëŸ¼ì˜ íƒ€ì… ì •ë³´
        """
        self.training_schema = training_schema
        logger.info(f"SchemaConsistencyValidator ì´ˆê¸°í™” ì™„ë£Œ - ê²€ì¦ ëŒ€ìƒ: {len(training_schema.get('inference_columns', []))}ê°œ ì»¬ëŸ¼")
    
    def validate_inference_consistency(self, inference_df: pd.DataFrame) -> bool:
        """
        ğŸ¯ 4ë‹¨ê³„ ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„± ê²€ì¦: ê¸°ë³¸ êµ¬ì¡° â†’ ì»¬ëŸ¼ ì¼ê´€ì„± â†’ íƒ€ì… í˜¸í™˜ì„± â†’ Point-in-Time íŠ¹ë³„ ê²€ì¦
        
        Args:
            inference_df (pd.DataFrame): ê²€ì¦í•  Inference ë°ì´í„°
            
        Returns:
            bool: ëª¨ë“  ê²€ì¦ í†µê³¼ ì‹œ True
            
        Raises:
            ValueError: ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜ ë°œê²¬ ì‹œ ìƒì„¸í•œ ì§„ë‹¨ ë©”ì‹œì§€ì™€ í•¨ê»˜ ë°œìƒ
        """
        
        # 1. ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ êµ¬ì¡° ê²€ì¦ (ê¸°ì¡´ validate_schema ë¡œì§ í™œìš©)
        logger.info("Phase 1: ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ êµ¬ì¡° ê²€ì¦ ì‹œì‘...")
        self._validate_basic_schema(inference_df)
        
        # 2. Training/Inference í•„ìˆ˜ ì»¬ëŸ¼ ì¼ê´€ì„± ê²€ì¦
        logger.info("Phase 2: Training/Inference ì»¬ëŸ¼ ì¼ê´€ì„± ê²€ì¦ ì‹œì‘...")
        self._validate_column_consistency(inference_df)
        
        # 3. íƒ€ì… í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤ ê²€ì¦ (í˜¸í™˜ í—ˆìš©, ë¹„í˜¸í™˜ ì°¨ë‹¨)
        logger.info("Phase 3: ê³ ê¸‰ íƒ€ì… í˜¸í™˜ì„± ê²€ì¦ ì‹œì‘...")
        self._validate_dtype_compatibility(inference_df)
        
        # 4. Entity/Timestamp íŠ¹ë³„ ê²€ì¦ (Phase 1, 2 ì—°ê³„)
        logger.info("Phase 4: Point-in-Time ì»¬ëŸ¼ íŠ¹ë³„ ê²€ì¦ ì‹œì‘...")
        self._validate_point_in_time_columns(inference_df)
        
        logger.info("âœ… ëª¨ë“  ìŠ¤í‚¤ë§ˆ ì¼ê´€ì„± ê²€ì¦ í†µê³¼")
        return True
    
    def _validate_basic_schema(self, df: pd.DataFrame):
        """ê¸°ì¡´ validate_schema ë¡œì§ì„ í™œìš©í•œ ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        # ê¸°ë³¸ì ì¸ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        required_cols = self.training_schema.get('inference_columns', [])
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            logger.warning(f"ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ ê²€ì¦ì—ì„œ ëˆ„ë½ ì»¬ëŸ¼ ë°œê²¬: {missing_cols}")
        else:
            logger.info("ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ êµ¬ì¡° ê²€ì¦ í†µê³¼")
    
    def _validate_column_consistency(self, inference_df: pd.DataFrame):
        """Training vs Inference í•„ìˆ˜ ì»¬ëŸ¼ ì¼ê´€ì„± ê²€ì¦"""
        required_cols = self.training_schema.get('inference_columns', [])
        missing_cols = set(required_cols) - set(inference_df.columns)
        
        if missing_cols:
            raise ValueError(
                f"ğŸš¨ Inference ë°ì´í„°ì— Training ì‹œ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}\n"
                f"Training ìŠ¤í‚¤ë§ˆ: {required_cols}\n"
                f"í˜„ì¬ ìŠ¤í‚¤ë§ˆ: {list(inference_df.columns)}\n"
                f"ğŸ’¡ í•´ê²°ë°©ì•ˆ: ëˆ„ë½ëœ ì»¬ëŸ¼ì„ ë°ì´í„°ì— ì¶”ê°€í•˜ê±°ë‚˜ preprocessor/augmenter ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”."
            )
        
        # ì¶”ê°€ ì»¬ëŸ¼ì€ ê²½ê³ ë§Œ (ìƒˆë¡œìš´ í”¼ì²˜ ì¶”ê°€ ê°€ëŠ¥ì„±)
        extra_cols = set(inference_df.columns) - set(required_cols)
        if extra_cols:
            logger.warning(f"Trainingì— ì—†ë˜ ì¶”ê°€ ì»¬ëŸ¼ ë°œê²¬: {extra_cols} (í—ˆìš©ë˜ì§€ë§Œ ë¬´ì‹œë©ë‹ˆë‹¤)")
        
        logger.info("ì»¬ëŸ¼ ì¼ê´€ì„± ê²€ì¦ í†µê³¼")
    
    def _validate_dtype_compatibility(self, inference_df: pd.DataFrame):
        """ê³ ê¸‰ íƒ€ì… í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤ ê²€ì¦ (ê¸°ì¡´ convert_schema ë¡œì§ í™•ì¥)"""
        column_types = self.training_schema.get('column_types', {})
        
        for col in self.training_schema.get('inference_columns', []):
            if col not in inference_df.columns:
                continue  # ì´ë¯¸ column_consistencyì—ì„œ ì²˜ë¦¬ë¨
                
            expected_dtype = column_types.get(col, 'unknown')
            actual_dtype = str(inference_df[col].dtype)
            
            if not self._is_compatible_dtype(expected_dtype, actual_dtype):
                raise ValueError(
                    f"ğŸš¨ ì»¬ëŸ¼ '{col}' íƒ€ì… ë¶ˆì¼ì¹˜:\n"
                    f"Training ì‹œ: {expected_dtype} â†’ í˜„ì¬ Inference: {actual_dtype}\n"
                    f"ì´ëŠ” í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íƒ€ì… ë³€ê²½ì…ë‹ˆë‹¤.\n"
                    f"ğŸ’¡ í•´ê²°ë°©ì•ˆ: ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ íƒ€ì…ì„ '{expected_dtype}'ë¡œ ë³€í™˜í•˜ê±°ë‚˜ ëª¨ë¸ì„ ì¬í•™ìŠµí•˜ì„¸ìš”."
                )
        
        logger.info("íƒ€ì… í˜¸í™˜ì„± ê²€ì¦ í†µê³¼")
    
    def _is_compatible_dtype(self, expected: str, actual: str) -> bool:
        """íƒ€ì… í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤ (ê¸°ì¡´ convert_schema ë¡œì§ í™•ì¥)"""
        # ì™„ì „ ë™ì¼í•œ ê²½ìš° í—ˆìš©
        if expected == actual:
            return True
        
        # í˜¸í™˜ ê°€ëŠ¥í•œ íƒ€ì… ê·¸ë£¹ë“¤
        compatible_groups = [
            # ìˆ«ìí˜•: int64 â†” int32 â†” int í˜¸í™˜ í—ˆìš©
            (['int64', 'int32', 'int'], ['int64', 'int32', 'int']),
            # ì‹¤ìˆ˜í˜•: float64 â†” float32 â†” float í˜¸í™˜ í—ˆìš©  
            (['float64', 'float32', 'float'], ['float64', 'float32', 'float']),
            # ë¬¸ìì—´: object â†” string í˜¸í™˜ í—ˆìš©
            (['object', 'string'], ['object', 'string']),
            # ë‚ ì§œ: datetime64 ë³€í˜•ë“¤ í—ˆìš©
            (['datetime64', 'datetime'], ['datetime64', 'datetime']),
            # ë¶€ìš¸: bool ë³€í˜•ë“¤ í—ˆìš©
            (['bool', 'boolean'], ['bool', 'boolean'])
        ]
        
        for expected_group, actual_group in compatible_groups:
            if any(e in expected for e in expected_group) and \
               any(a in actual for a in actual_group):
                return True
        
        # ìœ„í—˜í•œ íƒ€ì… ë³€ê²½ì€ ì°¨ë‹¨ (string â†’ int, int â†’ string ë“±)
        return False
    
    def _validate_point_in_time_columns(self, inference_df: pd.DataFrame):
        """Entity/Timestamp ì»¬ëŸ¼ íŠ¹ë³„ ê²€ì¦ (Phase 1, 2 ì—°ê³„)"""
        # Phase 1 EntitySchema ì •ë³´ í™œìš©
        entity_cols = self.training_schema.get('entity_columns', [])
        timestamp_col = self.training_schema.get('timestamp_column', '')
        
        # Entity ì»¬ëŸ¼ë“¤ ì¡´ì¬ ë° íƒ€ì… ê²€ì¦
        for col in entity_cols:
            if col not in inference_df.columns:
                raise ValueError(f"ğŸš¨ í•„ìˆ˜ Entity ì»¬ëŸ¼ ëˆ„ë½: '{col}' - Point-in-Time JOINì„ ìœ„í•´ í•„ìˆ˜ì…ë‹ˆë‹¤")
        
        # Timestamp ì»¬ëŸ¼ íŠ¹ë³„ ê²€ì¦
        if timestamp_col and timestamp_col in inference_df.columns:
            if not pd.api.types.is_datetime64_any_dtype(inference_df[timestamp_col]):
                raise ValueError(
                    f"ğŸš¨ Timestamp ì»¬ëŸ¼ '{timestamp_col}'ì´ datetime íƒ€ì…ì´ ì•„ë‹™ë‹ˆë‹¤: {inference_df[timestamp_col].dtype}\n"
                    f"ğŸ’¡ í•´ê²°ë°©ì•ˆ: pd.to_datetime(df['{timestamp_col}'])ë¡œ ë³€í™˜í•˜ê±°ë‚˜ ì „ì²˜ë¦¬ì—ì„œ ë‚ ì§œ í˜•ì‹ì„ ë§ì¶°ì£¼ì„¸ìš”."
                )
            
            # ë¯¸ë˜ ë°ì´í„° ì²´í¬ (Phase 2 Point-in-Time ì•ˆì „ì„± ì—°ê³„)
            max_timestamp = inference_df[timestamp_col].max()
            current_time = pd.Timestamp.now()
            if max_timestamp > current_time:
                logger.warning(
                    f"âš ï¸ ë¯¸ë˜ ë°ì´í„° ê°ì§€: ìµœëŒ€ timestamp {max_timestamp} > í˜„ì¬ ì‹œê°„ {current_time}\n"
                    f"Point-in-Time ì•ˆì „ì„±ì„ ìœ„í•´ ì£¼ì˜ í•„ìš”"
                )
        
        logger.info("Point-in-Time ì»¬ëŸ¼ íŠ¹ë³„ ê²€ì¦ í†µê³¼")


def generate_training_schema_metadata(training_df: pd.DataFrame, data_interface_config: dict) -> dict:
    """
    ğŸ†• Phase 4: Training ì‹œì ì— ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„° ìƒì„±
    Phase 1-3ì˜ ëª¨ë“  ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ìê¸° ê¸°ìˆ ì  ìŠ¤í‚¤ë§ˆ ìƒì„±
    
    Args:
        training_df (pd.DataFrame): Training ë°ì´í„°
        data_interface_config (dict): EntitySchema ì„¤ì • ì •ë³´
        
    Returns:
        dict: ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„°
    """
    target_column = data_interface_config.get('target_column')
    
    # Inferenceì— í•„ìš”í•œ ì»¬ëŸ¼ë“¤ (target ì œì™¸)
    inference_columns = [col for col in training_df.columns if col != target_column]
    
    schema_metadata = {
        # Phase 1 EntitySchema ì •ë³´ í™œìš©
        'entity_columns': data_interface_config.get('entity_columns', []),
        'timestamp_column': data_interface_config.get('timestamp_column', ''),
        'target_column': target_column,
        'task_type': data_interface_config.get('task_type', ''),
        
        # ì‹¤ì œ Training ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ë³´
        'training_columns': list(training_df.columns),
        'inference_columns': inference_columns,
        'column_types': {col: str(training_df[col].dtype) for col in training_df.columns},
        
        # ë©”íƒ€ë°ì´í„°
        'schema_version': '2.0',
        'created_at': datetime.now().isoformat(),
        'point_in_time_safe': True,  # Phase 2 ASOF JOIN ë³´ì¥
        'sql_injection_safe': True,  # Phase 3 ë³´ì•ˆ ê°•í™” ë³´ì¥
        'total_training_samples': len(training_df),
        'column_count': len(training_df.columns)
    }
    
    logger.info(f"âœ… Training ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(inference_columns)}ê°œ inference ì»¬ëŸ¼, {len(training_df.columns)}ê°œ ì „ì²´ ì»¬ëŸ¼")
    return schema_metadata 