# src/utils/system/mlflow_utils.py

import datetime
import json
import os
import uuid
from contextlib import contextmanager

# ìˆœí™˜ ì°¸ì¡°ë¥¼ í”¼í•˜ê¸° ìœ„í•´ íƒ€ì… íŒíŠ¸ë§Œ ì„í¬íŠ¸
from typing import TYPE_CHECKING, List, Optional
from urllib.parse import urlparse

import mlflow
import pandas as pd
from mlflow.models.signature import ModelSignature
from mlflow.tracking import MlflowClient
from mlflow.types import ColSpec, ParamSchema, ParamSpec, Schema

if TYPE_CHECKING:
    from src.settings import Settings
    from mlflow.entities import Run
    from mlflow.pyfunc import PyFuncModel

from src.utils.core.logger import log_mlflow, logger


def generate_unique_run_name(base_run_name: str) -> str:
    """
    ê¸°ë³¸ run nameì— timestampì™€ random suffixë¥¼ ì¶”ê°€í•˜ì—¬ ì™„ì „íˆ ìœ ë‹ˆí¬í•œ run nameì„ ìƒì„±í•©ë‹ˆë‹¤.
    ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œ MLflow run name ì¶©ëŒì„ ë°©ì§€í•©ë‹ˆë‹¤.

    Args:
        base_run_name (str): ê¸°ë³¸ run name (ì˜ˆ: "e2e_classification_test_run")

    Returns:
        str: ìœ ë‹ˆí¬í•œ run name (ì˜ˆ: "e2e_classification_test_run_20250907_143025_a1b2")
    """
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    random_suffix = str(uuid.uuid4())[:8]  # ì²˜ìŒ 8ìë¦¬ë§Œ ì‚¬ìš©
    unique_run_name = f"{base_run_name}_{timestamp}_{random_suffix}"

    logger.debug(f"[MLFLOW] ìœ ë‹ˆí¬ run name ìƒì„±: {base_run_name} -> {unique_run_name}")
    return unique_run_name


def setup_mlflow(settings: "Settings") -> None:
    """
    ì£¼ì…ëœ settings ê°ì²´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ MLflow í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    """
    mlflow.set_tracking_uri(settings.config.mlflow.tracking_uri)
    mlflow.set_experiment(settings.config.mlflow.experiment_name)

    log_mlflow("ì„¤ì • ì™„ë£Œ")
    logger.debug(f"[MLFLOW] Tracking URI: {settings.config.mlflow.tracking_uri}")
    logger.debug(f"[MLFLOW] Experiment: {settings.config.mlflow.experiment_name}")


@contextmanager
def start_run(settings: "Settings", run_name: str) -> "Run":
    """
    MLflow ì‹¤í–‰ì„ ì‹œì‘í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €.
    ì™¸ë¶€ í™˜ê²½ ë³€ìˆ˜ì˜ ì˜í–¥ì„ ë°›ì§€ ì•Šë„ë¡ tracking_urië¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    ìë™ìœ¼ë¡œ ìœ ë‹ˆí¬í•œ run nameì„ ìƒì„±í•˜ì—¬ ë³‘ë ¬ ì‹¤í–‰ ì‹œ ì¶©ëŒì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    # ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ìœ ë‹ˆí¬í•œ run name ìƒì„±
    unique_run_name = generate_unique_run_name(run_name)

    # ì™¸ë¶€ì—ì„œ ì§€ì •ëœ tracking_uri(ì˜ˆ: í…ŒìŠ¤íŠ¸)ê°€ ìˆë‹¤ë©´ ì¡´ì¤‘: ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •
    tracking_uri = settings.config.mlflow.tracking_uri
    if tracking_uri:
        # file:// ìŠ¤í† ì–´ëŠ” ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ë¯¸ë¦¬ ìƒì„±í•´ì•¼ í•¨
        parsed = urlparse(tracking_uri)
        if parsed.scheme == "file" and parsed.path:
            try:
                os.makedirs(parsed.path, exist_ok=True)
            except Exception:
                # ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨ëŠ” ì•„ë˜ ì„¤ì • ì‹œì ì—ì„œ ì—ëŸ¬ë¡œ ë…¸ì¶œë¨
                pass
        mlflow.set_tracking_uri(tracking_uri)

    # ì‹¤í—˜ëª… ì„¤ì • (tracking_uri ì„¤ì • ì´í›„)
    mlflow.set_experiment(settings.config.mlflow.experiment_name)

    try:
        with mlflow.start_run(run_name=unique_run_name) as run:
            log_mlflow(f"Run ì‹œì‘ - ID: {run.info.run_id[:8]}...")
            # ì›ë³¸ run nameì„ íƒœê·¸ë¡œ ì €ì¥í•˜ì—¬ ì¶”ì  ê°€ëŠ¥í•˜ê²Œ í•¨
            mlflow.set_tag("original_run_name", run_name)
            mlflow.set_tag("unique_run_name", unique_run_name)

            try:
                yield run
                mlflow.set_tag("status", "success")
                log_mlflow("Run ì™„ë£Œ")
            except Exception as e:
                mlflow.set_tag("status", "failed")
                logger.error(f"[MLFLOW] Run ì‹¤íŒ¨: {e}", exc_info=True)
                raise
    except Exception as mlflow_error:
        # MLflow ì‹¤í–‰ ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš° (ì˜ˆ: run name ì¶©ëŒì´ ì—¬ì „íˆ ë°œìƒí•œ ê²½ìš°)
        if (
            "already exists" in str(mlflow_error).lower()
            or "duplicate" in str(mlflow_error).lower()
        ):
            logger.warning(f"[MLFLOW] Run name ì¶©ëŒ ê°ì§€: {unique_run_name}")
            # ì¶”ê°€ random suffixë¡œ ì¬ì‹œë„
            retry_run_name = f"{unique_run_name}_{uuid.uuid4().hex[:4]}"
            logger.debug(f"[MLFLOW] ì¬ì‹œë„: {retry_run_name}")

            with mlflow.start_run(run_name=retry_run_name) as run:
                log_mlflow(f"Run ì‹œì‘ (ì¬ì‹œë„) - ID: {run.info.run_id[:8]}...")
                mlflow.set_tag("original_run_name", run_name)
                mlflow.set_tag("unique_run_name", retry_run_name)
                mlflow.set_tag("retry_count", "1")

                try:
                    yield run
                    mlflow.set_tag("status", "success")
                    log_mlflow("Run ì™„ë£Œ (ì¬ì‹œë„)")
                except Exception as e:
                    mlflow.set_tag("status", "failed")
                    logger.error(f"[MLFLOW] Run ì‹¤íŒ¨ (ì¬ì‹œë„): {e}", exc_info=True)
                    raise
        else:
            # ë‹¤ë¥¸ ì¢…ë¥˜ì˜ MLflow ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
            raise


def get_latest_run_id(settings: "Settings", experiment_name: str) -> str:
    """
    ì§€ì •ëœ experimentì—ì„œ ê°€ì¥ ìµœê·¼ì— ì„±ê³µí•œ runì˜ IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    setup_mlflow(settings)
    try:
        experiment = mlflow.get_experiment_by_name(experiment_name)
        if not experiment:
            raise ValueError(f"Experiment '{experiment_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        runs_df = mlflow.search_runs(
            experiment_ids=[experiment.experiment_id],
            filter_string="tags.status = 'success'",
            order_by=["start_time DESC"],
            max_results=1,
        )

        if runs_df.empty:
            raise ValueError(f"Experiment '{experiment_name}'ì—ì„œ ì„±ê³µí•œ runì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        latest_run_id = runs_df.iloc[0]["run_id"]
        logger.debug(f"[MLFLOW] ìµœê·¼ Run ID ì¡°íšŒ: {latest_run_id[:8]}...")
        return latest_run_id

    except Exception as e:
        logger.error(f"[MLFLOW] ìµœê·¼ Run ID ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise


def get_model_uri(run_id: str, artifact_path: str = "model") -> str:
    """
    Run IDì™€ ì•„í‹°íŒ©íŠ¸ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ URIë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    uri = f"runs:/{run_id}/{artifact_path}"
    logger.debug(f"ìƒì„±ëœ ëª¨ë¸ URI: {uri}")
    return uri


def load_pyfunc_model(settings: "Settings", model_uri: str) -> "PyFuncModel":
    """
    ì§€ì •ëœ URIì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ Pyfunc ëª¨ë¸ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì™¸ë¶€ í™˜ê²½ ë³€ìˆ˜ì˜ ì˜í–¥ì„ ë°›ì§€ ì•Šë„ë¡ MlflowClientë¥¼ ì§ì ‘ ìƒì„±í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    log_mlflow(f"ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_uri}")
    try:
        if model_uri.startswith("runs:/"):
            # MlflowClientë¥¼ ëª…ì‹œì ìœ¼ë¡œ ìƒì„±í•˜ì—¬ ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ
            import re

            def _parse_runs_uri(uri: str) -> tuple[str, str]:
                """'runs:/<run_id>/<artifact_path>' URIë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
                match = re.match(r"runs:/([^/]+)/(.+)", uri)
                if not match:
                    raise ValueError(f"'{uri}'ëŠ” ì˜¬ë°”ë¥¸ 'runs:/' URIê°€ ì•„ë‹™ë‹ˆë‹¤.")
                return match.group(1), match.group(2)

            client = MlflowClient(tracking_uri=settings.config.mlflow.tracking_uri)
            run_id, artifact_path = _parse_runs_uri(model_uri)

            local_path = client.download_artifacts(run_id=run_id, path=artifact_path)
            logger.debug(f"[MLFLOW] ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_path}")
            return mlflow.pyfunc.load_model(model_uri=local_path)
        else:
            # ì¼ë°˜ ê²½ë¡œ(local file, GCS, S3 ë“±)ëŠ” ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            mlflow.set_tracking_uri(settings.config.mlflow.tracking_uri)
            return mlflow.pyfunc.load_model(model_uri=model_uri)
    except Exception as e:
        logger.error(f"[MLFLOW] ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {model_uri}, ì˜¤ë¥˜: {e}", exc_info=True)
        raise


def download_artifacts(
    settings: "Settings", run_id: str, artifact_path: str, dst_path: str = None
) -> str:
    """
    ì§€ì •ëœ Run IDì—ì„œ íŠ¹ì • ì•„í‹°íŒ©íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , ë¡œì»¬ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    mlflow.set_tracking_uri(settings.config.mlflow.tracking_uri)
    logger.debug(f"[MLFLOW] ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {artifact_path}")
    try:
        local_path = mlflow.artifacts.download_artifacts(
            run_id=run_id, artifact_path=artifact_path, dst_path=dst_path
        )
        logger.debug(f"[MLFLOW] ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_path}")
        return local_path
    except Exception as e:
        logger.error(f"[MLFLOW] ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
        raise


def create_model_signature(
    input_df: pd.DataFrame, output_df: pd.DataFrame, params: dict = None
) -> ModelSignature:
    """
    ì…ë ¥ ë° ì¶œë ¥ ë°ì´í„°í”„ë ˆì„ì„ ê¸°ë°˜ìœ¼ë¡œ MLflow ModelSignatureë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        input_df (pd.DataFrame): ëª¨ë¸ ì…ë ¥ ë°ì´í„°í”„ë ˆì„ (í•™ìŠµ ì‹œ ì‚¬ìš©ëœ í˜•íƒœ)
        output_df (pd.DataFrame): ëª¨ë¸ ì¶œë ¥ ë°ì´í„°í”„ë ˆì„ (ì˜ˆì¸¡ ê²°ê³¼ í˜•íƒœ)

    Returns:
        ModelSignature: run_mode, return_intermediate íŒŒë¼ë¯¸í„°ë¥¼ í¬í•¨í•œ ì™„ì „í•œ signature
    """
    try:
        # ì…ë ¥ ìŠ¤í‚¤ë§ˆ ìƒì„±
        input_schema = Schema(
            [
                ColSpec(type=_infer_pandas_dtype_to_mlflow_type(input_df[col].dtype), name=col)
                for col in input_df.columns
            ]
        )

        # ì¶œë ¥ ìŠ¤í‚¤ë§ˆ ìƒì„±
        output_schema = Schema(
            [
                ColSpec(type=_infer_pandas_dtype_to_mlflow_type(output_df[col].dtype), name=col)
                for col in output_df.columns
            ]
        )

        # íŒŒë¼ë¯¸í„° ìŠ¤í‚¤ë§ˆ ìƒì„± (run_mode, return_intermediate ì§€ì›)
        params_schema = ParamSchema(
            [
                ParamSpec(name="run_mode", dtype="string", default="batch", shape=None),
                ParamSpec(name="return_intermediate", dtype="boolean", default=False, shape=None),
            ]
        )

        # ModelSignature ìƒì„±
        signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=params_schema)

        logger.debug(
            f"[MLFLOW] Signature ìƒì„± ì™„ë£Œ - ì…ë ¥: {len(input_schema.inputs)}ì—´, ì¶œë ¥: {len(output_schema.inputs)}ì—´"
        )

        return signature

    except Exception as e:
        logger.error(f"[MLFLOW] ModelSignature ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
        raise


def _infer_pandas_dtype_to_mlflow_type(pandas_dtype) -> str:
    """
    pandas dtypeì„ MLflow typeìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜

    Args:
        pandas_dtype: pandas ì»¬ëŸ¼ì˜ dtype

    Returns:
        str: MLflow í˜¸í™˜ íƒ€ì… ë¬¸ìì—´
    """
    dtype_str = str(pandas_dtype)

    # ì •ìˆ˜í˜•
    if pandas_dtype.name in [
        "int8",
        "int16",
        "int32",
        "int64",
        "uint8",
        "uint16",
        "uint32",
        "uint64",
    ]:
        return "long"

    # ì‹¤ìˆ˜í˜•
    elif pandas_dtype.name in ["float16", "float32", "float64"]:
        return "double"

    # ë¶ˆë¦°í˜•
    elif pandas_dtype.name == "bool":
        return "boolean"

    # ë¬¸ìì—´í˜•
    elif pandas_dtype.name == "object" or "string" in dtype_str:
        return "string"

    # ë‚ ì§œ/ì‹œê°„í˜•
    elif pandas_dtype.name.startswith("datetime"):
        return "datetime"

    # ê¸°ë³¸ê°’ (ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…)
    else:
        logger.warning(f"[MLFLOW] ì•Œ ìˆ˜ ì—†ëŠ” pandas dtype: {pandas_dtype}, 'string'ìœ¼ë¡œ ì²˜ë¦¬")
        return "string"


# ğŸ†• Phase 5: ì™„ì „ ìê¸° ê¸°ìˆ  Artifact - Enhanced MLflow í†µí•© í•¨ìˆ˜ë“¤


def create_enhanced_model_signature_with_schema(
    training_df: pd.DataFrame, data_interface_config: dict
) -> tuple[ModelSignature, dict]:
    """
    í•™ìŠµ-ì¶”ë¡  ì¼ê´€ì„±ì„ ìœ„í•œ MLflow Signatureì™€ ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„° ìƒì„±.

    Args:
        training_df (pd.DataFrame): Training ë°ì´í„° (ì „ì²˜ë¦¬ ì „)
        data_interface_config (dict): í”¼ì²˜ ì»¬ëŸ¼ ì •ë³´ (input/model ë¶„ë¦¬)

    Returns:
        tuple[ModelSignature, dict]: Signatureì™€ ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„°
    """
    logger.debug("[MLFLOW] í•™ìŠµ í”¼ì²˜ ê¸°ì¤€ Signature ìƒì„± ì¤‘")

    # ì…ë ¥ í”¼ì²˜ ì»¬ëŸ¼ ê²°ì • (ì „ì²˜ë¦¬ ì „ ê¸°ì¤€, Signature ìƒì„±ìš©)
    input_feature_cols = data_interface_config.get("input_feature_columns")
    if not input_feature_cols:
        # í•˜ìœ„ í˜¸í™˜ì„±: ê¸°ì¡´ feature_columns í´ë°±
        input_feature_cols = data_interface_config.get("feature_columns")

    if not input_feature_cols:
        # ìë™ ë„ì¶œ: entity/timestamp/target ì œì™¸
        exclude_cols = []
        if data_interface_config.get("entity_columns"):
            exclude_cols.extend(data_interface_config["entity_columns"])
        if data_interface_config.get("timestamp_column"):
            exclude_cols.append(data_interface_config["timestamp_column"])
        if data_interface_config.get("target_column"):
            exclude_cols.append(data_interface_config["target_column"])
        input_feature_cols = [col for col in training_df.columns if col not in exclude_cols]

    # training_dfì— ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ (ì „ì²˜ë¦¬ ì „ ë°ì´í„°ì´ë¯€ë¡œ)
    available_input_cols = [col for col in input_feature_cols if col in training_df.columns]

    # Signatureìš© input_example ìƒì„± (ì „ì²˜ë¦¬ ì „ í”¼ì²˜ ê¸°ì¤€)
    input_example = training_df.head(5).copy()
    input_example = input_example[available_input_cols] if available_input_cols else input_example
    sample_output = pd.DataFrame({"prediction": [0.0] * len(input_example)})
    signature = create_model_signature(input_example, sample_output)

    # ëª¨ë¸ í”¼ì²˜ ì»¬ëŸ¼ (ì „ì²˜ë¦¬ í›„ ê¸°ì¤€)
    model_feature_cols = data_interface_config.get("model_feature_columns") or available_input_cols

    # ë°ì´í„° íƒ€ì… ë§¤í•‘
    data_types = {}
    for col in available_input_cols:
        if col in training_df.columns:
            data_types[col] = str(training_df[col].dtype)

    # ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„° ìƒì„±
    data_schema = {
        "schema_version": "2.0",
        "entity_columns": data_interface_config.get("entity_columns") or [],
        "timestamp_column": data_interface_config.get("timestamp_column"),
        "target_column": data_interface_config.get("target_column"),
        # ì „ì²˜ë¦¬ ì „ ì…ë ¥ í”¼ì²˜ (ì¶”ë¡  ì…ë ¥ìš©)
        "input_feature_columns": available_input_cols,
        # ì „ì²˜ë¦¬ í›„ ëª¨ë¸ í”¼ì²˜ (ëª¨ë¸ ì…ë ¥ìš©)
        "model_feature_columns": model_feature_cols,
        # í•˜ìœ„ í˜¸í™˜ì„±
        "feature_columns": available_input_cols,
        "inference_columns": available_input_cols,
        # ì»¬ëŸ¼ ì •ë³´
        "column_count": len(available_input_cols),
        "data_types": data_types,
        # MLflow ì •ë³´
        "mlflow_version": mlflow.__version__,
        "signature_created_at": pd.Timestamp.now().isoformat(),
        "schema_created_at": pd.Timestamp.now().isoformat(),
        # Phase í†µí•© ì¶”ì 
        "phase_integration": {
            "phase_1_schema_first": True,
            "phase_2_point_in_time": True,
            "phase_3_secure_sql": True,
            "phase_4_auto_validation": True,
            "phase_5_enhanced_artifact": True,
        },
    }

    logger.debug(
        f"[MLFLOW] Signature ìƒì„± ì™„ë£Œ - ì…ë ¥: {len(available_input_cols)}ì—´, ëª¨ë¸: {len(model_feature_cols)}ì—´"
    )

    return signature, data_schema


def log_enhanced_model_with_schema(
    python_model,
    signature: ModelSignature,
    data_schema: dict,
    input_example: pd.DataFrame,
    pip_requirements: Optional[List[str]] = None,
):
    """
    ê¸°ì¡´ mlflow.pyfunc.log_model + í™•ì¥ëœ ë©”íƒ€ë°ì´í„° ì €ì¥

    ê¸°ì¡´ MLflow ì €ì¥ ê¸°ëŠ¥ì„ ë³´ì¡´í•˜ë©´ì„œ ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ì €ì¥.
    100% ì¬í˜„ì„±ê³¼ ìê¸° ê¸°ìˆ ì„±ì„ ë³´ì¥í•˜ëŠ” Enhanced Artifact êµ¬í˜„.

    Args:
        python_model: PyfuncWrapper ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        signature (ModelSignature): Enhanced Model Signature
        data_schema (dict): ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„°
        input_example (pd.DataFrame): ì…ë ¥ ì˜ˆì œ ë°ì´í„°
    """
    log_mlflow("ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì‹œì‘")

    # Signatureì™€ í˜¸í™˜ë˜ëŠ” input_example ìƒì„± - ì›ë³¸ íƒ€ì… ìœ ì§€
    sig_input_names = [col.name for col in signature.inputs.inputs]
    filtered_example = input_example[
        [c for c in sig_input_names if c in input_example.columns]
    ].copy()

    # 1. ê¸°ì¡´ MLflow ì €ì¥ ë¡œì§ í™œìš© (ê²€ì¦ëœ ê¸°ëŠ¥ ë³´ì¡´)
    mlflow.pyfunc.log_model(
        name="model",
        python_model=python_model,
        signature=signature,
        pip_requirements=pip_requirements,
        input_example=filtered_example,
        metadata={"data_schema": json.dumps(data_schema)},
    )
    logger.debug("[MLFLOW] Model ë¡œê·¸ ì™„ë£Œ")

    # 2. ì™„ì „í•œ ìŠ¤í‚¤ë§ˆ ë©”íƒ€ë°ì´í„° ì €ì¥
    mlflow.log_dict(data_schema, "model/data_schema.json")
    logger.debug("[MLFLOW] Data schema ì €ì¥ ì™„ë£Œ")

    # 3. í˜¸í™˜ì„± ë° ë²„ì „ ì •ë³´ ì €ì¥
    compatibility_info = {
        "artifact_version": "2.0",
        "creation_timestamp": pd.Timestamp.now().isoformat(),
        "mlflow_version": mlflow.__version__,
        "schema_validator_version": "2.0",
        # Phaseë³„ ê¸°ëŠ¥ í™œì„±í™” ìƒíƒœ
        "features_enabled": {
            "entity_timestamp_schema": True,
            "point_in_time_correctness": True,
            "sql_injection_protection": True,
            "automatic_schema_validation": True,
            "self_descriptive_artifact": True,
        },
        # í˜¸í™˜ì„± ì •ë³´
        "backward_compatibility": {
            "supports_legacy_models": False,
            "requires_enhanced_pipeline": True,
        },
        # í’ˆì§ˆ ë³´ì¦ ì •ë³´
        "quality_assurance": {
            "schema_drift_protection": True,
            "data_leakage_prevention": True,
            "reproducibility_guaranteed": True,
        },
    }
    mlflow.log_dict(compatibility_info, "model/compatibility_info.json")
    logger.debug("[MLFLOW] í˜¸í™˜ì„± ì •ë³´ ì €ì¥ ì™„ë£Œ")

    # 4. Phase í†µí•© ìš”ì•½ ì •ë³´ ì €ì¥
    phase_summary = {
        "phase_1": {
            "name": "Schema-First ì„¤ê³„",
            "achievements": ["Entity+Timestamp í•„ìˆ˜í™”", "EntitySchema êµ¬í˜„", "Recipe êµ¬ì¡° í˜„ëŒ€í™”"],
        },
        "phase_2": {
            "name": "Point-in-Time ì•ˆì „ì„±",
            "achievements": ["ASOF JOIN ê²€ì¦", "fetcher í˜„ëŒ€í™”", "ë¯¸ë˜ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€"],
        },
        "phase_3": {
            "name": "ë³´ì•ˆ ê°•í™” Dynamic SQL",
            "achievements": ["SQL Injection ë°©ì§€", "í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê²€ì¦", "ë³´ì•ˆ í…œí”Œë¦¿ í‘œì¤€í™”"],
        },
        "phase_4": {
            "name": "ì¼ê´€ì„± ìë™ ê²€ì¦",
            "achievements": ["Schema Drift ì¡°ê¸° ë°œê²¬", "íƒ€ì… í˜¸í™˜ì„± ì—”ì§„", "ìë™ ê²€ì¦ í†µí•©"],
        },
        "phase_5": {
            "name": "ì™„ì „ ìê¸° ê¸°ìˆ  Artifact",
            "achievements": ["100% ì¬í˜„ì„± ë³´ì¥", "ì™„ì „í•œ ë©”íƒ€ë°ì´í„° ìº¡ìŠí™”", "ìê¸° ê¸°ìˆ ì  êµ¬ì¡°"],
        },
    }
    mlflow.log_dict(phase_summary, "model/phase_integration_summary.json")

    log_mlflow("ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥ ì™„ë£Œ")


# --- Simple results logging helper -------------------------------------------------


def log_training_results(settings: "Settings", metrics: dict, training_results: dict) -> None:
    """
    íŒŒì´í”„ë¼ì¸ì—ì„œ ê°„ê²°í•˜ê²Œ í˜¸ì¶œí•˜ê¸° ìœ„í•œ ê²°ê³¼ ë¡œê¹… í—¬í¼.
    - ë©”íŠ¸ë¦­ ë¡œê¹…
    - HPO(on/off) ë¶„ê¸° ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°/ìµœì  ì ìˆ˜ ë¡œê¹…
    """
    # 1) Metrics
    if metrics:
        mlflow.log_metrics(metrics)
        # ë©”íŠ¸ë¦­ ë¡œê¹… (4ê°œì”© ë¬¶ì–´ì„œ ì—¬ëŸ¬ ì¤„ë¡œ ì¶œë ¥)
        items = [f"{k}={v:.4f}" if isinstance(v, float) else f"{k}={v}" for k, v in metrics.items()]
        log_mlflow(f"ë©”íŠ¸ë¦­ ê¸°ë¡ ({len(items)}ê°œ)")
        for i in range(0, len(items), 4):
            chunk = items[i : i + 4]
            log_mlflow(f"{', '.join(chunk)}")

    # 2) Hyperparameters / HPO
    hpo = (training_results or {}).get("trainer", {}).get("hyperparameter_optimization")
    if hpo and hpo.get("enabled"):
        best_params = hpo.get("best_params") or {}
        if best_params:
            mlflow.log_params(best_params)
        if "best_score" in hpo:
            mlflow.log_metric("best_score", hpo["best_score"])
        if "total_trials" in hpo:
            mlflow.log_metric("total_trials", hpo["total_trials"])
    else:
        # HPO ë¹„í™œì„±í™” ì‹œì—ë„ ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë¡ì„ ì‹œë„
        try:
            hp = settings.recipe.model.hyperparameters
            if hasattr(hp, "values") and hp.values:
                mlflow.log_params(hp.values)
        except Exception:
            pass
