import yaml
import importlib
from pathlib import Path
from typing import Optional, Dict, Any
from urllib.parse import urlparse

import mlflow
import pandas as pd

from src.core.augmenter import Augmenter, LocalFileAugmenter, BaseAugmenter, PassThroughAugmenter
from src.core.preprocessor import BasePreprocessor, Preprocessor
from src.interface.base_adapter import BaseAdapter
from src.settings import Settings
from src.utils.system.logger import logger
from src.utils.adapters.file_system_adapter import FileSystemAdapter
from src.utils.adapters.bigquery_adapter import BigQueryAdapter
from src.utils.adapters.gcs_adapter import GCSAdapter
from src.utils.adapters.s3_adapter import S3Adapter
# ğŸ†• Blueprint v17.0: Registry íŒ¨í„´ import
from src.core.registry import AdapterRegistry

# RedisëŠ” ì„ íƒì  ì˜ì¡´ì„±ìœ¼ë¡œ ì²˜ë¦¬
try:
    from src.utils.adapters.redis_adapter import RedisAdapter
    HAS_REDIS = True
except ImportError:
    RedisAdapter = None
    HAS_REDIS = False

class PyfuncWrapper(mlflow.pyfunc.PythonModel):
    """
    ì™„ì „í•œ Wrapped Artifact êµ¬í˜„: Blueprint v17.0
    í•™ìŠµ ì‹œì ì˜ ëª¨ë“  ë¡œì§ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì™„ì „íˆ ìº¡ìŠí™”í•œ ìê¸° ì™„ê²°ì  ì•„í‹°íŒ©íŠ¸
    + í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê²°ê³¼ ë° Data Leakage ë°©ì§€ ë©”íƒ€ë°ì´í„° í¬í•¨
    """
    def __init__(
        self,
        trained_model,
        trained_preprocessor: Optional[BasePreprocessor],
        trained_augmenter: BaseAugmenter,
        loader_sql_snapshot: str,
        augmenter_sql_snapshot: str,
        recipe_yaml_snapshot: str,
        training_metadata: Dict[str, Any],
        # ğŸ†• ìƒˆë¡œìš´ ì¸ìë“¤ (Optionalë¡œ í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥)
        model_class_path: Optional[str] = None,
        hyperparameter_optimization: Optional[Dict[str, Any]] = None,
        training_methodology: Optional[Dict[str, Any]] = None,
    ):
        # í•™ìŠµëœ ì»´í¬ë„ŒíŠ¸ë“¤
        self.trained_model = trained_model
        self.trained_preprocessor = trained_preprocessor
        self.trained_augmenter = trained_augmenter
        
        # ë¡œì§ì˜ ì™„ì „í•œ ìŠ¤ëƒ…ìƒ·
        self.loader_sql_snapshot = loader_sql_snapshot
        self.augmenter_sql_snapshot = augmenter_sql_snapshot
        self.recipe_yaml_snapshot = recipe_yaml_snapshot
        
        # ë©”íƒ€ë°ì´í„°
        self.training_metadata = training_metadata
        
        # ğŸ†• ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° (Blueprint v17.0)
        self.model_class_path = model_class_path
        self.hyperparameter_optimization = hyperparameter_optimization or {"enabled": False}
        self.training_methodology = training_methodology or {}
        
        # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
        self.augmenter = trained_augmenter
        self.preprocessor = trained_preprocessor
        self.model = trained_model
        self.loader_uri = training_metadata.get("loader_uri", "")
        self.recipe_snapshot = training_metadata.get("recipe_snapshot", {})

    def predict(
        self, context, model_input: pd.DataFrame, params: Optional[Dict[str, Any]] = None
    ) -> pd.DataFrame:
        """
        ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ì˜ˆì¸¡ ì‹¤í–‰ (Blueprint v13.0)
        ë°°ì¹˜ ì¶”ë¡ ê³¼ API ì„œë¹™ ëª¨ë‘ ì§€ì›í•˜ëŠ” í†µí•© ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸
        """
        params = params or {}
        run_mode = params.get("run_mode", "serving")
        return_intermediate = params.get("return_intermediate", False)

        logger.info(f"PyfuncWrapper.predict ì‹¤í–‰ ì‹œì‘ (ëª¨ë“œ: {run_mode})")

        # 1. í”¼ì²˜ ì¦ê°• (Augmentation)
        if run_mode == "batch":
            # ë°°ì¹˜ ëª¨ë“œ: SQL ì§ì ‘ ì‹¤í–‰
            augmented_df = self.trained_augmenter.augment_batch(
                model_input, 
                sql_snapshot=self.augmenter_sql_snapshot,
                context_params=params.get("context_params", {})
            )
        else:
            # ì‹¤ì‹œê°„ ëª¨ë“œ: Feature Store ì¡°íšŒ
            augmented_df = self.trained_augmenter.augment_realtime(
                model_input,
                sql_snapshot=self.augmenter_sql_snapshot,
                feature_store_config=params.get("feature_store_config"),
                feature_columns=params.get("feature_columns")
            )

        # 2. ì „ì²˜ë¦¬ (Preprocessing)
        if self.trained_preprocessor:
            preprocessed_df = self.trained_preprocessor.transform(augmented_df)
        else:
            preprocessed_df = augmented_df

        # 3. ëª¨ë¸ ì¶”ë¡  (Prediction)
        predictions = self.trained_model.predict(preprocessed_df)

        # 4. ê²°ê³¼ ì •ë¦¬
        results_df = model_input.merge(
            pd.DataFrame(predictions, index=model_input.index, columns=["uplift_score"]),
            left_index=True,
            right_index=True,
        )
        logger.info("PyfuncWrapper.predict ì‹¤í–‰ ì™„ë£Œ.")

        if return_intermediate and run_mode == "batch":
            return {
                "final_results": results_df,
                "augmented_data": augmented_df,
                "preprocessed_data": preprocessed_df,
                # ğŸ†• ìµœì í™” ë©”íƒ€ë°ì´í„° í¬í•¨ (Blueprint v17.0)
                "hyperparameter_optimization": self.hyperparameter_optimization,
                "training_methodology": self.training_methodology,
            }
        else:
            return results_df

class Factory:
    """
    ì„¤ì •(settings)ê³¼ URI ìŠ¤í‚´(scheme)ì— ê¸°ë°˜í•˜ì—¬ ëª¨ë“  í•µì‹¬ ì»´í¬ë„ŒíŠ¸ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì•™ íŒ©í† ë¦¬ í´ë˜ìŠ¤.
    """
    def __init__(self, settings: Settings):
        self.settings = settings
        logger.info("Factoryê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def create_data_adapter(self, adapter_purpose: str = "loader", source_path: str = None) -> BaseAdapter:
        """
        ğŸ†• Blueprint v17.0: Config-driven Dynamic Factory
        
        í™˜ê²½ë³„ ì–´ëŒ‘í„° ë§¤í•‘ê³¼ ë™ì  ìƒì„±ì„ í†µí•´ Blueprint ì›ì¹™ì„ ì™„ì „íˆ êµ¬í˜„í•©ë‹ˆë‹¤.
        - ì›ì¹™ 1: "ë ˆì‹œí”¼ëŠ” ë…¼ë¦¬, ì„¤ì •ì€ ì¸í”„ë¼" - config ê¸°ë°˜ ì–´ëŒ‘í„° ì„ íƒ
        - ì›ì¹™ 9: "í™˜ê²½ë³„ ì°¨ë“±ì  ê¸°ëŠ¥ ë¶„ë¦¬" - ë™ì¼í•œ ë…¼ë¦¬ ê²½ë¡œê°€ í™˜ê²½ë³„ë¡œ ë‹¤ë¥¸ ì–´ëŒ‘í„°
        
        Args:
            adapter_purpose: ì–´ëŒ‘í„° ëª©ì  ("loader", "storage", "feature_store")
            source_path: ë…¼ë¦¬ì  ê²½ë¡œ (ì„ íƒì , ì–´ëŒ‘í„° ì´ˆê¸°í™”ì— ì‚¬ìš©)
            
        Returns:
            BaseAdapter: í™˜ê²½ë³„ë¡œ ë™ì ìœ¼ë¡œ ìƒì„±ëœ ì–´ëŒ‘í„°
            
        Example:
            # LOCAL í™˜ê²½: FileSystemAdapter ìƒì„±
            adapter = factory.create_data_adapter("loader", "recipes/sql/loaders/user_spine.sql")
            
            # PROD í™˜ê²½: BigQueryAdapter ìƒì„± (ë™ì¼í•œ ë…¼ë¦¬ ê²½ë¡œ)
            adapter = factory.create_data_adapter("loader", "recipes/sql/loaders/user_spine.sql")
        """
        logger.info(f"Config-driven Dynamic Factory: {adapter_purpose} ì–´ëŒ‘í„° ìƒì„± ì‹œì‘")
        
        # 1. data_adapters ì„¤ì • í™•ì¸
        if not self.settings.data_adapters:
            raise ValueError(
                "data_adapters ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. config/*.yamlì— data_adapters ì„¹ì…˜ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."
            )
        
        # 2. ëª©ì ë³„ ê¸°ë³¸ ì–´ëŒ‘í„° ì¡°íšŒ
        try:
            adapter_name = self.settings.data_adapters.get_default_adapter(adapter_purpose)
            logger.info(f"í™˜ê²½ '{self.settings.environment.app_env}'ì—ì„œ {adapter_purpose} ëª©ì ìœ¼ë¡œ '{adapter_name}' ì–´ëŒ‘í„° ì„ íƒ")
        except ValueError as e:
            raise ValueError(f"ì–´ëŒ‘í„° ëª©ì  ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # 3. ì–´ëŒ‘í„° ì„¤ì • ì¡°íšŒ
        try:
            adapter_config = self.settings.data_adapters.get_adapter_config(adapter_name)
            logger.info(f"ì–´ëŒ‘í„° '{adapter_name}' ì„¤ì • ì¡°íšŒ ì™„ë£Œ: {adapter_config.class_name}")
        except ValueError as e:
            raise ValueError(f"ì–´ëŒ‘í„° ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # 4. ë™ì  ì–´ëŒ‘í„° í´ë˜ìŠ¤ import
        try:
            adapter_class = self._get_adapter_class(adapter_config.class_name)
            logger.info(f"ì–´ëŒ‘í„° í´ë˜ìŠ¤ '{adapter_config.class_name}' ë™ì  import ì™„ë£Œ")
        except Exception as e:
            raise ValueError(f"ì–´ëŒ‘í„° í´ë˜ìŠ¤ import ì‹¤íŒ¨: {adapter_config.class_name}, ì˜¤ë¥˜: {e}")
        
        # 5. ì–´ëŒ‘í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        try:
            adapter_instance = adapter_class(
                config=adapter_config.config,
                settings=self.settings,
                source_path=source_path
            )
            logger.info(f"ì–´ëŒ‘í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ: {adapter_config.class_name}")
            return adapter_instance
            
        except Exception as e:
            logger.error(f"ì–´ëŒ‘í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {adapter_config.class_name}, ì˜¤ë¥˜: {e}")
            # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback ì‹œë„
            try:
                logger.warning("ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback ì‹œë„")
                adapter_instance = adapter_class(self.settings)
                logger.info(f"Fallback ì–´ëŒ‘í„° ìƒì„± ì„±ê³µ: {adapter_config.class_name}")
                return adapter_instance
            except Exception as fallback_error:
                raise ValueError(
                    f"ì–´ëŒ‘í„° ìƒì„± ì‹¤íŒ¨: {adapter_config.class_name}\n"
                    f"ìƒˆë¡œìš´ ë°©ì‹ ì˜¤ë¥˜: {e}\n"
                    f"Fallback ì˜¤ë¥˜: {fallback_error}"
                )
    
    def _get_adapter_class(self, class_name: str):
        """
        ğŸ†• Blueprint v17.0: Registry íŒ¨í„´ ê¸°ë°˜ ì–´ëŒ‘í„° í´ë˜ìŠ¤ ì¡°íšŒ
        ê¸°ì¡´ í´ë˜ìŠ¤ëª… -> ì–´ëŒ‘í„° íƒ€ì… ë³€í™˜ í›„ Registryì—ì„œ ì¡°íšŒ
        
        Args:
            class_name: ì–´ëŒ‘í„° í´ë˜ìŠ¤ ì´ë¦„ (e.g., "FileSystemAdapter")
            
        Returns:
            ì–´ëŒ‘í„° í´ë˜ìŠ¤ ê°ì²´
        """
        # í´ë˜ìŠ¤ëª… -> ì–´ëŒ‘í„° íƒ€ì… ë§¤í•‘ (í•˜ìœ„ í˜¸í™˜ì„±)
        class_to_type_mapping = {
            "FileSystemAdapter": "filesystem",
            "BigQueryAdapter": "bigquery",
            "GCSAdapter": "gcs",
            "S3Adapter": "s3",
            "PostgreSQLAdapter": "postgresql",
            "RedisAdapter": "redis",
            "FeatureStoreAdapter": "feature_store",
            "OptunaAdapter": "optuna",
        }
        
        # 1. í´ë˜ìŠ¤ëª…ì„ ì–´ëŒ‘í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        adapter_type = class_to_type_mapping.get(class_name)
        if not adapter_type:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–´ëŒ‘í„° í´ë˜ìŠ¤: {class_name}")
        
        # 2. Registryì—ì„œ ì–´ëŒ‘í„° í´ë˜ìŠ¤ ì¡°íšŒ
        registered_adapters = AdapterRegistry.get_registered_adapters()
        if adapter_type not in registered_adapters:
            available_types = list(registered_adapters.keys())
            raise ValueError(
                f"Registryì— ë“±ë¡ë˜ì§€ ì•Šì€ ì–´ëŒ‘í„° íƒ€ì…: '{adapter_type}'\n"
                f"ì‚¬ìš© ê°€ëŠ¥í•œ íƒ€ì…: {available_types}"
            )
        
        adapter_class = registered_adapters[adapter_type]
        logger.info(f"Registryì—ì„œ ì–´ëŒ‘í„° í´ë˜ìŠ¤ ì¡°íšŒ: {class_name} -> {adapter_type} -> {adapter_class.__name__}")
        
        return adapter_class
        
    # ğŸ”„ ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
    def create_data_adapter_legacy(self, scheme: str) -> BaseAdapter:
        """
        ğŸ”„ ê¸°ì¡´ URI ìŠ¤í‚´ ê¸°ë°˜ ì–´ëŒ‘í„° ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
        
        âš ï¸ DEPRECATED: ìƒˆë¡œìš´ ì½”ë“œì—ì„œëŠ” create_data_adapter() ì‚¬ìš© ê¶Œì¥
        """
        logger.warning(f"DEPRECATED: URI ìŠ¤í‚´ ê¸°ë°˜ ì–´ëŒ‘í„° ìƒì„± (scheme: {scheme}). ìƒˆë¡œìš´ config ê¸°ë°˜ ë°©ì‹ ì‚¬ìš© ê¶Œì¥")
        
        if scheme == 'file':
            return FileSystemAdapter(self.settings)
        elif scheme == 'bq':
            return BigQueryAdapter(self.settings)
        elif scheme == 'gs':
            return GCSAdapter(self.settings)
        elif scheme == 's3':
            return S3Adapter(self.settings)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° ì–´ëŒ‘í„° ìŠ¤í‚´ì…ë‹ˆë‹¤: {scheme}")

    def create_redis_adapter(self):
        if not HAS_REDIS:
            logger.warning("Redis ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ Redis ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            raise ImportError("Redis ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. `pip install redis`ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        
        logger.info("Redis ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        return RedisAdapter(self.settings.serving.realtime_feature_store)

    def create_augmenter(self) -> "BaseAugmenter":
        """Augmenter ìƒì„±"""
        # Blueprint ì›ì¹™ 9: í™˜ê²½ë³„ ì°¨ë“±ì  ê¸°ëŠ¥ ë¶„ë¦¬
        # LOCAL í™˜ê²½ì—ì„œëŠ” PassThroughAugmenterë¥¼ ê°•ì œ ì‚¬ìš©í•˜ì—¬ ë¹ ë¥¸ ì‹¤í—˜ ì§€ì›
        if self.settings.environment.app_env == "local":
            logger.info("LOCAL í™˜ê²½: PassThroughAugmenter ìƒì„± (Blueprint ì›ì¹™ 9 - ì˜ë„ì  ì œì•½)")
            from src.core.augmenter import PassThroughAugmenter
            return PassThroughAugmenter(settings=self.settings)
        else:
            logger.info("DEV/PROD í™˜ê²½: FeatureStore ì—°ë™ Augmenter ìƒì„±")
            from src.core.augmenter import Augmenter
            return Augmenter(settings=self.settings, factory=self)

    def create_preprocessor(self) -> "BasePreprocessor":
        preprocessor_config = self.settings.model.preprocessor
        if not preprocessor_config: return None
        return Preprocessor(config=preprocessor_config, settings=self.settings)

    def create_model(self):
        """
        ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§ì ‘ ë¡œë”© ê¸°ë°˜ ë™ì  ëª¨ë¸ ìƒì„± ì‹œìŠ¤í…œ
        ë¬´ì œí•œì ì¸ YAML ê¸°ë°˜ ì‹¤í—˜ ììœ ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        """
        class_path = self.settings.model.class_path
        
        try:
            # ëª¨ë“ˆ ê²½ë¡œì™€ í´ë˜ìŠ¤ ì´ë¦„ ë¶„ë¦¬
            module_path, class_name = class_path.rsplit('.', 1)
            
            # ë™ì  ëª¨ë“ˆ ë¡œë“œ
            module = importlib.import_module(module_path)
            model_class = getattr(module, class_name)
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì „ë‹¬í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            hyperparams = self.settings.model.hyperparameters.root
            
            logger.info(f"ì™¸ë¶€ ëª¨ë¸ ë¡œë”©: {class_path}")
            return model_class(**hyperparams)
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {class_path}, ì˜¤ë¥˜: {e}")
            raise ValueError(f"ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {class_path}") from e

    def create_evaluator(self):
        """task_typeì— ë”°ë¥¸ ë™ì  evaluator ìƒì„±"""
        # Dynamic importë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
        from src.core.evaluator import (
            ClassificationEvaluator,
            RegressionEvaluator,
            ClusteringEvaluator,
            CausalEvaluator,
        )
        
        task_type = self.settings.model.data_interface.task_type
        data_interface = self.settings.model.data_interface
        
        evaluator_map = {
            "classification": ClassificationEvaluator,
            "regression": RegressionEvaluator,
            "clustering": ClusteringEvaluator,
            "causal": CausalEvaluator,
        }
        
        if task_type not in evaluator_map:
            supported_types = list(evaluator_map.keys())
            raise ValueError(
                f"ì§€ì›í•˜ì§€ ì•ŠëŠ” task_type: '{task_type}'. "
                f"ì§€ì› ê°€ëŠ¥í•œ íƒ€ì…: {supported_types}"
            )
        
        logger.info(f"'{task_type}' íƒ€ì…ìš© evaluatorë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        return evaluator_map[task_type](data_interface)

    # ğŸ†• ìƒˆë¡œìš´ ë©”ì„œë“œë“¤ ì¶”ê°€
    def create_feature_store_adapter(self):
        """í™˜ê²½ë³„ Feature Store ì–´ëŒ‘í„° ìƒì„±"""
        if not self.settings.feature_store:
            raise ValueError("Feature Store ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info("Feature Store ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        from src.utils.adapters.feature_store_adapter import FeatureStoreAdapter
        return FeatureStoreAdapter(self.settings)
    
    def create_optuna_adapter(self):
        """Optuna SDK ë˜í¼ ìƒì„±"""
        if not self.settings.hyperparameter_tuning:
            raise ValueError("Hyperparameter tuning ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info("Optuna ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        from src.utils.adapters.optuna_adapter import OptunaAdapter
        return OptunaAdapter(self.settings.hyperparameter_tuning)
    
    def create_tuning_utils(self):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìœ í‹¸ë¦¬í‹° ìƒì„±"""
        logger.info("Tuning ìœ í‹¸ë¦¬í‹°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        from src.utils.system.tuning_utils import TuningUtils
        return TuningUtils()

    def create_pyfunc_wrapper(
        self, 
        trained_model, 
        trained_preprocessor: Optional[BasePreprocessor],
        training_results: Optional[Dict[str, Any]] = None  # ğŸ†• Trainer ê²°ê³¼ ì „ë‹¬
    ) -> PyfuncWrapper:
        """
        ì™„ì „í•œ Wrapped Artifact ìƒì„± (Blueprint v17.0)
        í•™ìŠµ ì‹œì ì˜ ëª¨ë“  ë¡œì§ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì™„ì „íˆ ìº¡ìŠí™”
        + í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê²°ê³¼ ë° Data Leakage ë°©ì§€ ë©”íƒ€ë°ì´í„° í¬í•¨
        """
        logger.info("ì™„ì „í•œ Wrapped Artifact ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # 1. í•™ìŠµëœ Augmenter ìƒì„±
        trained_augmenter = self.create_augmenter()
        
        # 2. SQL ìŠ¤ëƒ…ìƒ· ìƒì„±
        loader_sql_snapshot = self._create_loader_sql_snapshot()
        augmenter_sql_snapshot = self._create_augmenter_sql_snapshot()
        
        # 3. Recipe YAML ìŠ¤ëƒ…ìƒ· ìƒì„±
        recipe_yaml_snapshot = self._create_recipe_yaml_snapshot()
        
        # 4. ë©”íƒ€ë°ì´í„° ìƒì„±
        training_metadata = self._create_training_metadata()
        
        # ğŸ†• 5. ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ (Blueprint v17.0)
        model_class_path = self.settings.model.class_path
        hyperparameter_optimization = None
        training_methodology = None
        
        if training_results:
            hyperparameter_optimization = training_results.get('hyperparameter_optimization')
            training_methodology = training_results.get('training_methodology')
        
        # 6. í™•ì¥ëœ Wrapper ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
        return PyfuncWrapper(
            trained_model=trained_model,
            trained_preprocessor=trained_preprocessor,
            trained_augmenter=trained_augmenter,
            loader_sql_snapshot=loader_sql_snapshot,
            augmenter_sql_snapshot=augmenter_sql_snapshot,
            recipe_yaml_snapshot=recipe_yaml_snapshot,
            training_metadata=training_metadata,
            # ğŸ†• ìƒˆë¡œìš´ ì¸ìë“¤
            model_class_path=model_class_path,
            hyperparameter_optimization=hyperparameter_optimization,
            training_methodology=training_methodology,
        )
    
    def _create_loader_sql_snapshot(self) -> str:
        """Loader SQL ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        loader_uri = self.settings.model.loader.source_uri
        
        if loader_uri.startswith("bq://"):
            # SQL íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
            sql_path = loader_uri.replace("bq://", "")
            sql_file = Path(sql_path)
            
            if sql_file.exists():
                return sql_file.read_text(encoding="utf-8")
        
        return ""
    
    def _create_augmenter_sql_snapshot(self) -> str:
        """Augmenter SQL ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        if not self.settings.model.augmenter:
            return ""
        
        augmenter_uri = self.settings.model.augmenter.source_uri
        
        # pass_through augmenterëŠ” source_uriê°€ ì—†ì„ ìˆ˜ ìˆìŒ
        if not augmenter_uri:
            return ""
        
        if augmenter_uri.startswith("bq://"):
            sql_path = augmenter_uri.replace("bq://", "")
            sql_file = Path(sql_path)
            
            if sql_file.exists():
                return sql_file.read_text(encoding="utf-8")
        
        return ""
    
    def _create_recipe_yaml_snapshot(self) -> str:
        """Recipe YAML ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        recipe_file = self.settings.model.computed["recipe_file"]
        recipe_path = Path(f"recipes/{recipe_file}.yaml")
        
        if recipe_path.exists():
            return recipe_path.read_text(encoding="utf-8")
        
        return ""
    
    def _create_training_metadata(self) -> Dict[str, Any]:
        """í•™ìŠµ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        from datetime import datetime
        
        return {
            "training_timestamp": datetime.now().isoformat(),
            "model_class": self.settings.model.computed["model_class_name"],
            "recipe_file": self.settings.model.computed["recipe_file"],
            "run_name": self.settings.model.computed["run_name"],
            "class_path": self.settings.model.class_path,
            "hyperparameters": self.settings.model.hyperparameters.root,
            "loader_uri": self.settings.model.loader.source_uri,
            "recipe_snapshot": self.settings.model.dict(),
        }
