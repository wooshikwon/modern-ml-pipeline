import yaml
import importlib
from pathlib import Path
from typing import Optional, Dict, Any
from urllib.parse import urlparse

import mlflow
import pandas as pd

from src.core.augmenter import Augmenter, LocalFileAugmenter, BaseAugmenter
from src.core.preprocessor import BasePreprocessor, Preprocessor
from src.interface.base_adapter import BaseAdapter
from src.settings.settings import Settings
from src.utils.system.logger import logger
from src.utils.adapters.file_system_adapter import FileSystemAdapter
from src.utils.adapters.bigquery_adapter import BigQueryAdapter
from src.utils.adapters.gcs_adapter import GCSAdapter
from src.utils.adapters.s3_adapter import S3Adapter

# RedisëŠ” ì„ íƒì  ì˜ì¡´ì„±ìœ¼ë¡œ ì²˜ë¦¬
try:
    from src.utils.adapters.redis_adapter import RedisAdapter
    HAS_REDIS = True
except ImportError:
    RedisAdapter = None
    HAS_REDIS = False

class PyfuncWrapper(mlflow.pyfunc.PythonModel):
    """
    ì™„ì „í•œ Wrapped Artifact êµ¬í˜„: Blueprint v17.0
    í•™ìŠµ ì‹œì ì˜ ëª¨ë“  ë¡œì§ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì™„ì „íˆ ìº¡ìŠí™”í•œ ìê¸° ì™„ê²°ì  ì•„í‹°íŒ©íŠ¸
    + í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê²°ê³¼ ë° Data Leakage ë°©ì§€ ë©”íƒ€ë°ì´í„° í¬í•¨
    """
    def __init__(
        self,
        trained_model,
        trained_preprocessor: Optional[BasePreprocessor],
        trained_augmenter: BaseAugmenter,
        loader_sql_snapshot: str,
        augmenter_sql_snapshot: str,
        recipe_yaml_snapshot: str,
        training_metadata: Dict[str, Any],
        # ğŸ†• ìƒˆë¡œìš´ ì¸ìë“¤ (Optionalë¡œ í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥)
        model_class_path: Optional[str] = None,
        hyperparameter_optimization: Optional[Dict[str, Any]] = None,
        training_methodology: Optional[Dict[str, Any]] = None,
    ):
        # í•™ìŠµëœ ì»´í¬ë„ŒíŠ¸ë“¤
        self.trained_model = trained_model
        self.trained_preprocessor = trained_preprocessor
        self.trained_augmenter = trained_augmenter
        
        # ë¡œì§ì˜ ì™„ì „í•œ ìŠ¤ëƒ…ìƒ·
        self.loader_sql_snapshot = loader_sql_snapshot
        self.augmenter_sql_snapshot = augmenter_sql_snapshot
        self.recipe_yaml_snapshot = recipe_yaml_snapshot
        
        # ë©”íƒ€ë°ì´í„°
        self.training_metadata = training_metadata
        
        # ğŸ†• ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° (Blueprint v17.0)
        self.model_class_path = model_class_path
        self.hyperparameter_optimization = hyperparameter_optimization or {"enabled": False}
        self.training_methodology = training_methodology or {}
        
        # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
        self.augmenter = trained_augmenter
        self.preprocessor = trained_preprocessor
        self.model = trained_model
        self.loader_uri = training_metadata.get("loader_uri", "")
        self.recipe_snapshot = training_metadata.get("recipe_snapshot", {})

    def predict(
        self, context, model_input: pd.DataFrame, params: Optional[Dict[str, Any]] = None
    ) -> pd.DataFrame:
        """
        ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ì˜ˆì¸¡ ì‹¤í–‰ (Blueprint v13.0)
        ë°°ì¹˜ ì¶”ë¡ ê³¼ API ì„œë¹™ ëª¨ë‘ ì§€ì›í•˜ëŠ” í†µí•© ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸
        """
        params = params or {}
        run_mode = params.get("run_mode", "serving")
        return_intermediate = params.get("return_intermediate", False)

        logger.info(f"PyfuncWrapper.predict ì‹¤í–‰ ì‹œì‘ (ëª¨ë“œ: {run_mode})")

        # 1. í”¼ì²˜ ì¦ê°• (Augmentation)
        if run_mode == "batch":
            # ë°°ì¹˜ ëª¨ë“œ: SQL ì§ì ‘ ì‹¤í–‰
            augmented_df = self.trained_augmenter.augment_batch(
                model_input, 
                sql_snapshot=self.augmenter_sql_snapshot,
                context_params=params.get("context_params", {})
            )
        else:
            # ì‹¤ì‹œê°„ ëª¨ë“œ: Feature Store ì¡°íšŒ
            augmented_df = self.trained_augmenter.augment_realtime(
                model_input,
                sql_snapshot=self.augmenter_sql_snapshot,
                feature_store_config=params.get("feature_store_config"),
                feature_columns=params.get("feature_columns")
            )

        # 2. ì „ì²˜ë¦¬ (Preprocessing)
        if self.trained_preprocessor:
            preprocessed_df = self.trained_preprocessor.transform(augmented_df)
        else:
            preprocessed_df = augmented_df

        # 3. ëª¨ë¸ ì¶”ë¡  (Prediction)
        predictions = self.trained_model.predict(preprocessed_df)

        # 4. ê²°ê³¼ ì •ë¦¬
        results_df = model_input.merge(
            pd.DataFrame(predictions, index=model_input.index, columns=["uplift_score"]),
            left_index=True,
            right_index=True,
        )
        logger.info("PyfuncWrapper.predict ì‹¤í–‰ ì™„ë£Œ.")

        if return_intermediate and run_mode == "batch":
            return {
                "final_results": results_df,
                "augmented_data": augmented_df,
                "preprocessed_data": preprocessed_df,
                # ğŸ†• ìµœì í™” ë©”íƒ€ë°ì´í„° í¬í•¨ (Blueprint v17.0)
                "hyperparameter_optimization": self.hyperparameter_optimization,
                "training_methodology": self.training_methodology,
            }
        else:
            return results_df

class Factory:
    """
    ì„¤ì •(settings)ê³¼ URI ìŠ¤í‚´(scheme)ì— ê¸°ë°˜í•˜ì—¬ ëª¨ë“  í•µì‹¬ ì»´í¬ë„ŒíŠ¸ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì•™ íŒ©í† ë¦¬ í´ë˜ìŠ¤.
    """
    def __init__(self, settings: Settings):
        self.settings = settings
        logger.info("Factoryê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def create_data_adapter(self, scheme: str) -> BaseAdapter:
        logger.info(f"'{scheme}' ìŠ¤í‚´ì— ëŒ€í•œ ë°ì´í„° ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        if scheme == 'file':
            return FileSystemAdapter(self.settings)
        elif scheme == 'bq':
            return BigQueryAdapter(self.settings)
        elif scheme == 'gs':
            return GCSAdapter(self.settings)
        elif scheme == 's3':
            return S3Adapter(self.settings)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° ì–´ëŒ‘í„° ìŠ¤í‚´ì…ë‹ˆë‹¤: {scheme}")

    def create_redis_adapter(self):
        if not HAS_REDIS:
            logger.warning("Redis ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ Redis ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            raise ImportError("Redis ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. `pip install redis`ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        
        logger.info("Redis ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        return RedisAdapter(self.settings.serving.realtime_feature_store)

    def create_augmenter(self) -> BaseAugmenter:
        """
        ğŸ†• Blueprint v17.0: Feature Store ë°©ì‹ê³¼ ê¸°ì¡´ SQL ë°©ì‹ ëª¨ë‘ ì§€ì›í•˜ëŠ” Augmenter ìƒì„±
        """
        augmenter_config = self.settings.model.augmenter
        if not augmenter_config:
            raise ValueError("Augmenter ì„¤ì •ì´ ë ˆì‹œí”¼ì— ì—†ìŠµë‹ˆë‹¤.")
        
        # ë¡œì»¬ í™˜ê²½ì—ì„œ local_override_uriê°€ ìˆëŠ” ê²½ìš° (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        is_local = self.settings.environment.app_env == "local"
        if is_local and hasattr(augmenter_config, 'local_override_uri') and augmenter_config.local_override_uri:
            logger.info("ë¡œì»¬ í™˜ê²½: LocalFileAugmenter ì‚¬ìš©")
            return LocalFileAugmenter(uri=augmenter_config.local_override_uri)
        
        # ğŸ†• Feature Store ë°©ì‹ ì²´í¬
        if hasattr(augmenter_config, 'type') and augmenter_config.type == "feature_store":
            logger.info("ğŸ†• Feature Store ë°©ì‹ Augmenter ìƒì„±")
            return Augmenter(
                source_uri=None,
                settings=self.settings,
                augmenter_config=augmenter_config.dict()  # Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜
            )
        else:
            # ğŸ”„ ê¸°ì¡´ SQL ë°©ì‹ (ì™„ì „ í˜¸í™˜ì„± ìœ ì§€)
            logger.info("ğŸ”„ ê¸°ì¡´ SQL ë°©ì‹ Augmenter ìƒì„±")
            source_uri = augmenter_config.source_uri if hasattr(augmenter_config, 'source_uri') else None
            if not source_uri:
                raise ValueError("ê¸°ì¡´ SQL ë°©ì‹ Augmenterì—ëŠ” source_uriê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            return Augmenter(
                source_uri=source_uri,
                settings=self.settings,
                augmenter_config={'type': 'sql'}  # ê¸°ì¡´ ë°©ì‹ ëª…ì‹œ
            )

    def create_preprocessor(self) -> Optional[BasePreprocessor]:
        preprocessor_config = self.settings.model.preprocessor
        if not preprocessor_config: return None
        return Preprocessor(config=preprocessor_config, settings=self.settings)

    def create_model(self):
        """
        ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§ì ‘ ë¡œë”© ê¸°ë°˜ ë™ì  ëª¨ë¸ ìƒì„± ì‹œìŠ¤í…œ
        ë¬´ì œí•œì ì¸ YAML ê¸°ë°˜ ì‹¤í—˜ ììœ ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        """
        class_path = self.settings.model.class_path
        
        try:
            # ëª¨ë“ˆ ê²½ë¡œì™€ í´ë˜ìŠ¤ ì´ë¦„ ë¶„ë¦¬
            module_path, class_name = class_path.rsplit('.', 1)
            
            # ë™ì  ëª¨ë“ˆ ë¡œë“œ
            module = importlib.import_module(module_path)
            model_class = getattr(module, class_name)
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì „ë‹¬í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            hyperparams = self.settings.model.hyperparameters.root
            
            logger.info(f"ì™¸ë¶€ ëª¨ë¸ ë¡œë”©: {class_path}")
            return model_class(**hyperparams)
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {class_path}, ì˜¤ë¥˜: {e}")
            raise ValueError(f"ëª¨ë¸ í´ë˜ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {class_path}") from e

    def create_evaluator(self):
        """task_typeì— ë”°ë¥¸ ë™ì  evaluator ìƒì„±"""
        # Dynamic importë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
        from src.core.evaluator import (
            ClassificationEvaluator,
            RegressionEvaluator,
            ClusteringEvaluator,
            CausalEvaluator,
        )
        
        task_type = self.settings.model.data_interface.task_type
        data_interface = self.settings.model.data_interface
        
        evaluator_map = {
            "classification": ClassificationEvaluator,
            "regression": RegressionEvaluator,
            "clustering": ClusteringEvaluator,
            "causal": CausalEvaluator,
        }
        
        if task_type not in evaluator_map:
            supported_types = list(evaluator_map.keys())
            raise ValueError(
                f"ì§€ì›í•˜ì§€ ì•ŠëŠ” task_type: '{task_type}'. "
                f"ì§€ì› ê°€ëŠ¥í•œ íƒ€ì…: {supported_types}"
            )
        
        logger.info(f"'{task_type}' íƒ€ì…ìš© evaluatorë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        return evaluator_map[task_type](data_interface)

    # ğŸ†• ìƒˆë¡œìš´ ë©”ì„œë“œë“¤ ì¶”ê°€
    def create_feature_store_adapter(self):
        """í™˜ê²½ë³„ Feature Store ì–´ëŒ‘í„° ìƒì„±"""
        if not self.settings.feature_store:
            raise ValueError("Feature Store ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info("Feature Store ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        from src.utils.adapters.feature_store_adapter import FeatureStoreAdapter
        return FeatureStoreAdapter(self.settings)
    
    def create_optuna_adapter(self):
        """Optuna SDK ë˜í¼ ìƒì„±"""
        if not self.settings.hyperparameter_tuning:
            raise ValueError("Hyperparameter tuning ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info("Optuna ì–´ëŒ‘í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        from src.utils.adapters.optuna_adapter import OptunaAdapter
        return OptunaAdapter(self.settings.hyperparameter_tuning)
    
    def create_tuning_utils(self):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìœ í‹¸ë¦¬í‹° ìƒì„±"""
        logger.info("Tuning ìœ í‹¸ë¦¬í‹°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        from src.utils.system.tuning_utils import TuningUtils
        return TuningUtils()

    def create_pyfunc_wrapper(
        self, 
        trained_model, 
        trained_preprocessor: Optional[BasePreprocessor],
        training_results: Optional[Dict[str, Any]] = None  # ğŸ†• Trainer ê²°ê³¼ ì „ë‹¬
    ) -> PyfuncWrapper:
        """
        ì™„ì „í•œ Wrapped Artifact ìƒì„± (Blueprint v17.0)
        í•™ìŠµ ì‹œì ì˜ ëª¨ë“  ë¡œì§ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì™„ì „íˆ ìº¡ìŠí™”
        + í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê²°ê³¼ ë° Data Leakage ë°©ì§€ ë©”íƒ€ë°ì´í„° í¬í•¨
        """
        logger.info("ì™„ì „í•œ Wrapped Artifact ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # 1. í•™ìŠµëœ Augmenter ìƒì„±
        trained_augmenter = self.create_augmenter()
        
        # 2. SQL ìŠ¤ëƒ…ìƒ· ìƒì„±
        loader_sql_snapshot = self._create_loader_sql_snapshot()
        augmenter_sql_snapshot = self._create_augmenter_sql_snapshot()
        
        # 3. Recipe YAML ìŠ¤ëƒ…ìƒ· ìƒì„±
        recipe_yaml_snapshot = self._create_recipe_yaml_snapshot()
        
        # 4. ë©”íƒ€ë°ì´í„° ìƒì„±
        training_metadata = self._create_training_metadata()
        
        # ğŸ†• 5. ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ (Blueprint v17.0)
        model_class_path = self.settings.model.class_path
        hyperparameter_optimization = None
        training_methodology = None
        
        if training_results:
            hyperparameter_optimization = training_results.get('hyperparameter_optimization')
            training_methodology = training_results.get('training_methodology')
        
        # 6. í™•ì¥ëœ Wrapper ìƒì„± (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
        return PyfuncWrapper(
            trained_model=trained_model,
            trained_preprocessor=trained_preprocessor,
            trained_augmenter=trained_augmenter,
            loader_sql_snapshot=loader_sql_snapshot,
            augmenter_sql_snapshot=augmenter_sql_snapshot,
            recipe_yaml_snapshot=recipe_yaml_snapshot,
            training_metadata=training_metadata,
            # ğŸ†• ìƒˆë¡œìš´ ì¸ìë“¤
            model_class_path=model_class_path,
            hyperparameter_optimization=hyperparameter_optimization,
            training_methodology=training_methodology,
        )
    
    def _create_loader_sql_snapshot(self) -> str:
        """Loader SQL ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        loader_uri = self.settings.model.loader.source_uri
        
        if loader_uri.startswith("bq://"):
            # SQL íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
            sql_path = loader_uri.replace("bq://", "")
            sql_file = Path(sql_path)
            
            if sql_file.exists():
                return sql_file.read_text(encoding="utf-8")
        
        return ""
    
    def _create_augmenter_sql_snapshot(self) -> str:
        """Augmenter SQL ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        if not self.settings.model.augmenter:
            return ""
        
        augmenter_uri = self.settings.model.augmenter.source_uri
        
        if augmenter_uri.startswith("bq://"):
            sql_path = augmenter_uri.replace("bq://", "")
            sql_file = Path(sql_path)
            
            if sql_file.exists():
                return sql_file.read_text(encoding="utf-8")
        
        return ""
    
    def _create_recipe_yaml_snapshot(self) -> str:
        """Recipe YAML ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        recipe_file = self.settings.model.computed["recipe_file"]
        recipe_path = Path(f"recipes/{recipe_file}.yaml")
        
        if recipe_path.exists():
            return recipe_path.read_text(encoding="utf-8")
        
        return ""
    
    def _create_training_metadata(self) -> Dict[str, Any]:
        """í•™ìŠµ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        from datetime import datetime
        
        return {
            "training_timestamp": datetime.now().isoformat(),
            "model_class": self.settings.model.computed["model_class_name"],
            "recipe_file": self.settings.model.computed["recipe_file"],
            "run_name": self.settings.model.computed["run_name"],
            "class_path": self.settings.model.class_path,
            "hyperparameters": self.settings.model.hyperparameters.root,
            "loader_uri": self.settings.model.loader.source_uri,
            "recipe_snapshot": self.settings.model.dict(),
        }
