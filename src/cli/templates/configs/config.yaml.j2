# Configuration for {{ env_name }} environment
# Generated by MMP get-config

environment:
  name: {{ env_name }}
  # Environment-specific settings are defined in each adapter/store config

{%- if use_mlflow %}

mlflow:
  tracking_uri: "${MLFLOW_TRACKING_URI:./mlruns}"
  experiment_name: "${MLFLOW_EXPERIMENT_NAME:mmp-{{ env_name }}}"
  # Optional authentication
  tracking_username: "${MLFLOW_TRACKING_USERNAME:}"
  tracking_password: "${MLFLOW_TRACKING_PASSWORD:}"
  # Optional S3-compatible storage endpoint
  s3_endpoint_url: "${MLFLOW_S3_ENDPOINT_URL:}"
{%- endif %}

data_source:
  name: {{ data_source }}
  {%- if data_source == "PostgreSQL" %}
  adapter_type: sql
  config:
    connection_uri: "postgresql://${DB_USER:postgres}:${DB_PASSWORD:postgres}@${DB_HOST:localhost}:${DB_PORT:5432}/${DB_NAME:mmp_db}"
    query_timeout: ${DB_TIMEOUT:30}
  {%- elif data_source == "BigQuery" %}
  adapter_type: bigquery
  config:
    project_id: "${GCP_PROJECT_ID:{{ gcp_project|default('') }}}"
    dataset_id: "${BQ_DATASET_ID:mmp_dataset}"
    location: "${BQ_LOCATION:US}"
    query_timeout: ${BQ_TIMEOUT:30}
  {%- elif data_source == "Local Files" %}
  adapter_type: storage
  config:
    base_path: "${DATA_PATH:./data}"
    storage_options: {}
  {%- elif data_source == "S3" %}
  adapter_type: storage
  config:
    base_path: "s3://${S3_BUCKET:mmp-data}/${S3_PREFIX:{{ env_name }}}"
    storage_options:
      aws_access_key_id: "${AWS_ACCESS_KEY_ID:}"
      aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY:}"
      region_name: "${AWS_REGION:us-east-1}"
  {%- elif data_source == "GCS" %}
  adapter_type: storage
  config:
    base_path: "gs://${GCS_BUCKET:mmp-data}/${GCS_PREFIX:{{ env_name }}}"
    storage_options:
      project: "${GCP_PROJECT_ID:{{ gcp_project|default('') }}}"
      token: "${GOOGLE_APPLICATION_CREDENTIALS:}"
  {%- endif %}

{%- if use_feast %}

feature_store:
  provider: feast
  feast_config:
    project: "${FEAST_PROJECT:feast_{{ env_name }}}"
    # Registry는 feature/entity 정의를 저장하는 메타데이터 DB
    # 실제 데이터가 아닌 "어떤 feature가 있는지" 정보만 저장
    registry: "${FEAST_REGISTRY_PATH:./feast_repo/registry.db}"
    # Online store는 실시간 서빙용 (선택사항)
    online_store:
      {%- if feast_online_store == "Redis" %}
      type: redis
      connection_string: "${REDIS_HOST:localhost}:${REDIS_PORT:6379}"
      {%- if feast_online_store == "Redis" %}
      password: "${REDIS_PASSWORD:}"
      {%- endif %}
      {%- elif feast_online_store == "DynamoDB" %}
      type: dynamodb
      region: "${DYNAMODB_REGION:us-east-1}"
      table_name: "${DYNAMODB_TABLE_NAME:feast-online-store}"
      {%- else %}
      type: sqlite
      path: "${FEAST_ONLINE_STORE_PATH:./feast_repo/online_store.db}"
      {%- endif %}
    # Offline store는 PIT join을 위한 historical features 저장
    # data_source와 동일한 백엔드 사용
    offline_store:
      {%- if data_source == "BigQuery" %}
      type: bigquery
      project_id: "${GCP_PROJECT_ID:{{ gcp_project|default('') }}}"
      dataset_id: "${BQ_DATASET_ID:mmp_dataset}"
      {%- elif data_source in ["S3", "GCS", "Local Files", "PostgreSQL"] %}
      type: file
      # Parquet files for historical feature storage
      path: "${FEAST_OFFLINE_PATH:./feast_repo/data}"
      {%- endif %}
    entity_key_serialization_version: 2
{%- else %}

feature_store:
  provider: none
  # Feature engineering will happen in-memory without persistence
{%- endif %}

{%- if enable_serving %}

serving:
  enabled: true
  host: "${API_HOST:0.0.0.0}"
  port: ${API_PORT:8000}
  workers: ${API_WORKERS:{{ serving_workers|default(1) }}}
  model_stage: "{{ model_stage|default('None') }}"
  {%- if enable_auth %}
  auth:
    enabled: true
    type: "${AUTH_TYPE:jwt}"
    secret_key: "${AUTH_SECRET_KEY:}"
  {%- endif %}
{%- else %}

serving:
  enabled: false
  model_stage: "None"
{%- endif %}

{%- if artifact_storage %}

# MLflow artifact storage configuration
artifact_store:
  {%- if artifact_storage == "Local" %}
  type: local
  config:
    base_path: "${MLFLOW_ARTIFACT_PATH:./mlruns/artifacts}"
  {%- elif artifact_storage == "S3" %}
  type: s3
  config:
    bucket: "${ARTIFACT_S3_BUCKET:mlflow-artifacts}"
    prefix: "${ARTIFACT_S3_PREFIX:{{ env_name }}}"
    # Optional: For S3-compatible storage (MinIO, Ceph, etc.)
    endpoint_url: "${MLFLOW_S3_ENDPOINT_URL:}"
    # AWS credentials are shared with data source if using S3
  {%- elif artifact_storage == "GCS" %}
  type: gcs
  config:
    bucket: "${ARTIFACT_GCS_BUCKET:mlflow-artifacts}"
    prefix: "${ARTIFACT_GCS_PREFIX:{{ env_name }}}"
    # GCP credentials are shared with data source if using GCS
  {%- endif %}
{%- endif %}


# Output targets configuration
output:
  inference:
    name: InferenceOutput
    enabled: ${INFER_OUTPUT_ENABLED:true}
    {%- if inference_output_source in ["Local Files", "S3", "GCS"] %}
    adapter_type: storage
    config:
      {%- if inference_output_source == "Local Files" %}
      base_path: "${INFER_OUTPUT_BASE_PATH:./artifacts/predictions}"
      storage_options: {}
      {%- elif inference_output_source == "S3" %}
      base_path: "s3://${INFER_OUTPUT_S3_BUCKET:mmp-out}/${INFER_OUTPUT_S3_PREFIX:{{ env_name }}/preds}"
      storage_options:
        aws_access_key_id: "${AWS_ACCESS_KEY_ID:}"
        aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY:}"
        region_name: "${AWS_REGION:us-east-1}"
      {%- elif inference_output_source == "GCS" %}
      base_path: "gs://${INFER_OUTPUT_GCS_BUCKET:mmp-out}/${INFER_OUTPUT_GCS_PREFIX:{{ env_name }}/preds}"
      storage_options:
        project: "${GCP_PROJECT_ID:{{ gcp_project|default('') }}}"
        token: "${GOOGLE_APPLICATION_CREDENTIALS:}"
      {%- endif %}
    {%- elif inference_output_source == "PostgreSQL" %}
    adapter_type: sql
    config:
      connection_uri: "postgresql://${DB_USER:postgres}:${DB_PASSWORD:postgres}@${DB_HOST:localhost}:${DB_PORT:5432}/${DB_NAME:mmp_db}"
      table: "${INFER_OUTPUT_PG_TABLE:predictions_{{ env_name }}}"
    {%- elif inference_output_source == "BigQuery" %}
    adapter_type: bigquery
    config:
      project_id: "${GCP_PROJECT_ID:{{ gcp_project|default('') }}}"
      dataset_id: "${INFER_OUTPUT_BQ_DATASET:analytics}"
      table: "${INFER_OUTPUT_BQ_TABLE:predictions_{{ env_name }}}"
      location: "${BQ_LOCATION:US}"
    {%- endif %}

  preprocessed:
    name: PreprocessedOutput
    enabled: ${PREPROC_OUTPUT_ENABLED:true}
    {%- if preproc_output_source in ["Local Files", "S3", "GCS"] %}
    adapter_type: storage
    config:
      {%- if preproc_output_source == "Local Files" %}
      base_path: "${PREPROC_OUTPUT_BASE_PATH:./artifacts/preprocessed}"
      storage_options: {}
      {%- elif preproc_output_source == "S3" %}
      base_path: "s3://${PREPROC_OUTPUT_S3_BUCKET:mmp-out}/${PREPROC_OUTPUT_S3_PREFIX:{{ env_name }}/preproc}"
      storage_options:
        aws_access_key_id: "${AWS_ACCESS_KEY_ID:}"
        aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY:}"
        region_name: "${AWS_REGION:us-east-1}"
      {%- elif preproc_output_source == "GCS" %}
      base_path: "gs://${PREPROC_OUTPUT_GCS_BUCKET:mmp-out}/${PREPROC_OUTPUT_GCS_PREFIX:{{ env_name }}/preproc}"
      storage_options:
        project: "${GCP_PROJECT_ID:{{ gcp_project|default('') }}}"
        token: "${GOOGLE_APPLICATION_CREDENTIALS:}"
      {%- endif %}
    {%- elif preproc_output_source == "PostgreSQL" %}
    adapter_type: sql
    config:
      connection_uri: "postgresql://${DB_USER:postgres}:${DB_PASSWORD:postgres}@${DB_HOST:localhost}:${DB_PORT:5432}/${DB_NAME:mmp_db}"
      table: "${PREPROC_OUTPUT_PG_TABLE:preprocessed_{{ env_name }}}"
    {%- elif preproc_output_source == "BigQuery" %}
    adapter_type: bigquery
    config:
      project_id: "${GCP_PROJECT_ID:{{ gcp_project|default('') }}}"
      dataset_id: "${PREPROC_OUTPUT_BQ_DATASET:feature_store}"
      table: "${PREPROC_OUTPUT_BQ_TABLE:preprocessed_{{ env_name }}}"
      location: "${BQ_LOCATION:US}"
    {%- endif %}

