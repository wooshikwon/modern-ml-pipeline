# src/components/datahandler/modules/deeplearning_handler.py
"""
DeepLearning DataHandler - ë”¥ëŸ¬ë‹ ì „ìš© ë°ì´í„° ì²˜ë¦¬

PyTorch ê¸°ë°˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì„ ìœ„í•œ íŠ¹í™”ëœ ë°ì´í„° ì²˜ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- LSTMìš© ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (3D: samples, seq_len, features)
- ì‹œê°„ ê¸°ì¤€ ë¶„í• ë¡œ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€
- ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬
"""

import numpy as np
import pandas as pd
from typing import Tuple, Dict, Any, Union
from datetime import datetime

from src.interface import BaseDataHandler
from ..registry import DataHandlerRegistry
from src.utils.system.logger import logger


class DeepLearningDataHandler(BaseDataHandler):
    """ë”¥ëŸ¬ë‹ ì „ìš© DataHandler - ì‹œí€€ìŠ¤ ì²˜ë¦¬, ë°°ì¹˜ ìƒì„± íŠ¹í™”"""
    
    def __init__(self, settings):
        super().__init__(settings)
        self.task_type = self.data_interface.task_type
        
        # ë”¥ëŸ¬ë‹ ì „ìš© ì„¤ì •ë“¤ (Recipe Schemaì—ì„œ í™•ì¥ ì˜ˆì •)
        self.sequence_length = getattr(self.data_interface, 'sequence_length', 30)
        self.use_gpu = getattr(self.data_interface, 'use_gpu', True)
        
        logger.info(f"ğŸ§  DeepLearning DataHandler ì´ˆê¸°í™”")
        logger.info(f"   Task Type: {self.task_type}")
        logger.info(f"   Sequence Length: {self.sequence_length}")
        logger.info(f"   Use GPU: {self.use_gpu}")
    
    def validate_data(self, df: pd.DataFrame) -> bool:
        """ë”¥ëŸ¬ë‹ ë°ì´í„° ê²€ì¦"""
        super().validate_data(df)  # ê¸°ë³¸ ê²€ì¦
        
        target_col = self.data_interface.target_column
        
        # Target ì»¬ëŸ¼ ì¡´ì¬ ê²€ì¦
        if target_col not in df.columns:
            raise ValueError(f"Target ì»¬ëŸ¼ '{target_col}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # Taskë³„ ì¶”ê°€ ê²€ì¦
        if self.task_type == "timeseries":
            timestamp_col = self.data_interface.timestamp_column
            if not timestamp_col or timestamp_col not in df.columns:
                raise ValueError(f"TimeSeries taskì— í•„ìš”í•œ timestamp_column '{timestamp_col}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ë°ì´í„° íƒ€ì… ê²€ì¦
            if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):
                try:
                    df[timestamp_col] = pd.to_datetime(df[timestamp_col])
                    logger.info(f"Timestamp ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤: {timestamp_col}")
                except:
                    raise ValueError(f"Timestamp ì»¬ëŸ¼ '{timestamp_col}'ì„ datetimeìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ì‹œí€€ìŠ¤ ê¸¸ì´ ê²€ì¦
            if len(df) < self.sequence_length + 1:
                raise ValueError(f"ì‹œí€€ìŠ¤ ìƒì„±ì„ ìœ„í•´ ìµœì†Œ {self.sequence_length + 1}ê°œ í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤. "
                               f"í˜„ì¬ ë°ì´í„°: {len(df)}ê°œ í–‰")
        
        return True
    
    def prepare_data(self, df: pd.DataFrame) -> Tuple[Union[pd.DataFrame, np.ndarray], Union[pd.Series, np.ndarray], Dict[str, Any]]:
        """ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        # ë°ì´í„° ê²€ì¦
        self.validate_data(df)
        
        if self.task_type == "timeseries":
            return self._prepare_timeseries_sequences(df)
        elif self.task_type in ["classification", "regression"]:
            return self._prepare_tabular_data(df)
        else:
            raise ValueError(f"DeepLearning handlerì—ì„œ ì§€ì›í•˜ì§€ ì•ŠëŠ” task: {self.task_type}")
    
    def _prepare_timeseries_sequences(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:
        """ì‹œê³„ì—´ â†’ LSTM ì‹œí€€ìŠ¤ ë°ì´í„° ë³€í™˜"""
        timestamp_col = self.data_interface.timestamp_column
        target_col = self.data_interface.target_column
        
        # ì‹œê°„ìˆœ ì •ë ¬ (í•„ìˆ˜)
        df = df.sort_values(timestamp_col).reset_index(drop=True)
        
        # Feature columns ì¶”ì¶œ
        exclude_cols = [target_col, timestamp_col] + (self.data_interface.entity_columns or [])
        
        if self.data_interface.feature_columns:
            # ëª…ì‹œì  feature ì„ íƒ
            feature_cols = self.data_interface.feature_columns
            forbidden_cols = [c for c in exclude_cols if c and c in df.columns]
            overlap = set(feature_cols) & set(forbidden_cols)
            if overlap:
                raise ValueError(f"feature_columnsì— ê¸ˆì§€ëœ ì»¬ëŸ¼ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {list(overlap)}. "
                               f"timestamp, target, entity ì»¬ëŸ¼ì€ featureë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ìë™ feature ì„ íƒ: ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì—ì„œ ì œì™¸ ì»¬ëŸ¼ ë¹¼ê¸°
            feature_cols = [col for col in df.select_dtypes(include=[np.number]).columns 
                           if col not in exclude_cols]
        
        logger.info(f"ğŸ“ˆ TimeSeries feature columns ({len(feature_cols)}): {feature_cols[:5]}{'...' if len(feature_cols) > 5 else ''}")
        
        # ê²°ì¸¡ì¹˜ ì²´í¬ ë° ê²½ê³ 
        missing_info = []
        for col in feature_cols:
            if col in df.columns:
                missing_ratio = df[col].isnull().sum() / len(df)
                if missing_ratio > 0.05:  # 5% ì´ìƒ
                    missing_info.append((col, missing_ratio))
        
        if missing_info:
            logger.warning("âš ï¸  ê²°ì¸¡ì¹˜ê°€ ë§ì€ feature ì»¬ëŸ¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
            for col, ratio in missing_info:
                logger.warning(f"   - {col}: {ratio:.1%}")
            logger.warning("   ğŸ’¡ ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”")
        
        # Sliding windowë¡œ ì‹œí€€ìŠ¤ ìƒì„±
        X_sequences, y_sequences = [], []
        
        for i in range(self.sequence_length, len(df)):
            # X: ê³¼ê±° sequence_lengthê°œ ì‹œì ì˜ featureë“¤ (3D: [seq_len, n_features])
            X_seq = df.iloc[i-self.sequence_length:i][feature_cols].values
            # y: í˜„ì¬ ì‹œì ì˜ target ê°’
            y_seq = df.iloc[i][target_col]
            
            # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì‹œí€€ìŠ¤ ìŠ¤í‚µ
            if np.isnan(X_seq).any() or np.isnan(y_seq):
                continue
                
            X_sequences.append(X_seq)
            y_sequences.append(y_seq)
        
        X_sequences = np.array(X_sequences)  # Shape: [n_samples, seq_len, n_features]
        y_sequences = np.array(y_sequences)  # Shape: [n_samples]
        
        logger.info(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {X_sequences.shape} sequences â†’ {y_sequences.shape} targets")
        logger.info(f"   Sequence length: {self.sequence_length}, Features: {X_sequences.shape[-1]}")
        
        # âœ… BaseModel í˜¸í™˜ì„ ìœ„í•´ DataFrameìœ¼ë¡œ ë³€í™˜ (ë©”íƒ€ë°ì´í„°ì— original shape ì €ì¥)
        original_shape = X_sequences.shape  # (n_samples, seq_len, n_features)
        X_flattened = X_sequences.reshape(len(X_sequences), -1)  # (n_samples, seq_len * n_features)
        
        # DataFrameìœ¼ë¡œ ë³€í™˜ (ì»¬ëŸ¼ëª…: seq0_feat0, seq0_feat1, ..., seq1_feat0, ...)
        n_samples, seq_len, n_features = original_shape
        column_names = [f"seq{t}_feat{f}" for t in range(seq_len) for f in range(n_features)]
        X_df = pd.DataFrame(X_flattened, columns=column_names)
        y_series = pd.Series(y_sequences, name='target')
        
        logger.info(f"ğŸ”„ DataFrame ë³€í™˜ ì™„ë£Œ: {original_shape} â†’ {X_df.shape} (BaseModel í˜¸í™˜)")
        
        additional_data = {
            'sequence_length': self.sequence_length,
            'feature_columns': feature_cols,
            'n_features': len(feature_cols),
            'is_timeseries': True,
            'is_timeseries_sequence': True,  # LSTM ëª¨ë¸ì´ ì¸ì‹í•  í”Œë˜ê·¸
            'original_sequence_shape': original_shape,  # (n_samples, seq_len, n_features)
            'data_shape': X_df.shape,  # flattened shape
            'original_timestamps': df[timestamp_col].iloc[self.sequence_length:].reset_index(drop=True)
        }
        
        return X_df, y_series, additional_data
    
    def _prepare_tabular_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, Dict[str, Any]]:
        """ì¼ë°˜ í…Œì´ë¸” ë°ì´í„° â†’ ë”¥ëŸ¬ë‹ìš© ë°°ì¹˜ ì²˜ë¦¬"""
        target_col = self.data_interface.target_column
        
        # Feature selection (ê¸°ì¡´ DataHandler ë¡œì§ê³¼ ë™ì¼)
        exclude_cols = [target_col] + (self.data_interface.entity_columns or [])
        
        if self.data_interface.feature_columns:
            # ëª…ì‹œì  feature ì„ íƒ
            forbidden_cols = [c for c in exclude_cols if c and c in df.columns]
            overlap = set(self.data_interface.feature_columns) & set(forbidden_cols)
            if overlap:
                raise ValueError(f"feature_columnsì— ê¸ˆì§€ëœ ì»¬ëŸ¼ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {list(overlap)}. "
                               f"target, entity ì»¬ëŸ¼ì€ featureë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            X = df[self.data_interface.feature_columns]
        else:
            # ìë™ feature ì„ íƒ
            X = df.drop(columns=[c for c in exclude_cols if c in df.columns])
            X = X.select_dtypes(include=[np.number])
        
        y = df[target_col]
        
        logger.info(f"ğŸ“Š Tabular data prepared: {X.shape} features â†’ {y.shape} targets")
        
        # ê²°ì¸¡ì¹˜ ì²´í¬
        missing_info = []
        for col in X.columns:
            missing_ratio = X[col].isnull().sum() / len(X)
            if missing_ratio > 0.05:  # 5% ì´ìƒ
                missing_info.append((col, missing_ratio))
        
        if missing_info:
            logger.warning("âš ï¸  ê²°ì¸¡ì¹˜ê°€ ë§ì€ feature ì»¬ëŸ¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
            for col, ratio in missing_info:
                logger.warning(f"   - {col}: {ratio:.1%}")
            logger.warning("   ğŸ’¡ ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”")
        
        additional_data = {
            'is_timeseries': False,
            'feature_columns': list(X.columns),
            'n_features': len(X.columns),
            'data_shape': X.shape
        }
        
        return X, y, additional_data
    
    def split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ì€ ì‹œê°„ ê¸°ì¤€, ì¼ë°˜ì€ random)"""
        if self.task_type == "timeseries":
            return self._time_based_split(df)
        else:
            # ì¼ë°˜ ë°ì´í„°ëŠ” random split
            from sklearn.model_selection import train_test_split
            train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
            logger.info(f"ğŸ“Š Random split ì™„ë£Œ: Train({len(train_df)}) / Test({len(test_df)})")
            return train_df, test_df
    
    def _time_based_split(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ì‹œê³„ì—´ ì‹œê°„ ê¸°ì¤€ ë¶„í•  (Data Leakage ë°©ì§€)"""
        timestamp_col = self.data_interface.timestamp_column
        df_sorted = df.sort_values(timestamp_col).reset_index(drop=True)
        
        # ì‹œí€€ìŠ¤ ìƒì„±ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        min_train_size = self.sequence_length + 10  # ìµœì†Œí•œì˜ í•™ìŠµ ë°ì´í„°
        total_sequences = len(df_sorted) - self.sequence_length
        
        if total_sequences < 20:  # ìµœì†Œ 20ê°œ ì‹œí€€ìŠ¤
            raise ValueError(f"ì‹œí€€ìŠ¤ ìƒì„± í›„ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. "
                           f"ì „ì²´ {len(df_sorted)}í–‰ â†’ {total_sequences}ê°œ ì‹œí€€ìŠ¤. "
                           f"ìµœì†Œ {self.sequence_length + 20}í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # 80-20 ë¶„í• 
        split_idx = int(len(df_sorted) * 0.8)
        
        # trainì—ì„œë„ ì‹œí€€ìŠ¤ ìƒì„±ì´ ê°€ëŠ¥í•˜ë„ë¡ ì¡°ì •
        if split_idx < min_train_size:
            split_idx = min_train_size
            logger.warning(f"Train ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„í•  ì§€ì ì„ ì¡°ì •í–ˆìŠµë‹ˆë‹¤: {split_idx}")
        
        train_df = df_sorted.iloc[:split_idx].copy()
        test_df = df_sorted.iloc[split_idx:].copy()
        
        train_sequences = len(train_df) - self.sequence_length
        test_sequences = len(test_df) - self.sequence_length
        
        train_period = f"{train_df[timestamp_col].min()} ~ {train_df[timestamp_col].max()}"
        test_period = f"{test_df[timestamp_col].min()} ~ {test_df[timestamp_col].max()}"
        
        logger.info(f"ğŸ• ì‹œê³„ì—´ ì‹œê°„ ê¸°ì¤€ ë¶„í• :")
        logger.info(f"   Train ({len(train_df)}í–‰ â†’ ~{max(0, train_sequences)}ê°œ ì‹œí€€ìŠ¤): {train_period}")
        logger.info(f"   Test ({len(test_df)}í–‰ â†’ ~{max(0, test_sequences)}ê°œ ì‹œí€€ìŠ¤): {test_period}")
        
        return train_df, test_df
    
    def get_data_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ë”¥ëŸ¬ë‹ ë°ì´í„° ë©”íƒ€ ì •ë³´ ë°˜í™˜"""
        base_info = super().get_data_info(df)
        
        # ë”¥ëŸ¬ë‹ íŠ¹í™” ì •ë³´ ì¶”ê°€
        deep_learning_info = {
            'task_type': self.task_type,
            'sequence_length': self.sequence_length,
            'estimated_sequences': max(0, len(df) - self.sequence_length) if self.task_type == "timeseries" else None,
            'use_gpu': self.use_gpu
        }
        
        # Target ì •ë³´
        target_col = self.data_interface.target_column
        if target_col in df.columns:
            deep_learning_info['target_info'] = {
                'name': target_col,
                'dtype': str(df[target_col].dtype),
                'unique_values': df[target_col].nunique(),
                'missing_ratio': df[target_col].isnull().sum() / len(df),
                'stats': df[target_col].describe().to_dict() if df[target_col].dtype in ['int64', 'float64'] else None
            }
        
        # Timestamp ì •ë³´ (ì‹œê³„ì—´ì¸ ê²½ìš°)
        if self.task_type == "timeseries":
            timestamp_col = self.data_interface.timestamp_column
            if timestamp_col and timestamp_col in df.columns:
                deep_learning_info['timestamp_info'] = {
                    'name': timestamp_col,
                    'dtype': str(df[timestamp_col].dtype),
                    'date_range': f"{df[timestamp_col].min()} ~ {df[timestamp_col].max()}",
                    'frequency_hint': self._estimate_frequency(df[timestamp_col])
                }
        
        # ë‘ ë”•ì…”ë„ˆë¦¬ ë³‘í•©
        return {**base_info, 'deeplearning_specific': deep_learning_info}
    
    def _estimate_frequency(self, timestamp_series: pd.Series) -> str:
        """ì‹œê³„ì—´ ì£¼ê¸° ì¶”ì •"""
        try:
            # ì •ë ¬ëœ íƒ€ì„ìŠ¤íƒ¬í”„ì˜ ê°„ê²©ë“¤ ê³„ì‚°
            sorted_ts = timestamp_series.dropna().sort_values()
            if len(sorted_ts) < 2:
                return "unknown"
            
            deltas = sorted_ts.diff().dropna()
            if len(deltas) == 0:
                return "unknown"
            
            # ê°€ì¥ ë¹ˆë²ˆí•œ ê°„ê²© ì°¾ê¸°
            mode_delta = deltas.mode()
            if len(mode_delta) == 0:
                return "unknown"
            
            mode_seconds = mode_delta.iloc[0].total_seconds()
            
            # ì¼ë°˜ì ì¸ ì£¼ê¸°ë“¤ê³¼ ë§¤ì¹­
            if mode_seconds == 86400:  # 1 day
                return "daily"
            elif mode_seconds == 3600:  # 1 hour
                return "hourly"
            elif mode_seconds == 60:  # 1 minute
                return "minutely"
            elif mode_seconds == 604800:  # 1 week
                return "weekly"
            elif 2505600 <= mode_seconds <= 2678400:  # 29-31 days
                return "monthly"
            else:
                return f"~{mode_seconds}s"
                
        except Exception:
            return "unknown"


# Registry ìë™ ë“±ë¡
DataHandlerRegistry.register("deeplearning", DeepLearningDataHandler)