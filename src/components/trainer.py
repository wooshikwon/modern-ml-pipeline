from __future__ import annotations
import pandas as pd
from typing import Dict, Any, Tuple, Optional, TYPE_CHECKING
from datetime import datetime

from sklearn.model_selection import train_test_split

from src.settings import Settings
from src.utils.system.logger import logger
from src.interface.base_trainer import BaseTrainer
from src.utils.system.schema_utils import validate_schema

if TYPE_CHECKING:
    from src.components.augmenter import BaseAugmenter
    from src.components.preprocessor import BasePreprocessor


class Trainer(BaseTrainer):
    """
    ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì „ì²´ ê³¼ì •ì„ ê´€ì¥í•˜ëŠ” í´ë˜ìŠ¤.
    í˜„ëŒ€í™”ëœ Recipe êµ¬ì¡° ì „ìš© (settings.recipe.model)
    """

    def __init__(self, settings: Settings):
        self.settings = settings
        logger.info("Trainerê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def train(self, df, model, augmenter, preprocessor, context_params=None):
        """
        í•™ìŠµ ì§„ì…ì . í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í™œì„±í™” ì—¬ë¶€ì— ë”°ë¼ ê²½ë¡œ ë¶„ê¸°.
        """
        tuning_config = self.settings.recipe.model.hyperparameter_tuning
        if tuning_config and tuning_config.enabled:
            return self._train_with_hyperparameter_optimization(df, model, augmenter, preprocessor, context_params)
        else:
            return self._train_with_fixed_hyperparameters(df, model, augmenter, preprocessor, context_params)

    def _train_with_hyperparameter_optimization(self, df, model, augmenter, preprocessor, context_params):
        """Optuna ê¸°ë°˜ ìë™ ìµœì í™” (ë‚´ë¶€ ë©”ì„œë“œ)"""
        logger.info("ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ìµœì í™” ëª¨ë“œ ì‹œì‘")
        
        # ê¸°ë³¸ ì„¤ì • ê²€ì¦ ë° ë°ì´í„° ë¶„í• 
        self.settings.recipe.model.data_interface.validate_required_fields()  # ğŸ”„ ìˆ˜ì •: entity_schema â†’ data_interface
        train_df, test_df = self._split_data(df)
        
        # í”¼ì²˜ ì¦ê°•
        if augmenter:
            logger.info("í”¼ì²˜ ì¦ê°•ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            train_df = augmenter.augment(train_df, run_mode="batch", context_params=context_params)
            test_df = augmenter.augment(test_df, run_mode="batch", context_params=context_params)
        
        # Optuna ê´€ë ¨ ì»´í¬ë„ŒíŠ¸ ìƒì„±
        from src.engine.factory import Factory
        factory = Factory(self.settings)
        
        try:
            optuna_integration = factory.create_optuna_integration()
            tuning_utils = factory.create_tuning_utils()
        except (ValueError, ImportError) as e:
            logger.warning(f"Optuna ì»´í¬ë„ŒíŠ¸ ìƒì„± ì‹¤íŒ¨, ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì§„í–‰: {e}")
            return self._train_with_fixed_hyperparameters(df, model, augmenter, preprocessor, context_params)
        
        # Optuna Study ìƒì„±
        study = optuna_integration.create_study(
            direction=self.settings.recipe.model.hyperparameter_tuning.direction,
            study_name=f"study_{self.settings.recipe.model.computed['run_name']}"
        )
        
        start_time = datetime.now()
        
        # Optuna ìµœì í™” ì‹¤í–‰
        try:
            def objective(trial):
                # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
                params = tuning_utils.suggest_hyperparameters_from_recipe(
                    trial, self.settings.recipe.model.hyperparameters
                )
                
                # ë‹¨ì¼ í•™ìŠµ iteration ì‹¤í–‰
                result = self._single_training_iteration(train_df, params, trial.number)
                return result['score']
            
            # Study ì‹¤í–‰
            study.optimize(
                objective, 
                n_trials=self.settings.recipe.model.hyperparameter_tuning.n_trials,
                timeout=getattr(self.settings.hyperparameter_tuning, 'timeout', None)
            )
            
            # ìµœì  ê²°ê³¼ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
            best_params = study.best_params
            logger.info(f"âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params}")
            
            # ìµœì¢… ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
            final_result = self._single_training_iteration(train_df, best_params, seed=42)
            
            # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€
            trained_model = final_result['model']
            trained_preprocessor = final_result['preprocessor']
            
            # Test ë°ì´í„°ë¡œ ìµœì¢… í‰ê°€
            evaluator = factory.create_evaluator()
            X_test, y_test, _ = self._prepare_training_data(test_df)
            
            if trained_preprocessor:
                X_test_processed = trained_preprocessor.transform(X_test)
            else:
                X_test_processed = X_test
            
            final_metrics = evaluator.evaluate(trained_model, X_test_processed, y_test, test_df)
            
            # ê²°ê³¼ ì¤€ë¹„
            end_time = datetime.now()
            optimization_time = (end_time - start_time).total_seconds()
            
            training_results = {
                'hyperparameter_optimization': {
                    'enabled': True,
                    'engine': 'optuna',
                    'best_params': best_params,
                    'best_score': study.best_value,
                    'optimization_history': [trial.value for trial in study.trials if trial.value is not None],
                    'total_trials': len(study.trials),
                    'pruned_trials': len([t for t in study.trials if t.state.name == 'PRUNED']),
                    'optimization_time': optimization_time,
                    'search_space': tuning_utils.extract_search_space_from_recipe(self.settings.recipe.model.hyperparameters)
                },
                'training_methodology': {
                    'train_test_split_method': 'stratified',
                    'train_ratio': 0.8,
                    'validation_strategy': 'train_validation_split_per_trial',
                    'random_state': 42,
                    'preprocessing_fit_scope': 'train_only'
                }
            }
            
            logger.info(f"ğŸ‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ! ìµœê³  ì ìˆ˜: {study.best_value:.4f} ({optimization_time:.1f}ì´ˆ)")
            return trained_model, trained_preprocessor, final_metrics, training_results
            
        except Exception as e:
            logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            # Fallback: ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì§„í–‰
            return self._train_with_fixed_hyperparameters(df, model, augmenter, preprocessor, context_params)

    def _train_with_fixed_hyperparameters(self, df, model, augmenter, preprocessor, context_params):
        """ê¸°ì¡´ ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°©ì‹ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)"""
        logger.info("ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ëª¨ë“œ (ê¸°ì¡´ ë°©ì‹)")
        
        # ê¸°ì¡´ train ë©”ì„œë“œì˜ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        self.settings.recipe.model.data_interface.validate_required_fields()
        task_type = self.settings.recipe.model.data_interface.task_type
        
        # ë°ì´í„° ë¶„í• 
        train_df, test_df = self._split_data(df)
        
        # í”¼ì²˜ ì¦ê°•
        if augmenter:
            logger.info("í”¼ì²˜ ì¦ê°•ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            train_df = augmenter.augment(train_df, run_mode="batch", context_params=context_params)
            test_df = augmenter.augment(test_df, run_mode="batch", context_params=context_params)
            logger.info("í”¼ì²˜ ì¦ê°• ì™„ë£Œ.")
        else:
            logger.info("Augmenterê°€ ì£¼ì…ë˜ì§€ ì•Šì•„ í”¼ì²˜ ì¦ê°•ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        # ë™ì  ë°ì´í„° ì¤€ë¹„
        X_train, y_train, additional_data = self._prepare_training_data(train_df)
        X_test, y_test, _ = self._prepare_training_data(test_df)
        
        # ì›ë³¸ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ê²€ì¦ (entity, timestamp, target ì»¬ëŸ¼ ìˆëŠ”ì§€)
        validate_schema(train_df, self.settings, for_training=False)

        # ì „ì²˜ë¦¬ê¸° í•™ìŠµ ë° ë³€í™˜ (ì£¼ì…ë°›ì€ Preprocessor ì‚¬ìš©)
        if preprocessor:
            logger.info("ì „ì²˜ë¦¬ê¸° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            preprocessor.fit(X_train)  # â† âœ… Data Leakage ë°©ì§€
            X_train_processed = preprocessor.transform(X_train)
            X_test_processed = preprocessor.transform(X_test)
            logger.info("ì „ì²˜ë¦¬ê¸° í•™ìŠµ ë° ë³€í™˜ ì™„ë£Œ.")
        else:
            X_train_processed = X_train
            X_test_processed = X_test
            logger.info("Preprocessorê°€ ì£¼ì…ë˜ì§€ ì•Šì•„ ì „ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

        # ìŠ¤í‚¤ë§ˆ ê²€ì¦ (ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°)
        validate_schema(X_train_processed, self.settings, for_training=True)

        # ë™ì  ëª¨ë¸ í•™ìŠµ
        logger.info(f"'{self.settings.recipe.model.class_path}' ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        self._fit_model(model, X_train_processed, y_train, additional_data)
        logger.info("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")

        # í‰ê°€ìë¡œ í‰ê°€ ìˆ˜í–‰
        from src.engine.factory import Factory
        factory = Factory(self.settings)
        evaluator = factory.create_evaluator()
        
        metrics = evaluator.evaluate(model, X_test_processed, y_test, test_df)
        
        # ê¸°ë³¸ training_results (HPO ì—†ìŒ)
        training_results = {
            'hyperparameter_optimization': None,
            'training_methodology': {
                'train_test_split_method': 'stratified',
                'train_ratio': 0.8,
                'validation_strategy': 'train_test_split',
                'random_state': 42,
                'preprocessing_fit_scope': 'train_only'
            }
        }
        
        return model, preprocessor, metrics, training_results

    def _single_training_iteration(self, train_df, params, seed):
        """í•µì‹¬: Data Leakage ë°©ì§€ + ë‹¨ì¼ í•™ìŠµ ë¡œì§"""
        
        # 1. Train/Validation Split (Data Leakage ë°©ì§€)
        train_data, val_data = train_test_split(
            train_df, test_size=0.2, random_state=seed, 
            stratify=self._get_stratify_column_data(train_df)
        )
        
        # 2. ë™ì  ë°ì´í„° ì¤€ë¹„
        X_train, y_train, additional_data = self._prepare_training_data(train_data)
        X_val, y_val, _ = self._prepare_training_data(val_data)
        
        # 3. Preprocessor fit (Train only) â† âœ… Data Leakage ë°©ì§€
        from src.engine.factory import Factory
        factory = Factory(self.settings)
        preprocessor = factory.create_preprocessor()
        
        if preprocessor:
            preprocessor.fit(X_train)  # Train ë°ì´í„°ì—ë§Œ fit
            X_train_processed = preprocessor.transform(X_train)
            X_val_processed = preprocessor.transform(X_val)
        else:
            X_train_processed = X_train
            X_val_processed = X_val
        
        # 4. Model ìƒì„± ë° í•™ìŠµ (ë™ì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©)
        tuning_utils = factory.create_tuning_utils()
        model_instance = tuning_utils.create_model_with_params(self.settings.recipe.model.class_path, params)
        self._fit_model(model_instance, X_train_processed, y_train, additional_data)
        
        # 5. í‰ê°€
        evaluator = factory.create_evaluator()
        metrics = evaluator.evaluate(model_instance, X_val_processed, y_val, val_data)
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ (tuningì— ì‚¬ìš©)
        score = tuning_utils.extract_optimization_score(
            metrics, self.settings.recipe.model.hyperparameter_tuning.metric
        )
        
        return {
            'model': model_instance,
            'preprocessor': preprocessor,
            'score': score,
            'metrics': metrics,
            'params': params
        }

    def _prepare_training_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, Dict[str, Any]]:
        """ë™ì  ë°ì´í„° ì¤€ë¹„ (task_typeì— ë”°ë¼ ë‹¤ë¦„)"""
        data_interface = self.settings.recipe.model.data_interface
        task_type = data_interface.task_type
        
        # ê³µí†µ: Featureì™€ Target ë¶„ë¦¬
        if task_type in ["classification", "regression"]:
            target_col = data_interface.target_column
            X = df.drop(columns=[target_col])
            y = df[target_col]
            additional_data = {}
            
        elif task_type == "clustering":
            # Clustering: target ì—†ìŒ
            X = df.copy()
            y = None
            additional_data = {}
            
        elif task_type == "causal":
            # Causal: treatmentì™€ target ëª¨ë‘ í•„ìš”
            target_col = data_interface.target_column
            treatment_col = data_interface.treatment_column
            X = df.drop(columns=[target_col, treatment_col])
            y = df[target_col]
            additional_data = {
                'treatment': df[treatment_col],
                'treatment_value': data_interface.treatment_value
            }
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” task_type: {task_type}")
        
        return X, y, additional_data

    def _fit_model(self, model, X, y, additional_data):
        """ë™ì  ëª¨ë¸ í•™ìŠµ (task_typeë³„ ì²˜ë¦¬)"""
        data_interface = self.settings.recipe.model.data_interface
        task_type = data_interface.task_type
        
        if task_type in ["classification", "regression"]:
            model.fit(X, y)
        elif task_type == "clustering":
            model.fit(X)  # y ì—†ìŒ
        elif task_type == "causal":
            # CausalML ëª¨ë¸ë“¤: X, y, treatment ëª¨ë‘ í•„ìš”
            treatment = additional_data['treatment']
            model.fit(X, treatment, y)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” task_type: {task_type}")

    def _split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Train/Test ë¶„í•  (stratify ì§€ì›)"""
        data_interface = self.settings.recipe.model.data_interface
        stratify_data = self._get_stratify_column_data(df)
        
        train_df, test_df = train_test_split(
            df, test_size=0.2, random_state=42, stratify=stratify_data
        )
        return train_df, test_df

    def _get_stratify_column_data(self, df: pd.DataFrame):
        """Stratifyìš© ì»¬ëŸ¼ ë°ì´í„° ì¶”ì¶œ"""
        data_interface = self.settings.recipe.model.data_interface
        task_type = data_interface.task_type
        
        if task_type == "classification":
            # ë¶„ë¥˜: target ì»¬ëŸ¼ìœ¼ë¡œ stratify
            target_col = data_interface.target_column
            return df[target_col] if target_col in df.columns else None
        elif task_type == "causal":
            # ì¸ê³¼ì¶”ë¡ : treatment ì»¬ëŸ¼ìœ¼ë¡œ stratify
            treatment_col = data_interface.treatment_column
            return df[treatment_col] if treatment_col in df.columns else None
        else:
            # íšŒê·€, í´ëŸ¬ìŠ¤í„°ë§: stratify ì—†ìŒ
            return None