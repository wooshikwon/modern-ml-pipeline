from __future__ import annotations
import pandas as pd
from typing import Dict, Any, Tuple, Optional, TYPE_CHECKING
from datetime import datetime

from sklearn.model_selection import train_test_split

from src.settings import Settings
from src.utils.system.logger import logger
from src.interface.base_trainer import BaseTrainer
from src.utils.system.schema_utils import validate_schema

if TYPE_CHECKING:
    from src.components.augmenter import BaseAugmenter
    from src.components.preprocessor import BasePreprocessor


class Trainer(BaseTrainer):
    """
    ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì „ì²´ ê³¼ì •ì„ ê´€ì¥í•˜ëŠ” í´ë˜ìŠ¤.
    ëª¨ë“  ì˜ì¡´ì„±ì€ ì™¸ë¶€(ì£¼ë¡œ Factory)ë¡œë¶€í„° ì£¼ì…ë°›ëŠ”ë‹¤.
    """

    def __init__(self, settings: Settings):
        self.settings = settings
        logger.info("Trainerê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def train(
        self,
        df: pd.DataFrame,
        model,
        augmenter: Optional[BaseAugmenter] = None,
        preprocessor: Optional[BasePreprocessor] = None,
        context_params: Optional[Dict[str, Any]] = None,
    ) -> Tuple[Optional[BasePreprocessor], Any, Dict[str, Any]]:
        """
        ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€í•˜ë©´ì„œ ë‚´ë¶€ì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì²˜ë¦¬
        """
        logger.info("ëª¨ë¸ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
        context_params = context_params or {}

        # ğŸ†• í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì—¬ë¶€ í™•ì¸
        hyperparameter_tuning_config = self.settings.model.hyperparameter_tuning
        is_tuning_enabled = (
            hyperparameter_tuning_config and 
            hyperparameter_tuning_config.enabled and
            self.settings.hyperparameter_tuning and
            self.settings.hyperparameter_tuning.enabled
        )

        if is_tuning_enabled:
            return self._train_with_hyperparameter_optimization(
                df, model, augmenter, preprocessor, context_params
            )
        else:
            return self._train_with_fixed_hyperparameters(
                df, model, augmenter, preprocessor, context_params
            )
    
    def _train_with_hyperparameter_optimization(self, df, model, augmenter, preprocessor, context_params):
        """Optuna ê¸°ë°˜ ìë™ ìµœì í™” (ë‚´ë¶€ ë©”ì„œë“œ)"""
        logger.info("ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ìµœì í™” ëª¨ë“œ ì‹œì‘")
        
        # ê¸°ë³¸ ì„¤ì • ê²€ì¦ ë° ë°ì´í„° ë¶„í• 
        self.settings.model.data_interface.validate_required_fields()
        train_df, test_df = self._split_data(df)
        
        # í”¼ì²˜ ì¦ê°•
        if augmenter:
            logger.info("í”¼ì²˜ ì¦ê°•ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            train_df = augmenter.augment(train_df, run_mode="batch", context_params=context_params)
            test_df = augmenter.augment(test_df, run_mode="batch", context_params=context_params)
        
        # Optuna ê´€ë ¨ ì»´í¬ë„ŒíŠ¸ ìƒì„±
        from src.engine.factory import Factory
        factory = Factory(self.settings)
        
        try:
            optuna_integration = factory.create_optuna_integration()
            tuning_utils = factory.create_tuning_utils()
        except (ValueError, ImportError) as e:
            logger.warning(f"Optuna ì»´í¬ë„ŒíŠ¸ ìƒì„± ì‹¤íŒ¨, ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì§„í–‰: {e}")
            return self._train_with_fixed_hyperparameters(df, model, augmenter, preprocessor, context_params)
        
        # Optuna Study ìƒì„±
        study = optuna_integration.create_study(
            direction=self.settings.model.hyperparameter_tuning.direction,
            study_name=f"study_{self.settings.model.computed['run_name']}"
        )
        
        start_time = datetime.now()
        
        def objective(trial):
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
            params = optuna_integration.suggest_hyperparameters(
                trial, self.settings.model.hyperparameters.root
            )
            
            # ë‹¨ì¼ í•™ìŠµ ì‹¤í–‰ (Data Leakage ë°©ì§€)
            result = self._single_training_iteration(
                train_df, params, seed=trial.number
            )
            
            # Pruning ì§€ì›
            trial.report(result['score'], step=trial.number)
            if trial.should_prune():
                import optuna
                raise optuna.TrialPruned()
                
            return result['score']
        
        # ìµœì í™” ì‹¤í–‰ (ì‹¤í—˜ ë…¼ë¦¬ + ì¸í”„ë¼ ì œì•½)
        study.optimize(
            objective,
            n_trials=self.settings.model.hyperparameter_tuning.n_trials,
            timeout=self.settings.hyperparameter_tuning.timeout
        )
        
        end_time = datetime.now()
        
        # ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… í•™ìŠµ
        best_params = study.best_params
        final_result = self._single_training_iteration(
            train_df, best_params, seed=42
        )
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€
        final_model = final_result['model']
        final_preprocessor = final_result['preprocessor']
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
        X_test, y_test, _ = self._prepare_training_data(test_df)
        if final_preprocessor:
            X_test_processed = final_preprocessor.transform(X_test)
        else:
            X_test_processed = X_test
        
        evaluator = factory.create_evaluator()
        final_metrics = evaluator.evaluate(final_model, X_test_processed, y_test, test_df)
        
        # ğŸ†• ìµœì í™” ë©”íƒ€ë°ì´í„° í¬í•¨
        results = {
            'metrics': final_metrics,
            'hyperparameter_optimization': tuning_utils.create_optimization_metadata(
                study, start_time, end_time, best_params
            ),
            'training_methodology': {
                'train_test_split_method': 'stratified',
                'preprocessing_fit_scope': 'train_only',  # Data Leakage ë°©ì§€ ì¦ëª…
                'optimization_trials': len(study.trials)
            }
        }
        
        logger.info(f"ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ! ìµœê³  ì ìˆ˜: {study.best_value}, ì´ {len(study.trials)}íšŒ ì‹œë„")
        
        # ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ì™€ í˜¸í™˜ë˜ëŠ” ë°˜í™˜ê°’
        return final_preprocessor, final_model, results
    
    def _train_with_fixed_hyperparameters(self, df, model, augmenter, preprocessor, context_params):
        """ê¸°ì¡´ ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°©ì‹ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)"""
        logger.info("ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„° ëª¨ë“œ (ê¸°ì¡´ ë°©ì‹)")
        
        # ê¸°ì¡´ train ë©”ì„œë“œì˜ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        self.settings.model.data_interface.validate_required_fields()
        task_type = self.settings.model.data_interface.task_type
        
        # ë°ì´í„° ë¶„í• 
        train_df, test_df = self._split_data(df)
        
        # í”¼ì²˜ ì¦ê°•
        if augmenter:
            logger.info("í”¼ì²˜ ì¦ê°•ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            train_df = augmenter.augment(train_df, run_mode="batch", context_params=context_params)
            test_df = augmenter.augment(test_df, run_mode="batch", context_params=context_params)
            logger.info("í”¼ì²˜ ì¦ê°• ì™„ë£Œ.")
        else:
            logger.info("Augmenterê°€ ì£¼ì…ë˜ì§€ ì•Šì•„ í”¼ì²˜ ì¦ê°•ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        # ë™ì  ë°ì´í„° ì¤€ë¹„
        X_train, y_train, additional_data = self._prepare_training_data(train_df)
        X_test, y_test, _ = self._prepare_training_data(test_df)

        # ì „ì²˜ë¦¬ê¸° í•™ìŠµ ë° ë³€í™˜ (ì£¼ì…ë°›ì€ Preprocessor ì‚¬ìš©)
        if preprocessor:
            logger.info("ì „ì²˜ë¦¬ê¸° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
            preprocessor.fit(X_train)  # â† âœ… Data Leakage ë°©ì§€
            X_train_processed = preprocessor.transform(X_train)
            X_test_processed = preprocessor.transform(X_test)
            logger.info("ì „ì²˜ë¦¬ê¸° í•™ìŠµ ë° ë³€í™˜ ì™„ë£Œ.")
        else:
            X_train_processed = X_train
            X_test_processed = X_test
            logger.info("Preprocessorê°€ ì£¼ì…ë˜ì§€ ì•Šì•„ ì „ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

        # ìŠ¤í‚¤ë§ˆ ê²€ì¦
        validate_schema(X_train_processed, self.settings)

        # ë™ì  ëª¨ë¸ í•™ìŠµ
        logger.info(f"'{self.settings.model.class_path}' ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        self._fit_model(model, X_train_processed, y_train, additional_data)
        logger.info("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")

        # ë™ì  í‰ê°€
        from src.engine.factory import Factory
        factory = Factory(self.settings)
        evaluator = factory.create_evaluator()
        metrics = evaluator.evaluate(model, X_test_processed, y_test, test_df)

        results = {
            "metrics": metrics, 
            "hyperparameter_optimization": {"enabled": False},  # ğŸ†• ì¼ê´€ì„± ìœ ì§€
            "training_methodology": {
                "train_test_split_method": "stratified",
                "preprocessing_fit_scope": "train_only"  # Data Leakage ë°©ì§€ ë³´ì¥
            }
        }
        logger.info("ëª¨ë¸ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

        return preprocessor, model, results
    
    def _single_training_iteration(self, train_df, params, seed):
        """í•µì‹¬: Data Leakage ë°©ì§€ + ë‹¨ì¼ í•™ìŠµ ë¡œì§"""
        
        # 1. Train/Validation Split (Data Leakage ë°©ì§€)
        train_data, val_data = train_test_split(
            train_df, test_size=0.2, random_state=seed, 
            stratify=self._get_stratify_column_data(train_df)
        )
        
        # 2. ë™ì  ë°ì´í„° ì¤€ë¹„
        X_train, y_train, additional_data = self._prepare_training_data(train_data)
        X_val, y_val, _ = self._prepare_training_data(val_data)
        
        # 3. Preprocessor fit (Train only) â† âœ… Data Leakage ë°©ì§€
        from src.engine.factory import Factory
        factory = Factory(self.settings)
        preprocessor = factory.create_preprocessor()
        
        if preprocessor:
            preprocessor.fit(X_train)  # Train ë°ì´í„°ì—ë§Œ fit
            X_train_processed = preprocessor.transform(X_train)
            X_val_processed = preprocessor.transform(X_val)
        else:
            X_train_processed = X_train
            X_val_processed = X_val
        
        # 4. Model ìƒì„± ë° í•™ìŠµ (ë™ì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ìš©)
        tuning_utils = factory.create_tuning_utils()
        model_instance = tuning_utils.create_model_with_params(self.settings.model.class_path, params)
        self._fit_model(model_instance, X_train_processed, y_train, additional_data)
        
        # 5. í‰ê°€
        evaluator = factory.create_evaluator()
        metrics = evaluator.evaluate(model_instance, X_val_processed, y_val, val_data)
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ (tuningì— ì‚¬ìš©)
        score = tuning_utils.extract_optimization_score(
            metrics, self.settings.model.hyperparameter_tuning.metric
        )
        
        return {
            'model': model_instance,
            'preprocessor': preprocessor,
            'score': score,
            'metrics': metrics,
            'params': params
        }
    
    def _get_stratify_column_data(self, df):
        """Data Leakage ë°©ì§€ìš© stratify ì»¬ëŸ¼ ë°ì´í„° ë°˜í™˜"""
        from src.utils.system.tuning_utils import TuningUtils
        stratify_col = TuningUtils.get_stratify_column(
            df, 
            self.settings.model.data_interface.task_type,
            self.settings.model.data_interface
        )
        
        if stratify_col:
            return df[stratify_col]
        else:
            return None

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ ìœ ì§€ (ë³€ê²½ ì—†ìŒ)
    def _prepare_training_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[pd.Series], Dict[str, Any]]:
        """task_typeì— ë”°ë¥¸ ë™ì  ë°ì´í„° ì¤€ë¹„"""
        task_type = self.settings.model.data_interface.task_type
        data_interface = self.settings.model.data_interface
        
        # ì œì™¸í•  ì»¬ëŸ¼ë“¤ ë™ì  ê²°ì •
        exclude_cols = []
        if data_interface.target_col:
            exclude_cols.append(data_interface.target_col)
        if data_interface.treatment_col:
            exclude_cols.append(data_interface.treatment_col)
        
        X = df.drop(columns=exclude_cols, errors="ignore")
        
        if task_type == "clustering":
            logger.info("í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸: target ë°ì´í„° ì—†ì´ ì§„í–‰")
            return X, None, {}
        
        y = df[data_interface.target_col]
        
        additional_data = {}
        if task_type == "causal":
            additional_data["treatment"] = df[data_interface.treatment_col]
            logger.info("ì¸ê³¼ì¶”ë¡  ëª¨ë¸: treatment ë°ì´í„° ì¶”ê°€")
        elif task_type == "regression" and data_interface.sample_weight_col:
            additional_data["sample_weight"] = df[data_interface.sample_weight_col]
            logger.info(f"íšŒê·€ ëª¨ë¸: sample_weight ì»¬ëŸ¼ ì‚¬ìš© ({data_interface.sample_weight_col})")
        
        return X, y, additional_data

    def _fit_model(self, model, X: pd.DataFrame, y: Optional[pd.Series], additional_data: Dict[str, Any]):
        """task_typeì— ë”°ë¥¸ ë™ì  ëª¨ë¸ í•™ìŠµ"""
        task_type = self.settings.model.data_interface.task_type
        
        if task_type == "clustering":
            model.fit(X)
        elif task_type == "causal":
            model.fit(X, y, additional_data["treatment"])
        elif task_type == "regression" and "sample_weight" in additional_data:
            model.fit(X, y, sample_weight=additional_data["sample_weight"])
        else:
            # classification, regression (without sample_weight)
            model.fit(X, y)

    def _split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ë°ì´í„°ë¥¼ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤. task_typeì— ë”°ë¼ ì ì ˆí•œ stratify ì»¬ëŸ¼ì„ ì„ íƒí•©ë‹ˆë‹¤."""
        task_type = self.settings.model.data_interface.task_type
        data_interface = self.settings.model.data_interface
        test_size = 0.2
        
        # task_typeì— ë”°ë¼ stratify ì»¬ëŸ¼ ê²°ì •
        stratify_col = None
        if task_type == "causal" and data_interface.treatment_col:
            stratify_col = data_interface.treatment_col
        elif task_type == "classification" and data_interface.target_col:
            stratify_col = data_interface.target_col
        
        logger.info(f"ë°ì´í„° ë¶„í•  (í…ŒìŠ¤íŠ¸ ì‚¬ì´ì¦ˆ: {test_size}, ê¸°ì¤€: {stratify_col})")
        
        # stratifyê°€ ê°€ëŠ¥í•œì§€ í™•ì¸
        if stratify_col and stratify_col in df.columns and df[stratify_col].nunique() > 1:
            train_df, test_df = train_test_split(
                df, test_size=test_size, random_state=42, stratify=df[stratify_col]
            )
            logger.info(f"'{stratify_col}' ì»¬ëŸ¼ ê¸°ì¤€ ê³„ì¸µí™” ë¶„í•  ìˆ˜í–‰")
        else:
            if stratify_col:
                logger.warning(f"'{stratify_col}' ì»¬ëŸ¼ìœ¼ë¡œ ê³„ì¸µí™” ë¶„í• ì„ í•  ìˆ˜ ì—†ì–´ ëœë¤ ë¶„í• í•©ë‹ˆë‹¤.")
            train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)

        logger.info(f"ë¶„í•  ì™„ë£Œ: í•™ìŠµì…‹ {len(train_df)} í–‰, í…ŒìŠ¤íŠ¸ì…‹ {len(test_df)} í–‰")
        return train_df, test_df