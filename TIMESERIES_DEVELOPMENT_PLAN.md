# ğŸ”® Timeseries Task + DataHandler ì•„í‚¤í…ì²˜ ê°œë°œ ê³„íš

**ëª©í‘œ**: í™•ì¥ ê°€ëŠ¥í•œ DataHandler ì•„í‚¤í…ì²˜ë¥¼ í†µí•´ timeseriesë¥¼ ì¶”ê°€í•˜ê³ , í–¥í›„ ë”¥ëŸ¬ë‹ê¹Œì§€ ì§€ì›í•˜ëŠ” ê°•ê±´í•œ ì‹œìŠ¤í…œ êµ¬ì¶•

## ğŸ“Š í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„ ë° ë¬¸ì œì 

### âŒ í˜„ì¬ êµ¬ì¡°ì˜ í•œê³„
```
src/components/trainer/modules/data_handler.py  # ëª¨ë“  task ì²˜ë¦¬ê°€ í•œ íŒŒì¼ì— ì§‘ì¤‘
â”œâ”€â”€ split_data()          # taskë³„ ë¶„ê¸° ì²˜ë¦¬ 
â”œâ”€â”€ prepare_training_data()  # taskë³„ ë¶„ê¸° ì²˜ë¦¬
â””â”€â”€ _get_exclude_columns()   # ê³µí†µ ë¡œì§
```

**ë¬¸ì œì **:
1. **ë‹¨ì¼ íŒŒì¼ ì§‘ì¤‘**: ëª¨ë“  task ë¡œì§ì´ í•œ íŒŒì¼ì— í˜¼ì¬
2. **í™•ì¥ì„± ë¶€ì¡±**: ìƒˆë¡œìš´ task ì¶”ê°€ ì‹œ ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • í•„ìš”
3. **ì±…ì„ ë¶„ì‚° ë¶€ì¡±**: tabular MLê³¼ timeseriesì˜ íŠ¹ì„±ì´ ë‹¤ë¦„ì—ë„ ë™ì¼ ì²˜ë¦¬
4. **ë”¥ëŸ¬ë‹ ë¯¸ì¤€ë¹„**: ë°°ì¹˜ ìƒì„±, ì‹œí€€ìŠ¤ ì²˜ë¦¬ ë“± ë”¥ëŸ¬ë‹ ìš”êµ¬ì‚¬í•­ ë¯¸ê³ ë ¤

### ğŸ¯ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ëª©í‘œ
1. **Taskë³„ ì „ìš© í•¸ë“¤ëŸ¬**: tabular vs timeseries vs deeplearning ë¶„ë¦¬
2. **í™•ì¥ì„±**: Registry íŒ¨í„´ìœ¼ë¡œ ìƒˆë¡œìš´ handler ì‰½ê²Œ ì¶”ê°€
3. **íŒŒì´í”„ë¼ì¸ ì¼ê´€ì„±**: Data Adapter â†’ Fetcher â†’ **DataHandler** â†’ Preprocessor â†’ Trainer
4. **ë¯¸ë˜ ì¤€ë¹„**: ë”¥ëŸ¬ë‹ ë°°ì¹˜ ìƒì„±, ì‹œí€€ìŠ¤ ì²˜ë¦¬ê¹Œì§€ ëŒ€ë¹„

---

## ğŸ—ï¸ ìƒˆë¡œìš´ DataHandler ì•„í‚¤í…ì²˜

### **ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”**
```
src/components/datahandler/
â”œâ”€â”€ registry.py                    # DataHandler Registry
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base.py                        # BaseDataHandler ì¸í„°í˜ì´ìŠ¤
â””â”€â”€ modules/
    â”œâ”€â”€ tabular_handler.py         # ê¸°ì¡´ 4ê°œ task (classification, regression, clustering, causal)
    â”œâ”€â”€ timeseries_handler.py      # timeseries ì „ìš© 
    â””â”€â”€ deeplearning_handler.py    # í–¥í›„ ë”¥ëŸ¬ë‹ìš© (batch ìƒì„±, sequence ì²˜ë¦¬)
```

### **Registry íŒ¨í„´**
```python
# src/components/datahandler/registry.py
class DataHandlerRegistry:
    handlers: Dict[str, Type[BaseDataHandler]] = {}
    
    @classmethod
    def get_handler_for_task(cls, task_type: str) -> BaseDataHandler:
        # task_typeì— ë”°ë¥¸ ìë™ ë§¤í•‘
        handler_mapping = {
            "classification": "tabular",
            "regression": "tabular", 
            "clustering": "tabular",
            "causal": "tabular",
            "timeseries": "timeseries",
            "deeplearning": "deeplearning"  # í–¥í›„
        }
        handler_type = handler_mapping.get(task_type, "tabular")
        return cls.create(handler_type)
```

### **íŒŒì´í”„ë¼ì¸ íë¦„**
```
Data Adapter (CSV/SQL/API) 
    â†“ 
Fetcher (Feature Store ì—°ë™)
    â†“
ğŸ†• DataHandler (taskë³„ ë°ì´í„° ì²˜ë¦¬)  â† ìƒˆë¡œ ì¶”ê°€ë˜ëŠ” ê³„ì¸µ
    â”œâ”€ TabularHandler: feature selection, validation, basic split
    â”œâ”€ TimeseriesHandler: time features, time-based split, lag/rolling
    â””â”€ DeepLearningHandler: batch generation, sequence padding, tokenization
    â†“
Preprocessor (scaling, encoding ë“± ë²”ìš© ì²˜ë¦¬)
    â†“
Trainer (model fitting)
```

---

## ğŸš€ Phaseë³„ ê°œë°œ ê³„íš

---

## **Phase 1: DataHandler ì•„í‚¤í…ì²˜ êµ¬ì¶•** (í•µì‹¬ ë¦¬íŒ©í† ë§)

### 1.1 DataHandler ì»´í¬ë„ŒíŠ¸ ì‹ ì„¤

#### A. ê¸°ë³¸ êµ¬ì¡° ìƒì„±
```bash
mkdir -p src/components/datahandler/modules/
touch src/components/datahandler/{__init__.py,registry.py,base.py}
touch src/components/datahandler/modules/{tabular_handler.py,timeseries_handler.py}
```

#### B. BaseDataHandler ì¸í„°í˜ì´ìŠ¤ (`src/components/datahandler/base.py`)
```python
from abc import ABC, abstractmethod
from typing import Tuple, Dict, Any
import pandas as pd
from src.settings import Settings

class BaseDataHandler(ABC):
    """ë°ì´í„° í•¸ë“¤ëŸ¬ ê¸°ë³¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, settings: Settings):
        self.settings = settings
        self.data_interface = settings.recipe.data.data_interface
        
    @abstractmethod
    def prepare_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, Dict[str, Any]]:
        """ë°ì´í„° ì¤€ë¹„ (X, y, additional_data ë°˜í™˜)"""
        pass
        
    @abstractmethod 
    def split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Train/Test ë¶„í• """
        pass
        
    def validate_data(self, df: pd.DataFrame) -> bool:
        """ë°ì´í„° ê²€ì¦ (ê° handlerë³„ êµ¬ì²´í™”)"""
        return True
        
    def get_data_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° ë©”íƒ€ ì •ë³´ ë°˜í™˜"""
        return {
            "shape": df.shape,
            "columns": list(df.columns),
            "dtypes": df.dtypes.to_dict(),
            "missing_ratio": (df.isnull().sum() / len(df)).to_dict()
        }
```

#### C. DataHandler Registry (`src/components/datahandler/registry.py`)
```python
from typing import Dict, Type, Optional
from .base import BaseDataHandler
from src.utils.system.logger import logger

class DataHandlerRegistry:
    """DataHandler ì¤‘ì•™ ë ˆì§€ìŠ¤íŠ¸ë¦¬"""
    handlers: Dict[str, Type[BaseDataHandler]] = {}
    
    @classmethod
    def register(cls, handler_type: str, handler_class: Type[BaseDataHandler]):
        if not issubclass(handler_class, BaseDataHandler):
            raise TypeError(f"{handler_class.__name__} must be a subclass of BaseDataHandler")
        cls.handlers[handler_type] = handler_class
        logger.debug(f"DataHandler registered: {handler_type} -> {handler_class.__name__}")
    
    @classmethod
    def create(cls, handler_type: str, *args, **kwargs) -> BaseDataHandler:
        handler_class = cls.handlers.get(handler_type)
        if not handler_class:
            available = list(cls.handlers.keys())
            raise ValueError(f"Unknown handler type: '{handler_type}'. Available: {available}")
        return handler_class(*args, **kwargs)
    
    @classmethod
    def get_handler_for_task(cls, task_type: str, settings) -> BaseDataHandler:
        """task_typeì— ë”°ë¥¸ ìë™ handler ë§¤í•‘"""
        handler_mapping = {
            "classification": "tabular",
            "regression": "tabular",
            "clustering": "tabular", 
            "causal": "tabular",
            "timeseries": "timeseries"
        }
        handler_type = handler_mapping.get(task_type, "tabular")
        return cls.create(handler_type, settings)
```

### 1.2 ê¸°ì¡´ data_handler.py â†’ tabular_handler.py ë¦¬íŒ©í† ë§

#### A. íŒŒì¼ ì´ë™ ë° í´ë˜ìŠ¤í™”
```python
# src/components/datahandler/modules/tabular_handler.py
import pandas as pd
import numpy as np
from typing import Dict, Any, Tuple
from sklearn.model_selection import train_test_split

from ..base import BaseDataHandler
from ..registry import DataHandlerRegistry
from src.utils.system.logger import logger

class TabularDataHandler(BaseDataHandler):
    """ì „í†µì ì¸ í…Œì´ë¸” í˜•íƒœ MLì„ ìœ„í•œ ë°ì´í„° í•¸ë“¤ëŸ¬ (classification, regression, clustering, causal)"""
    
    def split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Train/Test ë¶„í•  (ì¡°ê±´ë¶€ stratify)"""
        test_size = 0.2
        stratify_series = self._get_stratify_column_data(df)
        
        train_df, test_df = train_test_split(
            df, test_size=test_size, random_state=42, stratify=stratify_series
        )
        return train_df, test_df
    
    def prepare_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, Dict[str, Any]]:
        """í…Œì´ë¸” í˜•íƒœ ë°ì´í„° ì¤€ë¹„ (ê¸°ì¡´ prepare_training_data ë¡œì§)"""
        task_type = self.data_interface.task_type
        exclude_cols = self._get_exclude_columns(df)
        
        if task_type in ["classification", "regression"]:
            return self._prepare_supervised_data(df, exclude_cols)
        elif task_type == "clustering":
            return self._prepare_clustering_data(df, exclude_cols)  
        elif task_type == "causal":
            return self._prepare_causal_data(df, exclude_cols)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” task_type: {task_type}")
    
    # ê¸°ì¡´ ë¡œì§ë“¤ì„ private ë©”ì„œë“œë¡œ ì´ë™
    def _prepare_supervised_data(self, df, exclude_cols): ...
    def _prepare_clustering_data(self, df, exclude_cols): ... 
    def _prepare_causal_data(self, df, exclude_cols): ...
    def _get_stratify_column_data(self, df): ...
    def _get_exclude_columns(self, df): ...
    def _check_missing_values_warning(self, X): ...  # ê¸°ì¡´ í•¨ìˆ˜ë“¤ ëª¨ë‘ í¬í•¨

# Self-registration
DataHandlerRegistry.register("tabular", TabularDataHandler)
```

### 1.3 Recipe Schema í™•ì¥ (`src/settings/recipe.py`)

#### A. TaskType í™•ì¥
```python
# Line 127 ê·¼ì²˜ ìˆ˜ì •
task_type: Literal["classification", "regression", "clustering", "causal", "timeseries"] = Field(
    ..., 
    description="ML íƒœìŠ¤í¬ íƒ€ì…"
)
```

#### B. DataInterfaceì— timeseries í•„ë“œ ì¶”ê°€
```python
class DataInterface(BaseModel):
    """ë°ì´í„° ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
    task_type: Literal["classification", "regression", "clustering", "causal", "timeseries"] = Field(...)
    target_column: str = Field(..., description="íƒ€ê²Ÿ ì»¬ëŸ¼ ì´ë¦„")
    
    # ê¸°ì¡´ í•„ë“œë“¤...
    feature_columns: Optional[List[str]] = Field(None, ...)
    treatment_column: Optional[str] = Field(None, ...)
    entity_columns: List[str] = Field(..., ...)
    
    # âœ… Timeseries ì „ìš© í•„ë“œë“¤ ì¶”ê°€
    timestamp_column: Optional[str] = Field(None, description="ì‹œê³„ì—´ íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼ (timeseries taskì—ì„œ í•„ìˆ˜)")
    forecast_horizon: Optional[int] = Field(None, ge=1, le=365, description="ì˜ˆì¸¡ ê¸°ê°„ (timeseries taskì—ì„œ í•„ìˆ˜)")
    frequency: Optional[str] = Field(None, description="ì‹œê³„ì—´ ì£¼ê¸° ('D', 'H', 'M', 'W', 'Y')")
    
    @model_validator(mode='after')
    def validate_timeseries_fields(self):
        """timeseries taskì¼ ë•Œ í•„ìˆ˜ í•„ë“œ ê²€ì¦"""
        if self.task_type == "timeseries":
            if not self.timestamp_column:
                raise ValueError("timeseries taskì—ì„œëŠ” timestamp_columnì´ í•„ìˆ˜ì…ë‹ˆë‹¤")
            if not self.forecast_horizon:
                raise ValueError("timeseries taskì—ì„œëŠ” forecast_horizonì´ í•„ìˆ˜ì…ë‹ˆë‹¤")
        return self
```

### 1.4 TimeseriesDataHandler êµ¬í˜„ (`src/components/datahandler/modules/timeseries_handler.py`)

```python
import pandas as pd
import numpy as np
from typing import Dict, Any, Tuple
from datetime import datetime, timedelta

from ..base import BaseDataHandler  
from ..registry import DataHandlerRegistry
from src.utils.system.logger import logger

class TimeseriesDataHandler(BaseDataHandler):
    """ì‹œê³„ì—´ ë°ì´í„° ì „ìš© í•¸ë“¤ëŸ¬"""
    
    def validate_data(self, df: pd.DataFrame) -> bool:
        """ì‹œê³„ì—´ ë°ì´í„° ê²€ì¦"""
        timestamp_col = self.data_interface.timestamp_column
        target_col = self.data_interface.target_column
        
        # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ê²€ì¦
        if timestamp_col not in df.columns:
            raise ValueError(f"Timestamp ì»¬ëŸ¼ '{timestamp_col}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        if target_col not in df.columns:
            raise ValueError(f"Target ì»¬ëŸ¼ '{target_col}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
        # íƒ€ì„ìŠ¤íƒ¬í”„ ë°ì´í„° íƒ€ì… ê²€ì¦
        if not pd.api.types.is_datetime64_any_dtype(df[timestamp_col]):
            try:
                df[timestamp_col] = pd.to_datetime(df[timestamp_col])
                logger.info(f"Timestamp ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤: {timestamp_col}")
            except:
                raise ValueError(f"Timestamp ì»¬ëŸ¼ '{timestamp_col}'ì„ datetimeìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return True
    
    def split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ì‹œê°„ ê¸°ì¤€ ë¶„í•  (ì‹œê°„ ìˆœì„œ ìœ ì§€)"""
        timestamp_col = self.data_interface.timestamp_column
        test_size = 0.2
        
        # ì‹œê°„ ìˆœì„œë¡œ ì •ë ¬ (í•„ìˆ˜)
        df_sorted = df.sort_values(timestamp_col).reset_index(drop=True)
        
        # ì‹œê°„ ê¸°ì¤€ ë¶„í• ì  ê³„ì‚°
        split_idx = int(len(df_sorted) * (1 - test_size))
        
        train_df = df_sorted.iloc[:split_idx].copy()
        test_df = df_sorted.iloc[split_idx:].copy()
        
        logger.info(f"ì‹œê³„ì—´ ì‹œê°„ ê¸°ì¤€ ë¶„í• : Train({len(train_df)}) / Test({len(test_df)})")
        logger.info(f"Train ê¸°ê°„: {train_df[timestamp_col].min()} ~ {train_df[timestamp_col].max()}")
        logger.info(f"Test ê¸°ê°„: {test_df[timestamp_col].min()} ~ {test_df[timestamp_col].max()}")
        
        return train_df, test_df
    
    def prepare_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, Dict[str, Any]]:
        """ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„"""
        # ë°ì´í„° ê²€ì¦
        self.validate_data(df)
        
        timestamp_col = self.data_interface.timestamp_column
        target_col = self.data_interface.target_column
        
        # ì‹œê°„ ìˆœì„œ ì •ë ¬ (í•„ìˆ˜)
        df = df.sort_values(timestamp_col).reset_index(drop=True)
        
        # ì‹œê³„ì—´ íŠ¹ì„± ìƒì„±
        df_with_features = self._generate_time_features(df)
        
        # Feature/Target ë¶„ë¦¬
        exclude_cols = self._get_exclude_columns(df_with_features)
        
        if self.data_interface.feature_columns is None:
            # ìë™ ì„ íƒ: timestamp, target, entity ì œì™¸
            auto_exclude = [timestamp_col, target_col] + exclude_cols
            X = df_with_features.drop(columns=[c for c in auto_exclude if c in df_with_features.columns])
            logger.info(f"Timeseries feature columns ìë™ ì„ íƒ: {list(X.columns)}")
        else:
            # ëª…ì‹œì  ì„ íƒ
            X = df_with_features[self.data_interface.feature_columns]
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        X = X.select_dtypes(include=[np.number])
        
        # ê²°ì¸¡ì¹˜ ê²½ê³ 
        self._check_missing_values_warning(X)
        
        y = df[target_col]
        
        additional_data = {
            'timestamp': df[timestamp_col],
            'forecast_horizon': self.data_interface.forecast_horizon,
            'frequency': self.data_interface.frequency
        }
        
        return X, y, additional_data
    
    def _generate_time_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì‹œê³„ì—´ ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìë™ ìƒì„±"""
        timestamp_col = self.data_interface.timestamp_column
        df_copy = df.copy()
        
        # ê¸°ë³¸ ì‹œê°„ íŠ¹ì„±
        df_copy['year'] = df_copy[timestamp_col].dt.year
        df_copy['month'] = df_copy[timestamp_col].dt.month
        df_copy['day'] = df_copy[timestamp_col].dt.day
        df_copy['dayofweek'] = df_copy[timestamp_col].dt.dayofweek
        df_copy['quarter'] = df_copy[timestamp_col].dt.quarter
        df_copy['is_weekend'] = df_copy['dayofweek'].isin([5, 6]).astype(int)
        
        # Lag features (1, 2, 3, 7, 14ì¼ ì „)
        target_col = self.data_interface.target_column
        for lag in [1, 2, 3, 7, 14]:
            df_copy[f'{target_col}_lag_{lag}'] = df_copy[target_col].shift(lag)
            
        # Rolling features (3, 7, 14ì¼ í‰ê· )
        for window in [3, 7, 14]:
            df_copy[f'{target_col}_rolling_mean_{window}'] = df_copy[target_col].rolling(window=window).mean()
            df_copy[f'{target_col}_rolling_std_{window}'] = df_copy[target_col].rolling(window=window).std()
        
        logger.info(f"ì‹œê³„ì—´ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(df_copy.columns) - len(df)}ê°œ íŠ¹ì„± ì¶”ê°€")
        return df_copy
    
    def _get_exclude_columns(self, df: pd.DataFrame) -> list:
        """ì‹œê³„ì—´ì—ì„œ ì œì™¸í•  ì»¬ëŸ¼"""
        exclude_columns = []
        
        # Entity columns
        if self.data_interface.entity_columns:
            exclude_columns.extend(self.data_interface.entity_columns)
            
        return [col for col in exclude_columns if col in df.columns]
    
    def _check_missing_values_warning(self, X: pd.DataFrame, threshold: float = 0.05):
        """5% ì´ìƒ ê²°ì¸¡ì¹˜ ê²½ê³  (tabular_handlerì™€ ë™ì¼)"""
        if X.empty:
            return
            
        missing_info = []
        for col in X.columns:
            missing_count = X[col].isnull().sum()
            missing_ratio = missing_count / len(X)
            
            if missing_ratio >= threshold:
                missing_info.append({
                    'column': col,
                    'missing_count': missing_count,
                    'missing_ratio': missing_ratio,
                    'total_rows': len(X)
                })
        
        if missing_info:
            logger.warning("âš ï¸  ê²°ì¸¡ì¹˜ê°€ ë§ì€ ì»¬ëŸ¼ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:")
            for info in missing_info:
                logger.warning(
                    f"   - {info['column']}: {info['missing_count']:,}ê°œ ({info['missing_ratio']:.1%}) "
                    f"/ ì „ì²´ {info['total_rows']:,}ê°œ í–‰"
                )
            logger.warning("   ğŸ’¡ ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš” (Imputation, ì»¬ëŸ¼ ì œê±° ë“±)")
        else:
            logger.info(f"âœ… ëª¨ë“  íŠ¹ì„± ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ {threshold:.0%} ë¯¸ë§Œì…ë‹ˆë‹¤.")

# Self-registration
DataHandlerRegistry.register("timeseries", TimeseriesDataHandler)
```

### 1.5 Trainer í†µí•© (`src/components/trainer/trainer.py`)

#### A. DataHandler í†µí•©
```python
# imports ìˆ˜ì •
from src.components.datahandler import DataHandlerRegistry

class Trainer(BaseTrainer):
    def train(self, df, model, fetcher, preprocessor, evaluator, context_params):
        # ê¸°ì¡´ fetcher ë¡œì§...
        
        # âœ… DataHandler ì‚¬ìš© (ê¸°ì¡´ data_handler í•¨ìˆ˜ ëŒ€ì²´)
        data_handler = DataHandlerRegistry.get_handler_for_task(
            self.settings.recipe.data.data_interface.task_type,
            self.settings
        )
        
        # ë°ì´í„° ì¤€ë¹„
        X, y, additional_data = data_handler.prepare_data(df)
        
        # ë°ì´í„° ë¶„í• 
        train_df, test_df = data_handler.split_data(df)
        
        # ë‚˜ë¨¸ì§€ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼...
```

#### B. ê¸°ì¡´ import ì œê±°
```python
# ì œê±°: from .modules.data_handler import split_data, prepare_training_data
```

---

## **Phase 2: Timeseries Models Catalog êµ¬ì¶•**

### 2.1 Models Catalog êµ¬ì¶• (`src/models/catalog/Timeseries/`)

#### A. `LinearTrend.yaml`
```yaml
class_path: "sklearn.linear_model.LinearRegression"
description: "Linear trend model with automated time-based features"
library: "scikit-learn"
hyperparameters:
  fixed:
    fit_intercept: true
  tunable:
    normalize:
      type: "categorical"
      range: [true, false]
      default: false
  environment_defaults:
    local:
      normalize: false
    dev: 
      normalize: false
    prod:
      normalize: false
supported_tasks: ["timeseries"]
feature_requirements:
  numerical: true
  categorical: false
  text: false
  time_features_auto: true
preprocessing_notes: "TimeseriesDataHandlerì—ì„œ ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±ì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤"
```

#### B. `ARIMA.yaml` (sklearn wrapper ë²„ì „)
```yaml
class_path: "src.models.custom.timeseries_wrappers.ARIMAWrapper"  # ì»¤ìŠ¤í…€ wrapper
description: "ARIMA model wrapped for sklearn compatibility"
library: "statsmodels"
hyperparameters:
  tunable:
    order_p:
      type: "int"
      range: [0, 5] 
      default: 1
    order_d:
      type: "int"
      range: [0, 2]
      default: 1
    order_q:
      type: "int" 
      range: [0, 5]
      default: 1
  environment_defaults:
    local:
      order_p: 1
      order_d: 1
      order_q: 1
supported_tasks: ["timeseries"]
requires_custom_wrapper: true
feature_requirements:
  numerical: true
  univariate_preferred: true
```

#### C. `ExponentialSmoothing.yaml`
```yaml
class_path: "src.models.custom.timeseries_wrappers.ExponentialSmoothingWrapper"
description: "Exponential Smoothing with sklearn interface"
library: "statsmodels"
hyperparameters:
  tunable:
    trend:
      type: "categorical"
      range: [null, "add", "mul"]
      default: "add"
    seasonal:
      type: "categorical" 
      range: [null, "add", "mul"]
      default: null
    seasonal_periods:
      type: "int"
      range: [2, 52]
      default: 12
supported_tasks: ["timeseries"]
requires_custom_wrapper: true
```

### 2.2 Custom Wrappers êµ¬í˜„ (`src/models/custom/timeseries_wrappers.py`)

```python
"""Timeseries ëª¨ë¸ë“¤ì„ sklearn ì¸í„°í˜ì´ìŠ¤ë¡œ ê°ì‹¸ëŠ” wrapperë“¤"""
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, RegressorMixin

class ARIMAWrapper(BaseEstimator, RegressorMixin):
    """ARIMA ëª¨ë¸ì„ sklearn ì¸í„°í˜ì´ìŠ¤ë¡œ ê°ì‹¸ëŠ” wrapper"""
    
    def __init__(self, order_p=1, order_d=1, order_q=1):
        self.order_p = order_p
        self.order_d = order_d  
        self.order_q = order_q
        self.model_ = None
        self.fitted_model_ = None
    
    def fit(self, X, y):
        from statsmodels.tsa.arima.model import ARIMA
        
        # ARIMAëŠ” univariateì´ë¯€ë¡œ yë§Œ ì‚¬ìš©
        self.model_ = ARIMA(y, order=(self.order_p, self.order_d, self.order_q))
        self.fitted_model_ = self.model_.fit()
        return self
    
    def predict(self, X):
        if self.fitted_model_ is None:
            raise ValueError("Model must be fitted before prediction")
        
        # Xì˜ ê¸¸ì´ë§Œí¼ ì˜ˆì¸¡
        forecast_steps = len(X)
        forecast = self.fitted_model_.forecast(steps=forecast_steps)
        
        return np.array(forecast)

class ExponentialSmoothingWrapper(BaseEstimator, RegressorMixin):
    """Exponential Smoothing sklearn wrapper"""
    
    def __init__(self, trend="add", seasonal=None, seasonal_periods=12):
        self.trend = trend
        self.seasonal = seasonal
        self.seasonal_periods = seasonal_periods
        self.fitted_model_ = None
    
    def fit(self, X, y):
        from statsmodels.tsa.holtwinters import ExponentialSmoothing
        
        self.fitted_model_ = ExponentialSmoothing(
            y,
            trend=self.trend,
            seasonal=self.seasonal,
            seasonal_periods=self.seasonal_periods if self.seasonal else None
        ).fit()
        return self
    
    def predict(self, X):
        if self.fitted_model_ is None:
            raise ValueError("Model must be fitted before prediction")
        
        forecast_steps = len(X)
        forecast = self.fitted_model_.forecast(steps=forecast_steps)
        return np.array(forecast)
```

---

## **Phase 3: Factory í†µí•© ë° ê³ ê¸‰ ê¸°ëŠ¥**

### 3.1 Factory DataHandler í†µí•© (`src/factory/factory.py`)

```python
def create_data_handler(self, task_type: Optional[str] = None) -> Any:
    """DataHandler ìƒì„± (ì¼ê´€ëœ ì ‘ê·¼ íŒ¨í„´)"""
    from src.components.datahandler import DataHandlerRegistry
    
    task_type = task_type or self._recipe.data.data_interface.task_type
    
    try:
        handler = DataHandlerRegistry.get_handler_for_task(task_type, self._settings)
        logger.info(f"âœ… Created data handler: {task_type}")
        return handler
    except Exception as e:
        logger.error(f"Failed to create data handler for '{task_type}': {e}")
        raise
```

### 3.2 Timeseries Evaluator (`src/components/evaluator/timeseries.py`)

```python
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from src.interface import BaseEvaluator
from .registry import EvaluatorRegistry

def mean_absolute_percentage_error(y_true, y_pred):
    """MAPE ê³„ì‚°"""
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

def symmetric_mean_absolute_percentage_error(y_true, y_pred):
    """SMAPE ê³„ì‚°"""
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return 2.0 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true))) * 100

class TimeSeriesEvaluator(BaseEvaluator):
    """ì‹œê³„ì—´ ì „ìš© í‰ê°€ê¸°"""
    
    def evaluate(self, y_true, y_pred, data_interface=None) -> dict:
        """ì‹œê³„ì—´ í‰ê°€ ì§€í‘œ ê³„ì‚°"""
        
        # ê¸°ë³¸ regression ì§€í‘œ
        mae = mean_absolute_error(y_true, y_pred)
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_true, y_pred)
        
        # ì‹œê³„ì—´ íŠ¹í™” ì§€í‘œ
        mape = mean_absolute_percentage_error(y_true, y_pred)
        smape = symmetric_mean_absolute_percentage_error(y_true, y_pred)
        
        return {
            # Regression ì§€í‘œ
            'mae': mae,
            'mse': mse, 
            'rmse': rmse,
            'r2': r2,
            
            # Timeseries íŠ¹í™” ì§€í‘œ
            'mape': mape,
            'smape': smape,
            
            # ë©”íƒ€ ì •ë³´
            'n_predictions': len(y_true),
            'evaluation_type': 'timeseries'
        }

# Self-registration
EvaluatorRegistry.register_evaluator("timeseries", TimeSeriesEvaluator)
```

---

## **Phase 4: Recipe Builder ë° UI**

### 4.1 Recipe Builder í™•ì¥ (`src/cli/utils/recipe_builder.py`)

```python
# TASK_METRICS í™•ì¥ (Line 28 ê·¼ì²˜)
TASK_METRICS = {
    "Classification": ["accuracy", "precision", "recall", "f1", "roc_auc"],
    "Regression": ["mae", "mse", "rmse", "r2", "mape"],
    "Clustering": ["silhouette_score", "davies_bouldin", "calinski_harabasz"],
    "Causal": ["ate", "att", "confidence_intervals"],
    "Timeseries": ["mae", "mse", "rmse", "r2", "mape", "smape"]  # âœ… ì¶”ê°€
}

# _configure_data_interface ë©”ì„œë“œì— timeseries ì¼€ì´ìŠ¤ ì¶”ê°€
def _configure_data_interface(self, selections: Dict[str, Any]):
    """ë°ì´í„° ì¸í„°í˜ì´ìŠ¤ ì„¤ì •"""
    
    # ê¸°ì¡´ ë¡œì§...
    
    # âœ… Timeseries ì „ìš© ì„¤ì • ì¶”ê°€
    if selections["task"] == "Timeseries":
        self.ui.show_info("ğŸ• ì‹œê³„ì—´ ë°ì´í„° ì„¤ì •")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼ ì„ íƒ
        timestamp_col = self.ui.select_from_list(
            "íƒ€ì„ìŠ¤íƒ¬í”„ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”",
            available_columns
        )
        selections["timestamp_column"] = timestamp_col
        
        # Forecast horizon ì„¤ì •
        horizon = self.ui.number_input(
            "ì˜ˆì¸¡ ê¸°ê°„ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 30ì¼)",
            default=30,
            min_value=1,
            max_value=365
        )
        selections["forecast_horizon"] = horizon
        
        # Frequency ì„¤ì •
        freq = self.ui.select_from_list(
            "ì‹œê³„ì—´ ì£¼ê¸°ë¥¼ ì„ íƒí•˜ì„¸ìš”",
            ["auto", "D (ì¼ë³„)", "H (ì‹œê°„ë³„)", "M (ì›”ë³„)", "W (ì£¼ë³„)", "Y (ì—°ë³„)"]
        )
        freq_code = freq.split()[0] if freq != "auto" else None
        selections["frequency"] = freq_code
        
        self.ui.show_success("âœ… ì‹œê³„ì—´ ì„¤ì • ì™„ë£Œ")
        self.ui.print_divider()
```

### 4.2 Timeseries Template (`src/cli/templates/recipes/timeseries_template.yaml.j2`)

```yaml
name: "{{ recipe_name }}"
description: "Time Series Forecasting Recipe"

data:
  loader:
    source_uri: "{{ source_uri }}"
  fetcher:
    mode: "offline"
    feature_store:
      enabled: false
  data_interface:
    task_type: "timeseries"
    target_column: "{{ target_column }}"
    timestamp_column: "{{ timestamp_column }}"
    forecast_horizon: {{ forecast_horizon }}
    {% if frequency %}frequency: "{{ frequency }}"{% endif %}
    entity_columns: {{ entity_columns }}
    {% if feature_columns %}feature_columns: {{ feature_columns }}{% endif %}

preprocessing:
  enabled: {{ preprocessing_enabled }}
  steps: {{ preprocessing_steps }}

model:
  algorithm: "{{ algorithm }}"
  hyperparameters:
    tuning_enabled: {{ tuning_enabled }}
    {% if tuning_enabled %}
    optimization_metric: "{{ optimization_metric }}"
    n_trials: {{ n_trials }}
    timeout: {{ timeout }}
    fixed: {{ fixed_params }}
    tunable: {{ tunable_params }}
    {% else %}
    values: {{ hyperparameter_values }}
    {% endif %}

validation:
  method: "train_test_split"  # ì‹œê³„ì—´ì€ í•­ìƒ time-based split
  test_size: {{ test_size }}
  random_state: {{ random_state }}

evaluation:
  metrics: {{ evaluation_metrics }}
```

---

## **Phase 5: API/ë°°í¬ ì§€ì›**

### 5.1 PyfuncWrapper í™•ì¥ (`src/factory/artifact.py`)

```python
def predict(self, context, model_input):
    """ì˜ˆì¸¡ ìˆ˜í–‰ (timeseries ì§€ì› ì¶”ê°€)"""
    
    if self._task_type == "timeseries":
        return self._predict_timeseries(context, model_input)
    else:
        # ê¸°ì¡´ ë¡œì§
        return self._predict_tabular(context, model_input)

def _predict_timeseries(self, context, model_input):
    """ì‹œê³„ì—´ ì˜ˆì¸¡ ìˆ˜í–‰"""
    df_input = self._convert_input_to_dataframe(model_input)
    
    # DataHandlerë¡œ ì‹œê³„ì—´ íŠ¹ì„± ìƒì„±
    from src.components.datahandler import DataHandlerRegistry
    data_handler = DataHandlerRegistry.get_handler_for_task("timeseries", self._settings)
    
    # íŠ¹ì„± ìƒì„± (timestamp ê¸°ë°˜)
    df_with_features = data_handler._generate_time_features(df_input)
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    predictions = self._model.predict(processed_features)
    
    # ë¯¸ë˜ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    forecast_horizon = self._additional_data.get('forecast_horizon', len(predictions))
    last_timestamp = df_input[self._timestamp_column].max()
    future_timestamps = self._generate_future_timestamps(
        last_timestamp, 
        forecast_horizon,
        self._frequency
    )
    
    return {
        'predictions': predictions.tolist(),
        'timestamps': [ts.isoformat() for ts in future_timestamps],
        'forecast_horizon': forecast_horizon,
        'model_type': 'timeseries',
        'frequency': self._frequency
    }
```

---

## **Phase 6: ë”¥ëŸ¬ë‹ í™•ì¥ ì¤€ë¹„** (ë¯¸ë˜)

### 6.1 DeepLearning DataHandler ì„¤ê³„

```python
# src/components/datahandler/modules/deeplearning_handler.py
class DeepLearningDataHandler(BaseDataHandler):
    """ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ë°ì´í„° í•¸ë“¤ëŸ¬"""
    
    def prepare_data(self, df):
        """ë°°ì¹˜ ìƒì„±, ì‹œí€€ìŠ¤ íŒ¨ë”©, í† í¬ë‚˜ì´ì œì´ì…˜ ë“±"""
        pass
        
    def create_dataloaders(self, train_df, val_df, batch_size=32):
        """PyTorch/TensorFlow DataLoader ìƒì„±"""
        pass
        
    def prepare_sequences(self, df, sequence_length=50):
        """ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„ (RNN/LSTMìš©)"""
        pass
```

---

## ğŸ¯ êµ¬í˜„ ìš°ì„ ìˆœìœ„ ë° ë§ˆì¼ìŠ¤í†¤

### **ğŸš¨ Phase 1 (í•„ìˆ˜): ì•„í‚¤í…ì²˜ ë¦¬íŒ©í† ë§** 
**ëª©í‘œ**: DataHandler ì•„í‚¤í…ì²˜ êµ¬ì¶• ë° ê¸°ì¡´ ì½”ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜
1. BaseDataHandler + Registry êµ¬í˜„
2. TabularDataHandler êµ¬í˜„ (ê¸°ì¡´ data_handler.py ì´ê´€)
3. Recipe Schemaì— timeseries í•„ë“œ ì¶”ê°€
4. TimeseriesDataHandler ê¸°ë³¸ êµ¬í˜„
5. Trainerì—ì„œ DataHandler í†µí•©

**ì™„ë£Œ ê¸°ì¤€**: âœ… ê¸°ì¡´ 4ê°œ taskê°€ ìƒˆ ì•„í‚¤í…ì²˜ì—ì„œ ì •ìƒ ì‘ë™

### **âš¡ Phase 2 (í•µì‹¬): Timeseries ì™„ì„±**
**ëª©í‘œ**: Timeseries ì™„ì „ ì§€ì›
1. LinearTrend, ARIMA, ExponentialSmoothing ëª¨ë¸ êµ¬í˜„
2. Custom Wrapper êµ¬í˜„
3. TimeseriesEvaluator êµ¬í˜„

**ì™„ë£Œ ê¸°ì¤€**: âœ… CLIë¡œ timeseries í•™ìŠµ ë° í‰ê°€ ê°€ëŠ¥

### **ğŸ¨ Phase 3 (í–¥ìƒ): ì‚¬ìš©ì„±**
**ëª©í‘œ**: ì‚¬ìš©ì ì¹œí™”ì  í™˜ê²½
1. Recipe Builder timeseries ì„¤ì • UI
2. Timeseries template êµ¬í˜„

**ì™„ë£Œ ê¸°ì¤€**: âœ… CLIë¡œ timeseries í”„ë¡œì íŠ¸ ì‰½ê²Œ ìƒì„±

### **ğŸš€ Phase 4 (ì™„ì„±): ë°°í¬**
**ëª©í‘œ**: ìš´ì˜ í™˜ê²½ ì¤€ë¹„
1. PyfuncWrapper timeseries ì§€ì›
2. API endpoints í™•ì¥

**ì™„ë£Œ ê¸°ì¤€**: âœ… Timeseries API ì„œë¹„ìŠ¤ ê°€ëŠ¥

### **ğŸ”® Phase 5 (ë¯¸ë˜): ë”¥ëŸ¬ë‹ í™•ì¥**
**ëª©í‘œ**: ë”¥ëŸ¬ë‹ ìƒíƒœê³„ ì¤€ë¹„
1. DeepLearningDataHandler ì„¤ê³„
2. ë°°ì¹˜ ìƒì„±, ì‹œí€€ìŠ¤ ì²˜ë¦¬ ë¡œì§

---

## ğŸ“‹ í•µì‹¬ ì„¤ê³„ ì² í•™

### âœ… **ê´€ì‹¬ì‚¬ ë¶„ë¦¬ (Separation of Concerns)**
- **DataHandler**: ë°ì´í„° í˜•íƒœë³„ íŠ¹í™” ì²˜ë¦¬ (tabular vs timeseries vs deeplearning)
- **Preprocessor**: ë²”ìš© ë³€í™˜ (scaling, encoding ë“±)
- **Trainer**: ëª¨ë¸ í•™ìŠµ ë¡œì§

### âœ… **í™•ì¥ì„± (Extensibility)**
- Registry íŒ¨í„´ìœ¼ë¡œ ìƒˆë¡œìš´ Handler ì‰½ê²Œ ì¶”ê°€
- ê° HandlerëŠ” ë…ë¦½ì ìœ¼ë¡œ ë°œì „ ê°€ëŠ¥

### âœ… **í•˜ìœ„ í˜¸í™˜ì„± (Backward Compatibility)**
- ê¸°ì¡´ 4ê°œ taskëŠ” TabularDataHandlerë¡œ ì™„ì „ í˜¸í™˜
- ê¸°ì¡´ Recipe/ì„¤ì • êµ¬ì¡° ìœ ì§€

### âœ… **ë¯¸ë˜ ëŒ€ì‘ì„± (Future-Proofing)**
- ë”¥ëŸ¬ë‹, ë©€í‹°ëª¨ë‹¬ ë“± ë¯¸ë˜ í™•ì¥ ëŒ€ë¹„
- ë°°ì¹˜ ì²˜ë¦¬, GPU ê°€ì† ë“± ê³ ê¸‰ ê¸°ëŠ¥ ìˆ˜ìš© ê°€ëŠ¥

---

## ğŸ‰ **ê²°ê³¼ì ìœ¼ë¡œ ë‹¬ì„±í•˜ëŠ” ê²ƒ**

1. **ğŸ”„ ê¹”ë”í•œ ì•„í‚¤í…ì²˜**: ê° ì»´í¬ë„ŒíŠ¸ê°€ ëª…í™•í•œ ì±…ì„ì„ ê°€ì§
2. **ğŸš€ Timeseries ì™„ì „ ì§€ì›**: í•™ìŠµë¶€í„° API ì„œë¹™ê¹Œì§€
3. **ğŸ’ í™•ì¥ì„±**: ìƒˆë¡œìš´ ë°ì´í„° íƒ€ì…/ëª¨ë¸ ì‰½ê²Œ ì¶”ê°€
4. **ğŸ›¡ï¸ ê²¬ê³ í•¨**: ê° Handlerê°€ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸/ë°œì „ ê°€ëŠ¥
5. **ğŸŒŸ ë¯¸ë˜ ì¤€ë¹„**: ë”¥ëŸ¬ë‹, ë©€í‹°ëª¨ë‹¬ê¹Œì§€ ìˆ˜ìš© ê°€ëŠ¥í•œ êµ¬ì¡°

ì´ ê³„íšì€ **í˜„ì¬ë¥¼ ì¡´ì¤‘í•˜ë©´ì„œ ë¯¸ë˜ë¥¼ ì¤€ë¹„**í•˜ëŠ” ì ì§„ì  ì§„í™” ì „ëµì…ë‹ˆë‹¤! ğŸš€