"""
Trainer ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ (Blueprint v17.0 í˜„ëŒ€í™”)

í•™ìŠµ í”„ë¡œì„¸ìŠ¤, ì»´í¬ë„ŒíŠ¸ ì¡°í•©, ë©”íŠ¸ë¦­ ìˆ˜ì§‘, HPO ë¡œì§ í…ŒìŠ¤íŠ¸

Blueprint ì›ì¹™ ê²€ì¦:
- ì›ì¹™ 8: ìë™í™”ëœ HPO + Data Leakage ì™„ì „ ë°©ì§€
"""

import pytest
import pandas as pd
from unittest.mock import Mock, patch, call

pytest.skip("Deprecated/outdated test module pending Stage 6 test overhaul (trainer interface updated).", allow_module_level=True)

from src.components.trainer import Trainer
from src.engine.factory import Factory
from src.settings import Settings
from src.components.augmenter import Augmenter, PassThroughAugmenter
from src.components.preprocessor import Preprocessor

class TestTrainerModernized:
    """Trainer ì»´í¬ë„ŒíŠ¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Blueprint v17.0, ì™„ì „ í˜„ëŒ€í™”)"""

    def test_trainer_initialization(self, local_test_settings: Settings):
        """Trainerê°€ settings ê°ì²´ë¡œ ì˜¬ë°”ë¥´ê²Œ ì´ˆê¸°í™”ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
        trainer = Trainer(settings=local_test_settings)
        assert trainer.settings == local_test_settings

    @patch('src.core.trainer.mlflow')
    def test_train_method_flow_in_local_env(self, mock_mlflow, local_test_settings: Settings):
        """
        LOCAL í™˜ê²½ì—ì„œ Trainer.train() ë©”ì†Œë“œì˜ ì‹¤í–‰ íë¦„ì„ ê²€ì¦í•œë‹¤.
        (ì‹¤ì œ í•™ìŠµì€ Mocking)
        """
        trainer = Trainer(settings=local_test_settings)
        
        # Mock ì»´í¬ë„ŒíŠ¸ ë° ë°ì´í„°
        mock_model = Mock()
        mock_model.fit.return_value = mock_model
        
        mock_preprocessor = Mock()
        mock_preprocessor.fit_transform.return_value = pd.DataFrame({'f1': [0.1, 0.2]})
        mock_preprocessor.transform.return_value = pd.DataFrame({'f1': [0.1, 0.2]})

        mock_augmenter = PassThroughAugmenter(settings=local_test_settings)
        
        df = pd.DataFrame({
            'user_id': ['u1', 'u2', 'u3', 'u4'],
            'approved': [1, 0, 1, 0] # target_col for stratification
        })

        # train ë©”ì†Œë“œ ì‹¤í–‰
        trained_preprocessor, trained_model, training_results = trainer.train(
            df=df,
            model=mock_model,
            augmenter=mock_augmenter,
            preprocessor=mock_preprocessor,
        )

        # ê²€ì¦
        assert trained_preprocessor is not None
        assert trained_model is not None
        assert "metrics" in training_results
        
        # Preprocessorê°€ train ë°ì´í„°ì— fit ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert mock_preprocessor.fit_transform.call_count == 1
        # Modelì´ train ë°ì´í„°ì— fit ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert mock_model.fit.call_count == 1
        
        # MLflow ë¡œê¹…ì´ í˜¸ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
        mock_mlflow.log_metrics.assert_called_once()

    # ğŸ†• Blueprint v17.0: ìƒì„¸ ì‹¤í–‰ íë¦„ ê²€ì¦
    @patch('src.core.trainer.mlflow')
    @patch('src.core.trainer.train_test_split')
    def test_trainer_execution_flow_detailed_verification(self, mock_split, mock_mlflow, local_test_settings: Settings):
        """
        Trainer.train() ë©”ì†Œë“œì˜ ìƒì„¸ ì‹¤í–‰ íë¦„ì„ ìˆœì„œëŒ€ë¡œ ê²€ì¦í•œë‹¤.
        data_split â†’ augment â†’ preprocess â†’ model.fit â†’ evaluate â†’ mlflow.log_metrics
        """
        trainer = Trainer(settings=local_test_settings)
        
        # Mock ì„¤ì •
        mock_model = Mock()
        mock_preprocessor = Mock()
        mock_augmenter = Mock()
        
        # mock_split ë°˜í™˜ê°’ ì„¤ì •
        train_df = pd.DataFrame({'user_id': ['u1', 'u2'], 'approved': [1, 0]})
        test_df = pd.DataFrame({'user_id': ['u3', 'u4'], 'approved': [1, 0]})
        mock_split.return_value = (train_df, test_df)
        
        # mock_augmenter ë°˜í™˜ê°’ ì„¤ì •
        mock_augmenter.augment.return_value = pd.DataFrame({'f1': [0.1, 0.2], 'approved': [1, 0]})
        
        # mock_preprocessor ë°˜í™˜ê°’ ì„¤ì •
        mock_preprocessor.fit.return_value = mock_preprocessor
        mock_preprocessor.transform.return_value = pd.DataFrame({'f1': [0.1, 0.2]})
        
        df = pd.DataFrame({
            'user_id': ['u1', 'u2', 'u3', 'u4'],
            'approved': [1, 0, 1, 0]
        })

        # train ë©”ì†Œë“œ ì‹¤í–‰
        trainer.train(
            df=df,
            model=mock_model,
            augmenter=mock_augmenter,
            preprocessor=mock_preprocessor,
        )

        # 1. Data Splitì´ í˜¸ì¶œë˜ì—ˆëŠ”ì§€
        mock_split.assert_called_once()
        
        # 2. Augmenterê°€ train/test ë°ì´í„°ì— ê°ê° í˜¸ì¶œë˜ì—ˆëŠ”ì§€
        assert mock_augmenter.augment.call_count == 2
        
        # 3. Preprocessorê°€ train ë°ì´í„°ì— fit í˜¸ì¶œë˜ì—ˆëŠ”ì§€ (Data Leakage ë°©ì§€)
        mock_preprocessor.fit.assert_called_once()
        
        # 4. Preprocessorê°€ train/test ë°ì´í„°ì— transform í˜¸ì¶œë˜ì—ˆëŠ”ì§€
        assert mock_preprocessor.transform.call_count == 2
        
        # 5. Modelì´ fit í˜¸ì¶œë˜ì—ˆëŠ”ì§€
        mock_model.fit.assert_called_once()
        
        # 6. MLflow ë¡œê¹…ì´ í˜¸ì¶œë˜ì—ˆëŠ”ì§€
        mock_mlflow.log_metrics.assert_called_once()

    # ğŸ†• Blueprint v17.0: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸
    def test_trainer_with_hyperparameter_tuning_disabled(self, local_test_settings: Settings):
        """
        hyperparameter_tuning.enabled = Falseì¼ ë•Œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦í•œë‹¤.
        """
        # ì„¤ì •ì—ì„œ HPOê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        assert local_test_settings.model.hyperparameter_tuning is None or \
               not local_test_settings.model.hyperparameter_tuning.enabled
        
        trainer = Trainer(settings=local_test_settings)
        
        # Mock ì»´í¬ë„ŒíŠ¸
        mock_model = Mock()
        mock_preprocessor = Mock()
        mock_augmenter = PassThroughAugmenter(settings=local_test_settings)
        
        mock_preprocessor.fit.return_value = mock_preprocessor
        mock_preprocessor.transform.return_value = pd.DataFrame({'f1': [0.1, 0.2]})
        
        df = pd.DataFrame({
            'user_id': ['u1', 'u2', 'u3', 'u4'],
            'approved': [1, 0, 1, 0]
        })

        with patch('src.core.trainer.mlflow'):
            trained_preprocessor, trained_model, training_results = trainer.train(
                df=df,
                model=mock_model,
                augmenter=mock_augmenter,
                preprocessor=mock_preprocessor,
            )

        # HPOê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ hyperparameter_optimization ë©”íƒ€ë°ì´í„°ê°€ ì—†ê±°ë‚˜ enabled=False
        hpo_data = training_results.get('hyperparameter_optimization', {})
        assert not hpo_data.get('enabled', False)

    # ğŸ†• Blueprint v17.0: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í™œì„±í™” í…ŒìŠ¤íŠ¸
    @patch('src.core.trainer.mlflow')
    def test_trainer_with_hyperparameter_tuning_enabled(self, mock_mlflow, dev_test_settings: Settings):
        """
        hyperparameter_tuning.enabled = Trueì¼ ë•Œ Optuna ê´€ë ¨ ë¡œì§ì´ í˜¸ì¶œë˜ëŠ”ì§€ ê²€ì¦í•œë‹¤.
        """
        # HPO í™œì„±í™”ëœ ì„¤ì • ìƒì„±
        settings_with_hpo = dev_test_settings.model_copy(deep=True)
        if not hasattr(settings_with_hpo.model, 'hyperparameter_tuning') or \
           settings_with_hpo.model.hyperparameter_tuning is None:
            # HPO ì„¤ì •ì´ ì—†ë‹¤ë©´ ì¶”ê°€
            from src.settings.models import HyperparameterTuningSettings
            settings_with_hpo.model.hyperparameter_tuning = HyperparameterTuningSettings(
                enabled=True,
                n_trials=5,
                metric="accuracy",
                direction="maximize"
            )
        else:
            settings_with_hpo.model.hyperparameter_tuning.enabled = True
            settings_with_hpo.model.hyperparameter_tuning.n_trials = 5

        trainer = Trainer(settings=settings_with_hpo)

        # Optuna ê´€ë ¨ Mock
        with patch('src.core.trainer.optuna') as mock_optuna:
            mock_study = Mock()
            mock_trial = Mock()
            mock_trial.number = 1
            mock_optuna.create_study.return_value = mock_study
            mock_study.best_trial = mock_trial
            mock_study.best_trial.value = 0.95
            mock_study.best_trial.params = {"n_estimators": 100}
            mock_study.trials = [mock_trial]
            
            # Mock ì»´í¬ë„ŒíŠ¸
            mock_model = Mock()
            mock_preprocessor = Mock()
            mock_augmenter = Mock()
            
            mock_augmenter.augment.return_value = pd.DataFrame({'f1': [0.1, 0.2], 'approved': [1, 0]})
            mock_preprocessor.fit.return_value = mock_preprocessor
            mock_preprocessor.transform.return_value = pd.DataFrame({'f1': [0.1, 0.2]})
            
            df = pd.DataFrame({
                'user_id': ['u1', 'u2', 'u3', 'u4'],
                'approved': [1, 0, 1, 0]
            })

            trained_preprocessor, trained_model, training_results = trainer.train(
                df=df,
                model=mock_model,
                augmenter=mock_augmenter,
                preprocessor=mock_preprocessor,
            )

            # Optuna Studyê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ ê²€ì¦
            mock_optuna.create_study.assert_called_once()
            
            # HPO ê²°ê³¼ê°€ training_resultsì— í¬í•¨ë˜ì—ˆëŠ”ì§€ ê²€ì¦
            assert 'hyperparameter_optimization' in training_results
            hpo_data = training_results['hyperparameter_optimization']
            assert hpo_data['enabled'] == True
            assert 'best_params' in hpo_data
            assert 'best_score' in hpo_data

    # ğŸ†• Blueprint v17.0: HPOì™€ Data Leakage ë°©ì§€ ì¡°í•© ê²€ì¦
    @patch('src.core.trainer.mlflow')
    @patch('src.core.trainer.train_test_split')
    def test_trainer_hpo_with_data_leakage_prevention(self, mock_split, mock_mlflow, dev_test_settings: Settings):
        """
        HPO ê³¼ì •ì—ì„œë„ Data Leakage ë°©ì§€ê°€ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦í•œë‹¤.
        ê° trialë§ˆë‹¤ ë…ë¦½ì ì¸ train/validation splitì´ ìˆ˜í–‰ë˜ì–´ì•¼ í•¨.
        """
        # HPO í™œì„±í™”ëœ ì„¤ì •
        settings_with_hpo = dev_test_settings.model_copy(deep=True)
        if not hasattr(settings_with_hpo.model, 'hyperparameter_tuning') or \
           settings_with_hpo.model.hyperparameter_tuning is None:
            from src.settings.models import HyperparameterTuningSettings
            settings_with_hpo.model.hyperparameter_tuning = HyperparameterTuningSettings(
                enabled=True,
                n_trials=3,  # ì ì€ ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸
                metric="accuracy",
                direction="maximize"
            )
        else:
            settings_with_hpo.model.hyperparameter_tuning.enabled = True
            settings_with_hpo.model.hyperparameter_tuning.n_trials = 3

        trainer = Trainer(settings=settings_with_hpo)

        # train_test_split Mock ì„¤ì •
        train_df = pd.DataFrame({'user_id': ['u1', 'u2'], 'approved': [1, 0]})
        val_df = pd.DataFrame({'user_id': ['u3', 'u4'], 'approved': [1, 0]})
        mock_split.return_value = (train_df, val_df)

        with patch('src.core.trainer.optuna') as mock_optuna:
            # Optuna Mock ì„¤ì •
            mock_study = Mock()
            mock_trial = Mock()
            mock_trial.number = 1
            mock_trial.suggest_int.return_value = 100
            mock_trial.suggest_float.return_value = 0.1
            mock_optuna.create_study.return_value = mock_study
            mock_study.optimize.side_effect = lambda objective, n_trials: [objective(mock_trial) for _ in range(n_trials)]
            mock_study.best_trial = mock_trial
            mock_study.best_trial.value = 0.95
            mock_study.best_trial.params = {"n_estimators": 100}

            # Mock ì»´í¬ë„ŒíŠ¸
            mock_model = Mock()
            mock_preprocessor = Mock()
            mock_augmenter = Mock()
            
            mock_augmenter.augment.return_value = pd.DataFrame({'f1': [0.1, 0.2], 'approved': [1, 0]})
            mock_preprocessor.fit.return_value = mock_preprocessor
            mock_preprocessor.transform.return_value = pd.DataFrame({'f1': [0.1, 0.2]})
            
            df = pd.DataFrame({
                'user_id': ['u1', 'u2', 'u3', 'u4'],
                'approved': [1, 0, 1, 0]
            })

            trainer.train(
                df=df,
                model=mock_model,
                augmenter=mock_augmenter,
                preprocessor=mock_preprocessor,
            )

            # train_test_splitì´ í˜¸ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸ (HPOë“  ì¼ë°˜ì´ë“  data splitì€ í•„ìˆ˜)
            assert mock_split.call_count >= 1
            
            # Preprocessorê°€ fit í˜¸ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸ (Data Leakage ë°©ì§€)
            assert mock_preprocessor.fit.call_count >= 1

    # ğŸ†• Blueprint v17.0: training_results ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ ê²€ì¦
    @patch('src.core.trainer.mlflow')
    def test_trainer_training_results_metadata_completeness(self, mock_mlflow, local_test_settings: Settings):
        """
        training_resultsì— ëª¨ë“  í•„ìš”í•œ ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ë˜ëŠ”ì§€ ê²€ì¦í•œë‹¤.
        """
        trainer = Trainer(settings=local_test_settings)
        
        # Mock ì»´í¬ë„ŒíŠ¸
        mock_model = Mock()
        mock_preprocessor = Mock()
        mock_augmenter = PassThroughAugmenter(settings=local_test_settings)
        
        mock_preprocessor.fit.return_value = mock_preprocessor
        mock_preprocessor.transform.return_value = pd.DataFrame({'f1': [0.1, 0.2]})
        
        df = pd.DataFrame({
            'user_id': ['u1', 'u2', 'u3', 'u4'],
            'approved': [1, 0, 1, 0]
        })

        trained_preprocessor, trained_model, training_results = trainer.train(
            df=df,
            model=mock_model,
            augmenter=mock_augmenter,
            preprocessor=mock_preprocessor,
        )

        # í•„ìˆ˜ ë©”íƒ€ë°ì´í„° ì¡´ì¬ ê²€ì¦
        required_keys = ['metrics', 'training_methodology']
        for key in required_keys:
            assert key in training_results, f"training_resultsì— '{key}' ë©”íƒ€ë°ì´í„°ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        # training_methodologyì˜ Data Leakage ë°©ì§€ ë©”íƒ€ë°ì´í„° ê²€ì¦
        tm_data = training_results['training_methodology']
        assert 'preprocessing_fit_scope' in tm_data
        assert tm_data['preprocessing_fit_scope'] == 'train_only'
        
        # metrics ë°ì´í„° íƒ€ì… ê²€ì¦
        assert isinstance(training_results['metrics'], dict)

    # ğŸ†• Blueprint v17.0: ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    def test_trainer_handles_invalid_model_gracefully(self, local_test_settings: Settings):
        """
        ì˜ëª»ëœ ëª¨ë¸ì´ ì£¼ì…ëœ ê²½ìš° ì ì ˆíˆ ì²˜ë¦¬í•˜ëŠ”ì§€ ê²€ì¦í•œë‹¤.
        """
        trainer = Trainer(settings=local_test_settings)
        
        # ì˜ëª»ëœ ëª¨ë¸ (fit ë©”ì„œë“œê°€ ì—†ëŠ” ê°ì²´)
        invalid_model = "This is not a model"
        
        mock_preprocessor = Mock()
        mock_augmenter = PassThroughAugmenter(settings=local_test_settings)
        
        df = pd.DataFrame({
            'user_id': ['u1', 'u2'],
            'approved': [1, 0]
        })

        # ì ì ˆí•œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ”ì§€ ê²€ì¦
        with pytest.raises(AttributeError):
            trainer.train(
                df=df,
                model=invalid_model,
                augmenter=mock_augmenter,
                preprocessor=mock_preprocessor,
            ) 