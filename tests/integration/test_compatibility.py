"""
Blueprint v17.0 í˜¸í™˜ì„± ë³´ì¥ í†µí•© í…ŒìŠ¤íŠ¸ (í˜„ëŒ€í™”)

ê¸°ì¡´ ì½”ë“œì™€ ìƒˆë¡œìš´ ê¸°ëŠ¥ì´ í•¨ê»˜ ì •ìƒ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦í•˜ëŠ” í…ŒìŠ¤íŠ¸

Blueprint ì›ì¹™ ê²€ì¦:
- ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ì™„ì „í•œ í•˜ìœ„ í˜¸í™˜ì„±
- ìƒˆë¡œìš´ ê¸°ëŠ¥ì˜ ì ì§„ì  í™œì„±í™”
- ì¤‘ì•™ fixture ì‚¬ìš© í†µì¼
"""

import pytest
import pandas as pd
from unittest.mock import Mock, patch, MagicMock
from src.settings import Settings
from src.components.trainer import Trainer
from src.engine.factory import Factory


pytest.skip("Deprecated/outdated test module pending Stage 6 test overhaul (compatibility tests will be rewritten).", allow_module_level=True)


class TestBlueprintV17CompatibilityModernized:
    """Blueprint v17.0 ì „ì²´ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ (í˜„ëŒ€í™”)"""
    
    def test_existing_workflow_unchanged(self, local_test_settings: Settings):
        """
        ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°ê°€ ë³€ê²½ ì—†ì´ ë™ì‘í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•œë‹¤.
        Blueprint ì›ì¹™: 100% í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥
        """
        # LOCAL í™˜ê²½ì—ì„œëŠ” ê¸°ì¡´ ì„¤ì • ë°©ì‹ ìœ ì§€ (HPO ë¹„í™œì„±í™”)
        s = local_test_settings
        
        # ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ë“¤ì´ ì •ìƒ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸
        factory = Factory(s)
        trainer = Trainer(s)
        
        # ê¸°ì¡´ ì–´ëŒ‘í„°ë“¤ ìƒì„± ê°€ëŠ¥
        augmenter = factory.create_augmenter()
        preprocessor = factory.create_preprocessor()
        model = factory.create_model()
        
        # ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ì˜¬ë°”ë¥¸ íƒ€ì…ì¸ì§€ í™•ì¸
        from src.core.augmenter import Augmenter, PassThroughAugmenter
        from src.core.preprocessor import Preprocessor
        
        # LOCAL í™˜ê²½ì—ì„œëŠ” PassThroughAugmenter ì‚¬ìš©
        assert isinstance(augmenter, PassThroughAugmenter)
        assert isinstance(preprocessor, Preprocessor)
        assert trainer.settings == s
        
        # ê¸°ì¡´ ëª¨ë¸ í´ë˜ìŠ¤ ë¡œë”© í™•ì¸
        assert s.model.class_path == "sklearn.ensemble.RandomForestClassifier"
        print("âœ… ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° í•˜ìœ„ í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ")
    
    @patch('src.core.trainer.mlflow')
    def test_existing_training_produces_compatible_results(self, mock_mlflow, local_test_settings: Settings):
        """
        ê¸°ì¡´ í•™ìŠµ ë°©ì‹ì´ í˜¸í™˜ë˜ëŠ” ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•œë‹¤.
        """
        trainer = Trainer(local_test_settings)
        
        # Mock ì»´í¬ë„ŒíŠ¸ ì„¤ì •
        mock_model = Mock()
        mock_model.fit.return_value = mock_model
        
        mock_preprocessor = Mock()
        mock_preprocessor.fit.return_value = mock_preprocessor
        mock_preprocessor.transform.return_value = pd.DataFrame({'feature1': [0.1, 0.2]})
        
        from src.core.augmenter import PassThroughAugmenter
        mock_augmenter = PassThroughAugmenter(settings=local_test_settings)
        
        # ìƒ˜í”Œ ë°ì´í„°
        sample_data = pd.DataFrame({
            'user_id': ['u1', 'u2', 'u3', 'u4'],
            'feature1': [1, 2, 3, 4],
            'feature2': [0.1, 0.2, 0.3, 0.4],
            'approved': [0, 1, 0, 1]  # target column
        })
        
        # ê¸°ì¡´ ë°©ì‹ í•™ìŠµ ì‹¤í–‰
        trained_preprocessor, trained_model, training_results = trainer.train(
            df=sample_data,
            model=mock_model,
            augmenter=mock_augmenter,
            preprocessor=mock_preprocessor
        )
        
        # ê²°ê³¼ êµ¬ì¡° í™•ì¸ (Blueprint v17.0 í™•ì¥ í¬í•¨)
        assert trained_preprocessor is not None
        assert trained_model is not None
        assert isinstance(training_results, dict)
        
        # ê¸°ì¡´ metrics ìœ ì§€
        assert "metrics" in training_results
        
        # ìƒˆë¡œìš´ ë©”íƒ€ë°ì´í„° í¬í•¨ í™•ì¸ (ê¸°ë³¸ê°’ìœ¼ë¡œ)
        assert "hyperparameter_optimization" in training_results
        hpo_data = training_results["hyperparameter_optimization"]
        assert not hpo_data.get("enabled", False), "LOCAL í™˜ê²½ì—ì„œëŠ” HPOê°€ ë¹„í™œì„±í™”ë˜ì–´ì•¼ í•¨"
        
        assert "training_methodology" in training_results
        tm_data = training_results["training_methodology"]
        assert tm_data["preprocessing_fit_scope"] == "train_only"
        print("âœ… ê¸°ì¡´ í•™ìŠµ ë°©ì‹ í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ")

    def test_existing_pyfunc_wrapper_creation(self, local_test_settings: Settings):
        """
        ê¸°ì¡´ PyfuncWrapper ìƒì„± ë°©ì‹ì´ í˜¸í™˜ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•œë‹¤.
        """
        factory = Factory(local_test_settings)
        
        # Mock ì»´í¬ë„ŒíŠ¸ë“¤
        trained_model = Mock()
        trained_preprocessor = Mock()
        
        # ìµœì†Œí•œì˜ training_results (ê¸°ì¡´ í˜¸í™˜ì„±)
        basic_training_results = {
            "metrics": {"accuracy": 0.85},
            "hyperparameter_optimization": {"enabled": False},
            "training_methodology": {"preprocessing_fit_scope": "train_only"},
            "loader_sql_snapshot": "SELECT user_id, product_id FROM test_table",
            "augmenter_config_snapshot": {"type": "passthrough"},
            "model_class_path": local_test_settings.model.class_path
        }
        
        # PyfuncWrapper ìƒì„±
        wrapper = factory.create_pyfunc_wrapper(
            trained_model=trained_model,
            trained_preprocessor=trained_preprocessor,
            training_results=basic_training_results
        )
        
        # ê¸°ì¡´ ì†ì„±ë“¤ì´ ìœ ì§€ë˜ëŠ”ì§€ í™•ì¸
        assert wrapper.trained_model == trained_model
        assert wrapper.trained_preprocessor == trained_preprocessor
        assert wrapper.model_class_path == local_test_settings.model.class_path
        print("âœ… ê¸°ì¡´ PyfuncWrapper ìƒì„± ë°©ì‹ í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ")

    def test_new_features_activation_in_dev_env(self, dev_test_settings: Settings):
        """
        DEV í™˜ê²½ì—ì„œ ìƒˆë¡œìš´ ê¸°ëŠ¥ë“¤ì´ ì˜¬ë°”ë¥´ê²Œ í™œì„±í™”ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•œë‹¤.
        """
        # DEV í™˜ê²½ì—ì„œëŠ” ìƒˆë¡œìš´ ê¸°ëŠ¥ë“¤ì´ í™œì„±í™”ë˜ì–´ì•¼ í•¨
        s = dev_test_settings
        
        # HPO í™œì„±í™” í™•ì¸
        assert s.model.hyperparameter_tuning is not None
        assert s.model.hyperparameter_tuning.enabled == True
        
        # Feature Store í™œì„±í™” í™•ì¸
        assert s.model.augmenter.type == "feature_store"
        
        # ìƒˆë¡œìš´ ê¸°ëŠ¥ì´ í™œì„±í™”ëœ Factory ìƒì„±
        factory = Factory(s)
        
        # DEV í™˜ê²½ì—ì„œëŠ” ì‹¤ì œ Augmenter ì‚¬ìš© (PassThroughê°€ ì•„ë‹˜)
        augmenter = factory.create_augmenter()
        from src.core.augmenter import Augmenter, PassThroughAugmenter
        assert isinstance(augmenter, Augmenter)
        assert not isinstance(augmenter, PassThroughAugmenter)
        
        print("âœ… DEV í™˜ê²½ ìƒˆë¡œìš´ ê¸°ëŠ¥ í™œì„±í™” ê²€ì¦ ì™„ë£Œ")

    @patch('src.core.trainer.optuna')
    @patch('src.core.trainer.mlflow')
    def test_hyperparameter_optimization_integration(self, mock_mlflow, mock_optuna, dev_test_settings: Settings):
        """
        í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê¸°ëŠ¥ì´ ì˜¬ë°”ë¥´ê²Œ í†µí•©ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•œë‹¤.
        """
        # DEV í™˜ê²½ ì„¤ì •ì—ì„œ HPO í™•ì¸
        s = dev_test_settings
        assert s.model.hyperparameter_tuning.enabled == True
        
        # Optuna Mock ì„¤ì •
        mock_study = Mock()
        mock_trial = Mock()
        mock_trial.number = 1
        mock_trial.suggest_int.return_value = 100
        mock_trial.suggest_float.return_value = 0.1
        mock_optuna.create_study.return_value = mock_study
        mock_study.best_trial = mock_trial
        mock_study.best_trial.value = 0.95
        mock_study.best_trial.params = {"n_estimators": 100, "learning_rate": 0.1}
        mock_study.trials = [mock_trial]
        
        trainer = Trainer(s)
        
        # Mock ì»´í¬ë„ŒíŠ¸ë“¤
        mock_model = Mock()
        mock_preprocessor = Mock()
        mock_preprocessor.fit.return_value = mock_preprocessor
        mock_preprocessor.transform.return_value = pd.DataFrame({'f1': [0.1, 0.2]})
        
        mock_augmenter = Mock()
        mock_augmenter.augment.return_value = pd.DataFrame({'f1': [0.1, 0.2], 'approved': [1, 0]})
        
        sample_data = pd.DataFrame({
            'user_id': ['u1', 'u2', 'u3', 'u4'],
            'approved': [1, 0, 1, 0]
        })
        
        # HPOê°€ í™œì„±í™”ëœ í•™ìŠµ ì‹¤í–‰
        trained_preprocessor, trained_model, training_results = trainer.train(
            df=sample_data,
            model=mock_model,
            augmenter=mock_augmenter,
            preprocessor=mock_preprocessor
        )
        
        # HPO ê²°ê³¼ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert "hyperparameter_optimization" in training_results
        hpo_data = training_results["hyperparameter_optimization"]
        assert hpo_data["enabled"] == True
        assert "best_params" in hpo_data
        assert "best_score" in hpo_data
        
        # Optuna í˜¸ì¶œ í™•ì¸
        mock_optuna.create_study.assert_called_once()
        print("âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í†µí•© ê²€ì¦ ì™„ë£Œ")

    def test_settings_backward_compatibility(self, local_test_settings: Settings, dev_test_settings: Settings):
        """
        ì„¤ì • êµ¬ì¡°ì˜ í•˜ìœ„ í˜¸í™˜ì„±ì„ ê²€ì¦í•œë‹¤.
        """
        # ëª¨ë“  í™˜ê²½ì—ì„œ ê¸°ë³¸ ì†ì„±ë“¤ ì¡´ì¬ í™•ì¸
        for settings in [local_test_settings, dev_test_settings]:
            # ê¸°ì¡´ í•„ìˆ˜ ì†ì„±ë“¤
            assert hasattr(settings, 'environment')
            assert hasattr(settings, 'mlflow')
            assert hasattr(settings, 'serving')
            assert hasattr(settings, 'data_adapters')
            assert hasattr(settings, 'model')
            
            # ìƒˆë¡œìš´ ì†ì„±ë“¤ (v17.0)
            assert hasattr(settings, 'hyperparameter_tuning')
            
            # ëª¨ë¸ ë ˆë²¨ ìƒˆë¡œìš´ ì†ì„±ë“¤
            assert hasattr(settings.model, 'hyperparameter_tuning')
        
        # í™˜ê²½ë³„ ì°¨ì´ í™•ì¸
        # LOCAL: ë³´ìˆ˜ì  ì„¤ì •
        assert not local_test_settings.model.hyperparameter_tuning.enabled
        
        # DEV: ì‹ ê¸°ëŠ¥ í™œì„±í™”
        assert dev_test_settings.model.hyperparameter_tuning.enabled
        
        print("âœ… ì„¤ì • í•˜ìœ„ í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ")

    def test_feature_store_environment_differentiation(self, local_test_settings: Settings, dev_test_settings: Settings):
        """
        Feature Store ê¸°ëŠ¥ì˜ í™˜ê²½ë³„ ì°¨ë“± ì ìš©ì„ ê²€ì¦í•œë‹¤.
        Blueprint ì›ì¹™ 9: í™˜ê²½ë³„ ì°¨ë“±ì  ê¸°ëŠ¥ ë¶„ë¦¬
        """
        # LOCAL í™˜ê²½: PassThrough ë°©ì‹
        local_factory = Factory(local_test_settings)
        local_augmenter = local_factory.create_augmenter()
        
        from src.core.augmenter import PassThroughAugmenter
        assert isinstance(local_augmenter, PassThroughAugmenter)
        
        # DEV í™˜ê²½: Feature Store ì—°ë™
        dev_factory = Factory(dev_test_settings)
        with patch.object(dev_factory, 'create_feature_store_adapter'):
            dev_augmenter = dev_factory.create_augmenter()
            from src.core.augmenter import Augmenter
            assert isinstance(dev_augmenter, Augmenter)
            assert not isinstance(dev_augmenter, PassThroughAugmenter)
        
        print("âœ… Feature Store í™˜ê²½ë³„ ì°¨ë“± ì ìš© ê²€ì¦ ì™„ë£Œ")

    def test_blueprint_principle_compliance_comprehensive(self, local_test_settings: Settings, dev_test_settings: Settings):
        """
        Blueprint v17.0ì˜ 10ëŒ€ í•µì‹¬ ì›ì¹™ ì¤€ìˆ˜ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€ì¦í•œë‹¤.
        """
        # ì›ì¹™ 1: ë ˆì‹œí”¼ëŠ” ë…¼ë¦¬, ì„¤ì •ì€ ì¸í”„ë¼
        assert local_test_settings.model.class_path == dev_test_settings.model.class_path  # ë…¼ë¦¬ ë™ì¼
        assert local_test_settings.environment.app_env != dev_test_settings.environment.app_env  # ì¸í”„ë¼ ë‹¤ë¦„
        
        # ì›ì¹™ 3: URI ê¸°ë°˜ ë™ì‘ ë° ë™ì  íŒ©í† ë¦¬
        local_factory = Factory(local_test_settings)
        dev_factory = Factory(dev_test_settings)
        
        # ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ ë‹¤ë¥¸ êµ¬í˜„ì²´ ìƒì„±
        local_model = local_factory.create_model()
        dev_model = dev_factory.create_model()
        assert type(local_model) == type(dev_model)  # ë™ì¼ í´ë˜ìŠ¤
        
        # ì›ì¹™ 9: í™˜ê²½ë³„ ì°¨ë“±ì  ê¸°ëŠ¥ ë¶„ë¦¬
        local_augmenter = local_factory.create_augmenter()
        with patch.object(dev_factory, 'create_feature_store_adapter'):
            dev_augmenter = dev_factory.create_augmenter()
        
        # í™˜ê²½ë³„ë¡œ ë‹¤ë¥¸ Augmenter êµ¬í˜„
        assert type(local_augmenter) != type(dev_augmenter)
        
        print("âœ… Blueprint v17.0 10ëŒ€ ì›ì¹™ ì¢…í•© ì¤€ìˆ˜ ê²€ì¦ ì™„ë£Œ")
        print("ğŸ‰ ëª¨ë“  í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ í†µê³¼! Blueprint v17.0 ì™„ì „ í˜¸í™˜ì„± í™•ë³´") 