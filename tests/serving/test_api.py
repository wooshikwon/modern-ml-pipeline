"""
Serving API í…ŒìŠ¤íŠ¸ (Blueprint v17.0 í˜„ëŒ€í™”)

Blueprint v17.0ì˜ ìƒˆë¡œìš´ API ì—”ë“œí¬ì¸íŠ¸ë“¤ê³¼ E2E í…ŒìŠ¤íŠ¸ ê°•í™”

Blueprint ì›ì¹™ ê²€ì¦:
- ì›ì¹™ 6: ìê¸° ê¸°ìˆ  API (Self-Describing API)
- ì™„ì „í•œ ì»¤ë²„ë¦¬ì§€: ëª¨ë“  ë©”íƒ€ë°ì´í„° ì—”ë“œí¬ì¸íŠ¸ ê²€ì¦
"""

import pytest
import pandas as pd
from unittest.mock import Mock, patch, MagicMock
from fastapi.testclient import TestClient
from src.settings import Settings
import os

import mlflow
import shutil

from src.settings import Settings
from serving.api import app, setup_api_context
from src.pipelines.train_pipeline import run_training

@pytest.fixture(scope="module")
def trained_model_run_id_for_api(dev_test_settings: Settings):
    """
    API í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë¯¸ë¦¬ í•™ìŠµëœ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë¥¼ ìƒì„±í•˜ê³ 
    í•´ë‹¹ run_idë¥¼ ì œê³µí•˜ëŠ” Fixture.
    """
    test_tracking_uri = "./test_mlruns_api"
    mlflow.set_tracking_uri(test_tracking_uri)
    
    result_artifact = run_training(settings=dev_test_settings)
    
    yield result_artifact.run_id
    
    shutil.rmtree(test_tracking_uri, ignore_errors=True)
    mlflow.set_tracking_uri("mlruns")

@pytest.mark.requires_dev_stack
class TestServingAPIComplete:
    """
    Serving APIì˜ ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ë¥¼ ê²€ì¦í•˜ëŠ” ì™„ì „í•œ E2E í…ŒìŠ¤íŠ¸.
    ì‹¤ì œ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ APIì˜ ë™ì‘ì„ ê²€ì¦í•œë‹¤.
    Blueprint v17.0 ì™„ì „ í˜„ëŒ€í™”
    """

    @pytest.fixture(scope="class")
    def client(self, dev_test_settings: Settings, trained_model_run_id_for_api: str):
        """
        í…ŒìŠ¤íŠ¸ìš© API í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„¤ì •í•˜ëŠ” Fixture.
        í•™ìŠµëœ ëª¨ë¸ë¡œ API ì»¨í…ìŠ¤íŠ¸ë¥¼ ì´ˆê¸°í™”í•œë‹¤.
        """
        setup_api_context(run_id=trained_model_run_id_for_api, settings=dev_test_settings)
        return TestClient(app)

    def test_health_endpoint(self, client: TestClient, trained_model_run_id_for_api: str):
        """GET /health: API ì„œë²„ì˜ ìƒíƒœì™€ ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ë¥¼ í™•ì¸í•œë‹¤."""
        response = client.get("/health")
        assert response.status_code == 200
        data = response.json()
        assert data["status"] == "healthy"
        assert trained_model_run_id_for_api in data["model_uri"]
        assert data["model_name"] == "sklearn.ensemble.RandomForestClassifier"

    def test_root_endpoint(self, client: TestClient, trained_model_run_id_for_api: str):
        """GET /: ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ê°€ ì˜¬ë°”ë¥¸ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤."""
        response = client.get("/")
        assert response.status_code == 200
        data = response.json()
        assert data["message"] == "Modern ML Pipeline API"
        assert data["status"] == "ready"
        assert trained_model_run_id_for_api in data["model_uri"]

    def test_predict_endpoint(self, client: TestClient):
        """POST /predict: ë‹¨ì¼ ì˜ˆì¸¡ ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ë˜ëŠ”ì§€ í™•ì¸í•œë‹¤."""
        # dev_classification_test.yamlì˜ loaderê°€ user_idì™€ product_idë¥¼ ìš”êµ¬í•œë‹¤ê³  ê°€ì •
        request_data = {"user_id": "u1001", "product_id": "p2001"}
        response = client.post("/predict", json=request_data)
        
        assert response.status_code == 200
        data = response.json()
        assert "prediction" in data
        assert isinstance(data["prediction"], float)
        assert "input_features" in data
        assert data["input_features"]["product_price"] > 0 # ì˜ˆì‹œ ê²€ì¦

    def test_predict_endpoint_invalid_input(self, client: TestClient):
        """POST /predict: ì˜ëª»ëœ ì…ë ¥ì— ëŒ€í•´ 422 Unprocessable Entityë¥¼ ë°˜í™˜í•˜ëŠ”ì§€ í™•ì¸í•œë‹¤."""
        # product_id í•„ë“œê°€ ëˆ„ë½ëœ ìš”ì²­
        request_data = {"user_id": "u1001"}
        response = client.post("/predict", json=request_data)
        assert response.status_code == 422

    # ğŸ†• Blueprint v17.0: ì™„ì „í•œ ë©”íƒ€ë°ì´í„° ì—”ë“œí¬ì¸íŠ¸ ê²€ì¦
    def test_model_metadata_endpoint_complete(self, client: TestClient):
        """
        GET /model/metadata: ëª¨ë¸ì˜ ì™„ì „í•œ ë©”íƒ€ë°ì´í„° ë°˜í™˜ì„ ê²€ì¦í•œë‹¤.
        Blueprint ì›ì¹™ 6: ìê¸° ê¸°ìˆ  API
        """
        response = client.get("/model/metadata")
        assert response.status_code == 200
        data = response.json()
        
        # í•„ìˆ˜ ë©”íƒ€ë°ì´í„° í•„ë“œ ê²€ì¦
        required_fields = [
            "model_uri", "model_class_path", "training_timestamp",
            "hyperparameter_optimization", "training_methodology", "api_schema"
        ]
        for field in required_fields:
            assert field in data, f"ë©”íƒ€ë°ì´í„°ì—ì„œ '{field}' í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë©”íƒ€ë°ì´í„° ìƒì„¸ ê²€ì¦
        hpo = data["hyperparameter_optimization"]
        assert isinstance(hpo["enabled"], bool)
        if hpo["enabled"]:
            assert "best_params" in hpo
            assert "best_score" in hpo
            assert "total_trials" in hpo
        
        # í•™ìŠµ ë°©ë²•ë¡  ë©”íƒ€ë°ì´í„° ê²€ì¦ (Data Leakage ë°©ì§€)
        tm = data["training_methodology"]
        assert tm["preprocessing_fit_scope"] == "train_only"
        assert "train_test_split_method" in tm
        
        # API ìŠ¤í‚¤ë§ˆ ì •ë³´ ê²€ì¦
        api_schema = data["api_schema"]
        assert "input_fields" in api_schema
        assert isinstance(api_schema["input_fields"], list)

    def test_model_optimization_endpoint(self, client: TestClient):
        """
        GET /model/optimization: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ìƒì„¸ ì •ë³´ë¥¼ ê²€ì¦í•œë‹¤.
        """
        response = client.get("/model/optimization")
        assert response.status_code == 200
        data = response.json()
        
        # ê¸°ë³¸ í•„ë“œ ê²€ì¦
        assert "enabled" in data
        assert "optimization_history" in data
        assert "search_space" in data
        assert "convergence_info" in data
        
        # í™œì„±í™” ì—¬ë¶€ì— ë”°ë¥¸ ìƒì„¸ ê²€ì¦
        if data["enabled"]:
            assert isinstance(data["optimization_history"], list)
            assert isinstance(data["search_space"], dict)
            assert "best_score" in data["convergence_info"]
        else:
            assert data["optimization_history"] == []
            assert data["search_space"] == {}

    def test_model_schema_endpoint(self, client: TestClient):
        """
        GET /model/schema: ë™ì ìœ¼ë¡œ ìƒì„±ëœ API ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ê²€ì¦í•œë‹¤.
        Blueprint ì›ì¹™ 6: ìê¸° ê¸°ìˆ  APIì˜ í•µì‹¬
        """
        response = client.get("/model/schema")
        assert response.status_code == 200
        data = response.json()
        
        # í•„ìˆ˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ ê²€ì¦
        required_schema_fields = [
            "prediction_request_schema", "batch_prediction_request_schema",
            "loader_sql_snapshot", "extracted_fields", "feature_store_info"
        ]
        for field in required_schema_fields:
            assert field in data, f"ìŠ¤í‚¤ë§ˆ ì •ë³´ì—ì„œ '{field}' í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        # ë™ì  ìŠ¤í‚¤ë§ˆ ê²€ì¦
        prediction_schema = data["prediction_request_schema"]
        assert "properties" in prediction_schema
        assert isinstance(prediction_schema["properties"], dict)
        
        # SQL ìŠ¤ëƒ…ìƒ· ê²€ì¦
        assert isinstance(data["loader_sql_snapshot"], str)
        assert len(data["loader_sql_snapshot"]) > 0
        
        # ì¶”ì¶œëœ í•„ë“œ ê²€ì¦
        assert isinstance(data["extracted_fields"], list)
        assert len(data["extracted_fields"]) > 0

    # ğŸ†• Blueprint v17.0: ë°°ì¹˜ ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸ (ìƒˆë¡œ ì¶”ê°€)
    def test_batch_predict_endpoint(self, client: TestClient):
        """
        POST /batch_predict: ë°°ì¹˜ ì˜ˆì¸¡ ìš”ì²­ì´ ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ë˜ëŠ”ì§€ í™•ì¸í•œë‹¤.
        """
        # ë°°ì¹˜ ìš”ì²­ ë°ì´í„°
        batch_request = {
            "requests": [
                {"user_id": "u1001", "product_id": "p2001"},
                {"user_id": "u1002", "product_id": "p2002"}
            ]
        }
        
        response = client.post("/batch_predict", json=batch_request)
        assert response.status_code == 200
        data = response.json()
        
        assert "predictions" in data
        assert len(data["predictions"]) == 2
        for prediction in data["predictions"]:
            assert "prediction" in prediction
            assert isinstance(prediction["prediction"], float)

    # ğŸ†• Blueprint v17.0: ì—ëŸ¬ ì²˜ë¦¬ ê²€ì¦
    def test_api_error_handling(self, client: TestClient):
        """APIì˜ ì ì ˆí•œ ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ê²€ì¦í•œë‹¤."""
        # 1. ì˜ëª»ëœ ì—”ë“œí¬ì¸íŠ¸
        response = client.get("/nonexistent")
        assert response.status_code == 404
        
        # 2. ì˜ëª»ëœ HTTP ë©”ì„œë“œ
        response = client.get("/predict")  # POSTë§Œ í—ˆìš©
        assert response.status_code == 405
        
        # 3. ì˜ëª»ëœ JSON í˜•ì‹
        response = client.post("/predict", 
                              data="invalid json", 
                              headers={"Content-Type": "application/json"})
        assert response.status_code == 422

    # ğŸ†• Blueprint v17.0: ì„±ëŠ¥ ê²€ì¦
    def test_api_performance_targets(self, client: TestClient):
        """API ì‘ë‹µ ì‹œê°„ ëª©í‘œ ë‹¬ì„±ì„ ê²€ì¦í•œë‹¤."""
        import time
        
        request_data = {"user_id": "u1001", "product_id": "p2001"}
        
        # ì—¬ëŸ¬ ë²ˆ ì¸¡ì •í•˜ì—¬ í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_times = []
        for _ in range(5):
            start_time = time.time()
            response = client.post("/predict", json=request_data)
            end_time = time.time()
            
            assert response.status_code == 200
            response_times.append((end_time - start_time) * 1000)  # msë¡œ ë³€í™˜
        
        avg_response_time = sum(response_times) / len(response_times)
        
        # ì„±ëŠ¥ ëª©í‘œ: í‰ê·  ì‘ë‹µ ì‹œê°„ 500ms ì´í•˜ (DEV í™˜ê²½ ê¸°ì¤€)
        assert avg_response_time < 500, f"í‰ê·  ì‘ë‹µ ì‹œê°„ ëª©í‘œ ë¯¸ë‹¬ì„±: {avg_response_time:.2f}ms"


# ğŸ†• Blueprint v17.0: ìê¸° ê¸°ìˆ  API ì‹¬í™” í…ŒìŠ¤íŠ¸
@pytest.mark.requires_dev_stack
@pytest.mark.blueprint_principle_6
class TestSelfDescribingAPIAdvanced:
    """ìê¸° ê¸°ìˆ  APIì˜ ê³ ê¸‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def setup_method(self):
        os.environ['APP_ENV'] = 'dev'
    
    def teardown_method(self):
        if 'APP_ENV' in os.environ:
            del os.environ['APP_ENV']

    def test_dynamic_schema_adaptation(self, dev_test_settings: Settings, trained_model_run_id_for_api: str):
        """
        ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì˜ loader SQLì— ë”°ë¼ API ìŠ¤í‚¤ë§ˆê°€ ë™ì ìœ¼ë¡œ ë³€ê²½ë˜ëŠ”ì§€ ê²€ì¦í•œë‹¤.
        """
        # API ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        setup_api_context(run_id=trained_model_run_id_for_api, settings=dev_test_settings)
        client = TestClient(app)
        
        # ìŠ¤í‚¤ë§ˆ ì •ë³´ ì¡°íšŒ
        response = client.get("/model/schema")
        assert response.status_code == 200
        data = response.json()
        
        # í˜„ì¬ ëª¨ë¸ì˜ ìŠ¤í‚¤ë§ˆ í•„ë“œ í™•ì¸
        current_fields = data["extracted_fields"]
        
        # ì˜ˆìƒë˜ëŠ” í•„ë“œë“¤ (dev_classification_test.yaml ê¸°ì¤€)
        expected_fields = ["user_id", "product_id"]  # event_timestampëŠ” ì œì™¸
        for field in expected_fields:
            assert field in current_fields, f"ì˜ˆìƒ í•„ë“œ '{field}'ê°€ ì¶”ì¶œëœ í•„ë“œì— ì—†ìŠµë‹ˆë‹¤."

    def test_feature_store_integration_visibility(self, dev_test_settings: Settings, trained_model_run_id_for_api: str):
        """
        APIê°€ Feature Store ì—°ë™ ì •ë³´ë¥¼ ì˜¬ë°”ë¥´ê²Œ ë…¸ì¶œí•˜ëŠ”ì§€ ê²€ì¦í•œë‹¤.
        """
        setup_api_context(run_id=trained_model_run_id_for_api, settings=dev_test_settings)
        client = TestClient(app)
        
        response = client.get("/model/schema")
        assert response.status_code == 200
        data = response.json()
        
        # Feature Store ì •ë³´ ê²€ì¦
        fs_info = data["feature_store_info"]
        assert "feature_columns" in fs_info
        assert "join_key" in fs_info
        assert isinstance(fs_info["feature_columns"], list)

    def test_api_documentation_completeness(self, dev_test_settings: Settings, trained_model_run_id_for_api: str):
        """
        API ë¬¸ì„œí™”ê°€ ì™„ì „í•œì§€ ê²€ì¦í•œë‹¤. (FastAPI ìë™ ìƒì„± ë¬¸ì„œ)
        """
        setup_api_context(run_id=trained_model_run_id_for_api, settings=dev_test_settings)
        client = TestClient(app)
        
        # OpenAPI ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
        response = client.get("/openapi.json")
        assert response.status_code == 200
        openapi_schema = response.json()
        
        # í•„ìˆ˜ ì—”ë“œí¬ì¸íŠ¸ê°€ ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        paths = openapi_schema["paths"]
        expected_endpoints = ["/", "/health", "/predict", "/model/metadata", "/model/schema", "/model/optimization"]
        
        for endpoint in expected_endpoints:
            assert endpoint in paths, f"API ë¬¸ì„œì—ì„œ '{endpoint}' ì—”ë“œí¬ì¸íŠ¸ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        # API ë¬¸ì„œ UI ì ‘ê·¼ ê°€ëŠ¥ì„± í™•ì¸
        docs_response = client.get("/docs")
        assert docs_response.status_code == 200


# ê¸°ì¡´ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ë“¤ ìœ ì§€ (í˜¸í™˜ì„±ì„ ìœ„í•´)
class TestServingAPIMetadataEndpoints:
    """ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    
    @pytest.fixture
    def mock_app_context_full(self):
        """ì „ì²´ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ Mock AppContext ì„¤ì •"""
        from serving.api import AppContext
        context = AppContext()
        
        # Mock ëª¨ë¸ ì„¤ì • (ì™„ì „í•œ ë©”íƒ€ë°ì´í„° í¬í•¨)
        mock_model = Mock()
        mock_model.hyperparameter_optimization = {
            "enabled": True,
            "engine": "optuna",
            "best_params": {"learning_rate": 0.1, "n_estimators": 100, "max_depth": 5},
            "best_score": 0.924,
            "total_trials": 50,
            "pruned_trials": 12,
            "optimization_time": "0:15:32",
            "optimization_history": [
                {"trial": 1, "score": 0.85, "params": {"learning_rate": 0.05}},
                {"trial": 2, "score": 0.90, "params": {"learning_rate": 0.1}}
            ],
            "search_space": {
                "learning_rate": {"type": "float", "low": 0.01, "high": 0.3},
                "n_estimators": {"type": "int", "low": 50, "high": 200}
            },
            "timeout_occurred": False
        }
        mock_model.training_methodology = {
            "train_test_split_method": "stratified",
            "train_ratio": 0.8,
            "validation_strategy": "train_validation_split",
            "preprocessing_fit_scope": "train_only",
            "random_state": 42
        }
        mock_model.model_class_path = "causalml.inference.meta.XGBTRegressor"
        mock_model.training_metadata = {
            "timestamp": "2024-01-01T10:30:00",
            "mlflow_run_id": "abc123def456"
        }
        mock_model.loader_sql_snapshot = "SELECT user_id, product_id, session_id FROM users WHERE active = 1"
        
        context.model = mock_model
        context.model_uri = "runs:/abc123def456/model"
        context.feature_columns = ["age", "country", "ltv", "click_count"]
        context.join_key = "user_id"
        
        # Mock settings
        mock_settings = Mock()
        mock_settings.serving.realtime_feature_store = {"type": "redis", "host": "localhost"}
        context.settings = mock_settings
        context.feature_store_config = {"type": "redis", "host": "localhost"}
        
        # Mock Pydantic ëª¨ë¸
        from pydantic import BaseModel, Field
        class MockPredictionRequest(BaseModel):
            user_id: str = Field(..., description="User ID")
            product_id: str = Field(..., description="Product ID")
            session_id: str = Field(..., description="Session ID")
        
        context.PredictionRequest = MockPredictionRequest
        context.BatchPredictionRequest = Mock()
        
        return context

    @patch('serving.api.app_context')
    def test_model_metadata_endpoint_mock(self, mock_context, mock_app_context_full):
        """GET /model/metadata ì—”ë“œí¬ì¸íŠ¸ Mock í…ŒìŠ¤íŠ¸ (ê¸°ì¡´ í˜¸í™˜ì„±)"""
        mock_context.return_value = mock_app_context_full
        
        client = TestClient(app)
        
        # API í˜¸ì¶œ
        response = client.get("/model/metadata")
        
        # ì‘ë‹µ í™•ì¸
        assert response.status_code == 200
        data = response.json()
        
        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° í™•ì¸
        assert data["model_uri"] == "runs:/abc123def456/model"
        assert data["model_class_path"] == "causalml.inference.meta.XGBTRegressor"
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì •ë³´ í™•ì¸
        hpo = data["hyperparameter_optimization"]
        assert hpo["enabled"] is True
        assert hpo["engine"] == "optuna"
        assert hpo["best_score"] == 0.924
        assert hpo["total_trials"] == 50
        assert hpo["pruned_trials"] == 12
        assert "learning_rate" in hpo["best_params"]
        
        # í•™ìŠµ ë°©ë²•ë¡  ì •ë³´ í™•ì¸
        tm = data["training_methodology"]
        assert tm["train_test_split_method"] == "stratified"
        assert tm["preprocessing_fit_scope"] == "train_only"
        assert tm["random_state"] == 42
        
        # API ìŠ¤í‚¤ë§ˆ ì •ë³´ í™•ì¸
        api_schema = data["api_schema"]
        assert "input_fields" in api_schema
        assert "user_id" in api_schema["input_fields"]
        assert "feature_columns" in api_schema
        assert "join_key" in api_schema


# ğŸ†• Blueprint v17.0: í˜¸í™˜ì„± ë³´ì¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤
class TestServingAPICompatibility:
    """API í˜¸í™˜ì„± ë³´ì¥ í…ŒìŠ¤íŠ¸"""
    
    @patch('serving.api.app_context')
    def test_backward_compatibility_predict(self, mock_context):
        """ê¸°ì¡´ predict ì—”ë“œí¬ì¸íŠ¸ í•˜ìœ„ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        from serving.api import AppContext
        context = AppContext()
        
        # ìµœì í™”ê°€ ë¹„í™œì„±í™”ëœ ëª¨ë¸ Mock
        mock_model = Mock()
        mock_model.hyperparameter_optimization = {"enabled": False}
        mock_model.predict.return_value = pd.DataFrame({"uplift_score": [0.75]})
        
        context.model = mock_model
        context.model_uri = "runs:/legacy123/model"
        context.settings = Mock()
        context.settings.serving.realtime_feature_store = {}
        context.feature_store_config = {}
        context.feature_columns = []
        
        # Mock Pydantic ëª¨ë¸
        from pydantic import BaseModel, Field
        class MockPredictionRequest(BaseModel):
            user_id: str = Field(..., description="User ID")
        
        context.PredictionRequest = MockPredictionRequest
        mock_context.return_value = context
        
        from serving.api import create_app
        app = create_app("legacy_run_id")
        client = TestClient(app)
        
        # API í˜¸ì¶œ
        response = client.post("/predict", json={"user_id": "user123"})
        
        # ì‘ë‹µ í™•ì¸
        assert response.status_code == 200
        data = response.json()
        
        # ê¸°ì¡´ í•„ë“œë“¤ í™•ì¸
        assert "uplift_score" in data
        assert "model_uri" in data
        
        # ìƒˆë¡œìš´ í•„ë“œë“¤ì´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert "optimization_enabled" in data
        assert "best_score" in data
        assert data["optimization_enabled"] is False
        assert data["best_score"] == 0.0  # ê¸°ë³¸ê°’
    
    def test_response_schema_extensions(self):
        """ì‘ë‹µ ìŠ¤í‚¤ë§ˆ í™•ì¥ì´ ì˜¬ë°”ë¥´ê²Œ ì ìš©ë˜ì—ˆëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
        from serving.schemas import PredictionResponse, BatchPredictionResponse
        
        # PredictionResponse ìŠ¤í‚¤ë§ˆ í™•ì¸
        pred_schema = PredictionResponse.schema()
        properties = pred_schema["properties"]
        
        # ê¸°ì¡´ í•„ë“œë“¤ í™•ì¸
        assert "uplift_score" in properties
        assert "model_uri" in properties
        
        # ìƒˆë¡œìš´ í•„ë“œë“¤ í™•ì¸
        assert "optimization_enabled" in properties
        assert "best_score" in properties
        
        # ìƒˆë¡œìš´ í•„ë“œë“¤ì´ Optionalì¸ì§€ í™•ì¸ (ê¸°ë³¸ê°’ ìˆìŒ)
        assert properties["optimization_enabled"]["default"] is False
        assert properties["best_score"]["default"] == 0.0
        
        # BatchPredictionResponseë„ ë™ì¼í•˜ê²Œ í™•ì¸
        batch_schema = BatchPredictionResponse.schema()
        batch_properties = batch_schema["properties"]
        
        assert "optimization_enabled" in batch_properties
        assert "best_score" in batch_properties
        assert batch_properties["optimization_enabled"]["default"] is False
        assert batch_properties["best_score"]["default"] == 0.0 


# ğŸ†• í™˜ê²½ë³„ ì°¨ë“± API ì„œë¹™ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ë“¤ ì¶”ê°€
@pytest.mark.blueprint_principle_9
class TestEnvironmentSpecificAPIServing:
    """í™˜ê²½ë³„ ì°¨ë“± API ì„œë¹™ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def teardown_method(self):
        """í™˜ê²½ë³€ìˆ˜ ì •ë¦¬"""
        if 'APP_ENV' in os.environ:
            del os.environ['APP_ENV']
    
    @pytest.mark.local_env
    def test_local_env_api_serving_blocked(self):
        """LOCAL í™˜ê²½: API ì„œë¹™ ì‹œìŠ¤í…œì  ì°¨ë‹¨ ê²€ì¦"""
        os.environ['APP_ENV'] = 'local'
        
        with patch('serving.api.load_config_files') as mock_load_config:
            # LOCAL í™˜ê²½ì—ì„œ API ì„œë¹™ ì°¨ë‹¨ ì„¤ì •
            mock_config = {
                'environment': {'app_env': 'local'},
                'api_serving': {
                    'enabled': False,
                    'blocked_reason': 'LOCAL í™˜ê²½ì˜ ì² í•™ì— ë”°ë¼ ë¹ ë¥¸ ì‹¤í—˜ê³¼ ë””ë²„ê¹…ì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤.'
                }
            }
            mock_load_config.return_value = mock_config
            
            # API ì„œë¹™ ì‹œë„ ì‹œ ì°¨ë‹¨ í™•ì¸
            with pytest.raises(RuntimeError, match="LOCAL í™˜ê²½ì—ì„œëŠ” API ì„œë¹™ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"):
                from serving.api import create_app
                app = create_app("test_run_id")
    
    @pytest.mark.dev_env
    @pytest.mark.requires_dev_stack
    def test_dev_env_api_serving_enabled(self):
        """DEV í™˜ê²½: API ì„œë¹™ ì™„ì „ í—ˆìš© ê²€ì¦"""
        os.environ['APP_ENV'] = 'dev'
        
        with patch('serving.api.load_config_files') as mock_load_config, \
             patch('mlflow.pyfunc.load_model') as mock_load_model:
            
            # DEV í™˜ê²½ì—ì„œ API ì„œë¹™ í—ˆìš© ì„¤ì •
            mock_config = {
                'environment': {'app_env': 'dev'},
                'api_serving': {'enabled': True},
                'serving': {
                    'realtime_feature_store': {
                        'store_type': 'redis',
                        'connection': {'host': 'localhost', 'port': 6379}
                    }
                }
            }
            mock_load_config.return_value = mock_config
            
            # Mock Wrapped Artifact
            mock_wrapper = Mock()
            mock_wrapper.unwrap_python_model.return_value = Mock()
            mock_wrapper.unwrap_python_model.return_value.loader_sql_snapshot = 'SELECT user_id FROM users'
            mock_load_model.return_value = mock_wrapper
            
            # API ì•± ìƒì„± ì„±ê³µ í™•ì¸
            from serving.api import create_app
            app = create_app("test_run_id")
            assert app is not None
    
    @pytest.mark.prod_env
    def test_prod_env_api_serving_enhanced(self):
        """PROD í™˜ê²½: í–¥ìƒëœ API ì„œë¹™ ê¸°ëŠ¥ ê²€ì¦"""
        os.environ['APP_ENV'] = 'prod'
        
        with patch('serving.api.load_config_files') as mock_load_config, \
             patch('mlflow.pyfunc.load_model') as mock_load_model:
            
            # PROD í™˜ê²½ì—ì„œ í–¥ìƒëœ API ì„œë¹™ ì„¤ì •
            mock_config = {
                'environment': {'app_env': 'prod'},
                'api_serving': {
                    'enabled': True,
                    'enhanced_features': {
                        'monitoring': True,
                        'rate_limiting': True,
                        'caching': True,
                        'health_checks': True
                    }
                },
                'serving': {
                    'realtime_feature_store': {
                        'store_type': 'redis_cluster',
                        'connection': {'host': 'redis-cluster.prod.internal'}
                    }
                }
            }
            mock_load_config.return_value = mock_config
            
            # Mock Wrapped Artifact
            mock_wrapper = Mock()
            mock_wrapper.unwrap_python_model.return_value = Mock()
            mock_wrapper.unwrap_python_model.return_value.loader_sql_snapshot = 'SELECT user_id FROM users'
            mock_load_model.return_value = mock_wrapper
            
            # PROD í™˜ê²½ API ì•± ìƒì„± ì„±ê³µ í™•ì¸
            from serving.api import create_app
            app = create_app("test_run_id")
            assert app is not None


@pytest.mark.requires_dev_stack
@pytest.mark.blueprint_principle_6
class TestSelfDescribingAPIReal:
    """ì‹¤ì œ ì¸í”„ë¼ì—ì„œ ìê¸° ê¸°ìˆ  API í…ŒìŠ¤íŠ¸"""
    
    def setup_method(self):
        os.environ['APP_ENV'] = 'dev'
    
    def teardown_method(self):
        if 'APP_ENV' in os.environ:
            del os.environ['APP_ENV']
    
    def test_dynamic_schema_generation_real(self):
        """ì‹¤ì œ Wrapped Artifactì—ì„œ ë™ì  ìŠ¤í‚¤ë§ˆ ìƒì„± í…ŒìŠ¤íŠ¸"""
        from serving.schemas import create_dynamic_prediction_request
        from src.utils.system.sql_utils import parse_select_columns
        
        # ì‹¤ì œ Loader SQL ì˜ˆì‹œ
        real_loader_sql = """
        SELECT 
            u.user_id,
            p.product_id,
            s.session_id,
            CURRENT_TIMESTAMP as event_timestamp
        FROM users u
        CROSS JOIN products p  
        CROSS JOIN sessions s
        WHERE u.is_active = 1
        """
        
        with patch('serving.api.load_config_files') as mock_config, \
             patch('mlflow.pyfunc.load_model') as mock_load_model:
            
            mock_config.return_value = {
                'environment': {'app_env': 'dev'},
                'api_serving': {'enabled': True}
            }
            
            # ì‹¤ì œ SQL íŒŒì‹±ì„ í¬í•¨í•œ Mock Wrapper
            mock_wrapper = Mock()
            mock_inner_model = Mock()
            mock_inner_model.loader_sql_snapshot = real_loader_sql
            mock_wrapper.unwrap_python_model.return_value = mock_inner_model
            mock_load_model.return_value = mock_wrapper
            
            # ì‹¤ì œ SQL íŒŒì‹± í…ŒìŠ¤íŠ¸
            columns = parse_select_columns(real_loader_sql)
            expected_columns = ['user_id', 'product_id', 'session_id']  # event_timestamp ì œì™¸
            
            # ë™ì  ìŠ¤í‚¤ë§ˆ ìƒì„± í…ŒìŠ¤íŠ¸
            PredictionRequest = create_dynamic_prediction_request(expected_columns)
            
            # ìŠ¤í‚¤ë§ˆ ê²€ì¦
            assert hasattr(PredictionRequest, '__annotations__')
            for col in expected_columns:
                assert col in PredictionRequest.__annotations__
            
            # API ì•± ìƒì„± ë° ìŠ¤í‚¤ë§ˆ ì ìš© í™•ì¸
            from serving.api import create_app
            app = create_app("test_run_id")
            assert app is not None
    
    def test_api_consistency_with_batch_inference(self):
        """API ì…ë ¥ê³¼ ë°°ì¹˜ ì¶”ë¡  ì…ë ¥ì˜ ì™„ì „í•œ ì¼ê´€ì„± ê²€ì¦"""
        # ë°°ì¹˜ ì¶”ë¡  SQL
        batch_sql = """
        SELECT 
            customer_id,
            product_id, 
            interaction_timestamp as event_timestamp
        FROM customer_interactions
        WHERE interaction_date >= '2024-01-01'
        """
        
        with patch('serving.api.load_config_files') as mock_config, \
             patch('mlflow.pyfunc.load_model') as mock_load_model:
            
            mock_config.return_value = {
                'environment': {'app_env': 'dev'},
                'api_serving': {'enabled': True}
            }
            
            # Mock Wrapper with batch SQL
            mock_wrapper = Mock()
            mock_inner_model = Mock()
            mock_inner_model.loader_sql_snapshot = batch_sql
            mock_wrapper.unwrap_python_model.return_value = mock_inner_model
            mock_load_model.return_value = mock_wrapper
            
            # API ìŠ¤í‚¤ë§ˆ ìƒì„±
            from src.utils.system.sql_utils import parse_select_columns
            from serving.schemas import create_dynamic_prediction_request
            
            api_columns = parse_select_columns(batch_sql)
            # event_timestampëŠ” APIì—ì„œ ì œì™¸ë˜ì–´ì•¼ í•¨
            expected_api_columns = ['customer_id', 'product_id']
            
            assert set(api_columns) == set(expected_api_columns)
            
            # ë™ì  ìŠ¤í‚¤ë§ˆ ìƒì„± í™•ì¸
            PredictionRequest = create_dynamic_prediction_request(api_columns)
            
            # ë°°ì¹˜ì™€ APIì˜ ì—”í‹°í‹° í‚¤ ì¼ê´€ì„± ê²€ì¦
            for col in expected_api_columns:
                assert col in PredictionRequest.__annotations__


@pytest.mark.requires_dev_stack
@pytest.mark.blueprint_principle_5
class TestContextInjectionAPIReal:
    """ì‹¤ì œ í™˜ê²½ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì£¼ì… API í…ŒìŠ¤íŠ¸"""
    
    def setup_method(self):
        os.environ['APP_ENV'] = 'dev'
    
    def teardown_method(self):
        if 'APP_ENV' in os.environ:
            del os.environ['APP_ENV']
    
    def test_serving_mode_context_injection(self):
        """ì‹¤ì œ API ì„œë¹™ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì£¼ì… í…ŒìŠ¤íŠ¸"""
        from fastapi.testclient import TestClient
        
        with patch('serving.api.load_config_files') as mock_config, \
             patch('mlflow.pyfunc.load_model') as mock_load_model:
            
            mock_config.return_value = {
                'environment': {'app_env': 'dev'},
                'api_serving': {'enabled': True},
                'serving': {
                    'realtime_feature_store': {
                        'store_type': 'redis',
                        'connection': {'host': 'localhost', 'port': 6379}
                    }
                }
            }
            
            # Mock Wrapperê°€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ í™•ì¸
            mock_wrapper = Mock()
            mock_inner_model = Mock()
            mock_inner_model.loader_sql_snapshot = 'SELECT user_id FROM users'
            
            # ì„œë¹™ ëª¨ë“œ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ Mock
            def mock_predict(input_df, params=None):
                # ì»¨í…ìŠ¤íŠ¸ í™•ì¸
                assert params is not None
                assert params.get('run_mode') == 'serving'
                
                return pd.DataFrame({
                    'user_id': input_df['user_id'],
                    'prediction': [0.85] * len(input_df)
                })
            
            mock_wrapper.predict = mock_predict
            mock_wrapper.unwrap_python_model.return_value = mock_inner_model
            mock_load_model.return_value = mock_wrapper
            
            # API ì•± ìƒì„± ë° í…ŒìŠ¤íŠ¸
            from serving.api import create_app
            app = create_app("test_run_id")
            client = TestClient(app)
            
            # ì‹¤ì œ ì˜ˆì¸¡ ìš”ì²­ (ì»¨í…ìŠ¤íŠ¸ ì£¼ì… í™•ì¸)
            response = client.post("/predict", json={"user_id": "test_user_123"})
            
            # ì„œë¹™ ëª¨ë“œ ì»¨í…ìŠ¤íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ì£¼ì…ë˜ì—ˆëŠ”ì§€ í™•ì¸
            assert response.status_code == 200
            result = response.json()
            assert 'prediction' in result
    
    def test_batch_vs_serving_context_difference(self):
        """ë°°ì¹˜ì™€ ì„œë¹™ ì»¨í…ìŠ¤íŠ¸ ì°¨ì´ ê²€ì¦"""
        # ë™ì¼í•œ Wrapperê°€ ë‹¤ë¥¸ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹¤ë¥´ê²Œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸
        
        mock_wrapper = Mock()
        
        def context_aware_predict(input_df, params=None):
            run_mode = params.get('run_mode', 'batch') if params else 'batch'
            
            if run_mode == 'serving':
                # ì„œë¹™ ëª¨ë“œ: ì˜¨ë¼ì¸ Feature Store ì‚¬ìš© ì‹œë®¬ë ˆì´ì…˜
                return pd.DataFrame({
                    'user_id': input_df['user_id'],
                    'prediction': [0.85] * len(input_df),
                    'source': ['online_features'] * len(input_df)
                })
            else:
                # ë°°ì¹˜ ëª¨ë“œ: ì˜¤í”„ë¼ì¸ Feature Store ì‚¬ìš© ì‹œë®¬ë ˆì´ì…˜
                return pd.DataFrame({
                    'user_id': input_df['user_id'],
                    'prediction': [0.85] * len(input_df),
                    'source': ['offline_features'] * len(input_df)
                })
        
        mock_wrapper.predict = context_aware_predict
        
        test_df = pd.DataFrame({'user_id': ['u1', 'u2']})
        
        # ë°°ì¹˜ ì»¨í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
        batch_result = mock_wrapper.predict(test_df, params={'run_mode': 'batch'})
        assert batch_result['source'].iloc[0] == 'offline_features'
        
        # ì„œë¹™ ì»¨í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
        serving_result = mock_wrapper.predict(test_df, params={'run_mode': 'serving'})
        assert serving_result['source'].iloc[0] == 'online_features'


@pytest.mark.requires_dev_stack
@pytest.mark.performance
class TestAPIPerformanceReal:
    """ì‹¤ì œ API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def setup_method(self):
        os.environ['APP_ENV'] = 'dev'
    
    def teardown_method(self):
        if 'APP_ENV' in os.environ:
            del os.environ['APP_ENV']
    
    @pytest.mark.benchmark
    def test_api_response_time_target(self):
        """API ì‘ë‹µ ì‹œê°„ ëª©í‘œ ë‹¬ì„± í…ŒìŠ¤íŠ¸"""
        import time
        from fastapi.testclient import TestClient
        
        with patch('serving.api.load_config_files') as mock_config, \
             patch('mlflow.pyfunc.load_model') as mock_load_model:
            
            mock_config.return_value = {
                'environment': {'app_env': 'dev'},
                'api_serving': {'enabled': True},
                'serving': {
                    'performance_targets': {
                        'max_response_time_ms': 100  # 100ms ëª©í‘œ
                    }
                }
            }
            
            # ë¹ ë¥¸ ì˜ˆì¸¡ì„ ìœ„í•œ Mock Wrapper
            mock_wrapper = Mock()
            mock_inner_model = Mock()
            mock_inner_model.loader_sql_snapshot = 'SELECT user_id FROM users'
            
            def fast_predict(input_df, params=None):
                return pd.DataFrame({
                    'user_id': input_df['user_id'],
                    'prediction': [0.75] * len(input_df)
                })
            
            mock_wrapper.predict = fast_predict
            mock_wrapper.unwrap_python_model.return_value = mock_inner_model
            mock_load_model.return_value = mock_wrapper
            
            # API ì•± ìƒì„± ë° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            from serving.api import create_app
            app = create_app("test_run_id")
            client = TestClient(app)
            
            # ì‘ë‹µ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            response = client.post("/predict", json={"user_id": "perf_test_user"})
            end_time = time.time()
            
            response_time_ms = (end_time - start_time) * 1000
            
            # ì„±ëŠ¥ ëª©í‘œ ê²€ì¦
            assert response.status_code == 200
            assert response_time_ms < 100, f"API ì‘ë‹µ ì‹œê°„ ëª©í‘œ ë¯¸ë‹¬ì„±: {response_time_ms:.2f}ms" 