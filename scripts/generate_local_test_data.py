#!/usr/bin/env python3
"""
LOCAL í™˜ê²½ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸
Blueprint v17.0: LOCAL í™˜ê²½ì˜ ì™„ì „ ë…ë¦½ì„±ì„ ìœ„í•œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
from datetime import datetime, timedelta


def generate_classification_data():
    """ë¶„ë¥˜ ë¬¸ì œìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    n_samples = 1000
    
    # ê¸°ë³¸ í”¼ì²˜ë“¤
    data = {
        'user_id': [f'user_{i:04d}' for i in range(n_samples)],
        'age': np.random.randint(18, 70, n_samples),
        'income': np.random.normal(50000, 15000, n_samples),
        'education_level': np.random.choice(['high_school', 'bachelor', 'master', 'phd'], n_samples),
        'region': np.random.choice(['north', 'south', 'east', 'west'], n_samples),
        'credit_score': np.random.randint(300, 850, n_samples),
        'num_products': np.random.poisson(2, n_samples),
        'event_timestamp': [
            datetime.now() - timedelta(days=np.random.randint(0, 365))
            for _ in range(n_samples)
        ]
    }
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ (ì‹ ìš©ì¹´ë“œ ìŠ¹ì¸ ì—¬ë¶€)
    # ê°„ë‹¨í•œ ë¡œì§: ë‚˜ì´, ì†Œë“, ì‹ ìš©ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ìŠ¹ì¸ í™•ë¥  ë†’ìŒ
    approval_prob = (
        (data['age'] - 18) / 52 * 0.3 +
        np.clip((np.array(data['income']) - 30000) / 50000, 0, 1) * 0.4 +
        (np.array(data['credit_score']) - 300) / 550 * 0.3
    )
    data['approved'] = np.random.binomial(1, approval_prob, n_samples)
    
    return pd.DataFrame(data)


def generate_regression_data():
    """íšŒê·€ ë¬¸ì œìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    n_samples = 1000
    
    data = {
        'property_id': [f'prop_{i:04d}' for i in range(n_samples)],
        'size_sqft': np.random.normal(2000, 500, n_samples),
        'bedrooms': np.random.randint(1, 6, n_samples),
        'bathrooms': np.random.randint(1, 4, n_samples),
        'age_years': np.random.randint(0, 50, n_samples),
        'location_score': np.random.uniform(1, 10, n_samples),
        'school_rating': np.random.uniform(1, 10, n_samples),
        'crime_rate': np.random.uniform(0, 100, n_samples),
        'event_timestamp': [
            datetime.now() - timedelta(days=np.random.randint(0, 365))
            for _ in range(n_samples)
        ]
    }
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ (ì£¼íƒ ê°€ê²©)
    price = (
        data['size_sqft'] * 150 +
        np.array(data['bedrooms']) * 5000 +
        np.array(data['bathrooms']) * 3000 +
        (50 - np.array(data['age_years'])) * 1000 +
        np.array(data['location_score']) * 10000 +
        np.array(data['school_rating']) * 8000 +
        (100 - np.array(data['crime_rate'])) * 500 +
        np.random.normal(0, 20000, n_samples)  # ë…¸ì´ì¦ˆ
    )
    data['price'] = np.clip(price, 50000, 1000000)  # í˜„ì‹¤ì ì¸ ë²”ìœ„ë¡œ ì œí•œ
    
    return pd.DataFrame(data)


def generate_causal_data():
    """ì¸ê³¼ì¶”ë¡ /ì—…ë¦¬í”„íŠ¸ ë¬¸ì œìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    n_samples = 1000
    
    data = {
        'member_id': [f'member_{i:04d}' for i in range(n_samples)],
        'age': np.random.randint(18, 70, n_samples),
        'gender': np.random.choice(['M', 'F'], n_samples),
        'lifetime_value': np.random.lognormal(8, 1, n_samples),
        'days_since_last_purchase': np.random.exponential(30, n_samples),
        'avg_order_value': np.random.lognormal(4, 0.5, n_samples),
        'purchase_frequency': np.random.poisson(2, n_samples),
        'email_engagement': np.random.uniform(0, 1, n_samples),
        'event_timestamp': [
            datetime.now() - timedelta(days=np.random.randint(0, 365))
            for _ in range(n_samples)
        ]
    }
    
    # íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸ í• ë‹¹ (ëœë¤í™”)
    data['grp'] = np.random.choice(['control', 'treatment'], n_samples)
    
    # ê²°ê³¼ ë³€ìˆ˜ (êµ¬ë§¤ ì—¬ë¶€)
    # ë² ì´ìŠ¤ë¼ì¸ êµ¬ë§¤ í™•ë¥ 
    base_prob = np.clip(
        np.log(data['lifetime_value']) / 15 +
        (1 / (np.array(data['days_since_last_purchase']) + 1)) * 0.3 +
        np.array(data['email_engagement']) * 0.2,
        0.05, 0.95
    )
    
    # íŠ¸ë¦¬íŠ¸ë¨¼íŠ¸ íš¨ê³¼ (ì¼ë¶€ ê³ ê°ì—ê²Œë§Œ íš¨ê³¼ì )
    treatment_effect = np.where(
        (np.array(data['lifetime_value']) > np.median(data['lifetime_value'])) &
        (np.array(data['email_engagement']) > 0.5),
        0.15, 0.02  # ê³ ê°€ì¹˜ + ë†’ì€ ì´ë©”ì¼ ì°¸ì—¬ë„ ê³ ê°ì—ê²Œ ë” í° íš¨ê³¼
    )
    
    # ìµœì¢… êµ¬ë§¤ í™•ë¥ 
    final_prob = np.where(
        np.array(data['grp']) == 'treatment',
        base_prob + treatment_effect,
        base_prob
    )
    
    # í™•ë¥  ê°’ ê²€ì¦ ë° ì •ê·œí™”
    final_prob = np.clip(final_prob, 0.01, 0.99)  # 0-1 ë²”ìœ„ë¡œ ì œí•œ
    final_prob = np.nan_to_num(final_prob, nan=0.5)  # NaN ì²˜ë¦¬
    
    data['outcome'] = np.random.binomial(1, final_prob, n_samples)
    
    return pd.DataFrame(data)


def save_data():
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    data_dir = Path("data")
    
    # ë¶„ë¥˜ ë°ì´í„°
    classification_df = generate_classification_data()
    classification_df.to_parquet(data_dir / "processed" / "classification_test.parquet", index=False)
    classification_df.to_csv(data_dir / "processed" / "classification_test.csv", index=False)
    
    # íšŒê·€ ë°ì´í„°
    regression_df = generate_regression_data()
    regression_df.to_parquet(data_dir / "processed" / "regression_test.parquet", index=False)
    regression_df.to_csv(data_dir / "processed" / "regression_test.csv", index=False)
    
    # ì¸ê³¼ì¶”ë¡  ë°ì´í„°
    causal_df = generate_causal_data()
    causal_df.to_parquet(data_dir / "processed" / "causal_test.parquet", index=False)
    causal_df.to_csv(data_dir / "processed" / "causal_test.csv", index=False)
    
    # ë©”íƒ€ë°ì´í„° ìƒì„±
    metadata = {
        "generated_at": datetime.now().isoformat(),
        "datasets": {
            "classification": {
                "file": "classification_test.parquet",
                "task_type": "classification",
                "target_col": "approved",
                "n_samples": len(classification_df),
                "description": "ì‹ ìš©ì¹´ë“œ ìŠ¹ì¸ ì˜ˆì¸¡ ë°ì´í„°"
            },
            "regression": {
                "file": "regression_test.parquet", 
                "task_type": "regression",
                "target_col": "price",
                "n_samples": len(regression_df),
                "description": "ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ ë°ì´í„°"
            },
            "causal": {
                "file": "causal_test.parquet",
                "task_type": "causal", 
                "target_col": "outcome",
                "treatment_col": "grp",
                "treatment_value": "treatment",
                "n_samples": len(causal_df),
                "description": "ìº í˜ì¸ ì—…ë¦¬í”„íŠ¸ íš¨ê³¼ ë°ì´í„°"
            }
        }
    }
    
    with open(data_dir / "processed" / "metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
    
    print("âœ… LOCAL í™˜ê²½ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {data_dir.absolute() / 'processed'}")
    print(f"ğŸ“Š ë¶„ë¥˜ ë°ì´í„°: {len(classification_df):,} í–‰")
    print(f"ğŸ“Š íšŒê·€ ë°ì´í„°: {len(regression_df):,} í–‰")  
    print(f"ğŸ“Š ì¸ê³¼ì¶”ë¡  ë°ì´í„°: {len(causal_df):,} í–‰")
    print("")
    print("ğŸš€ í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´:")
    print("  APP_ENV=local python main.py train --recipe-file local_classification_test")


if __name__ == "__main__":
    save_data() 