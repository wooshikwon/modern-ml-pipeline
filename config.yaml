# 1. 실행 환경 설정
environment:
  run_mode: ${RUN_MODE:local}
  gcp_credential_path: ${GCP_CREDENTIAL_PATH}
  gcp_project_id: ${GCP_PROJECT_ID}

# 2-1. loader.py 관련 설정
loader:
  abt_logs: # 데이터셋 이름 (여러개가 될 수 있음)
    sql_file_path: "src/sql/abt_logs.sql"
    output: # output 저장 경로
      type: "bigquery"
      project_id: ${GCP_PROJECT_ID}
      dataset_id: "uplift_virtual_coupon"
      table_id: "abt_logs"
      unique_column: "member_id"
# e.g.
# other_data_name:
#   sql_file_path: "src/sql/other_data.sql"
#   output:
#     type: "bigquery"
#     project_id: ${GCP_PROJECT_ID}
#     dataset_id: "uplift_virtual_coupon"
#     table_id: "other_data"

# 2-2. preprocessor.py 관련 설정 (이번 프로젝트에선 불필요)
# preprocessor:

# 2-3. transformer.py 관련 설정
transformer:
  params:
    criterion_col: 'rsvn_30_count' # categorical 컬럼별로 grouping하여 기준 컬럼의 중앙값이 큰 순서대로 0 ~ 정수형 순서가 부여됨, 공란인 경우 빈도 기반으로 순서가 부여됨
    exclude_cols: ['member_id', 'grp', 'outcome'] # PK나 y값, treatment 컬럼 등 학습 컬럼이 아닌 컬럼들은 변환에서 제외
  output:
    type: "gcs"
    bucket_name: ${GCS_BUCKET_NAME}

# 3. 실험 관리 도구 설정 (mlflow)
mlflow:
  tracking_uri: ${MLFLOW_TRACKING_URI:"./local/artifacts"}
  experiment_name: "uplift-virtual-coupon"