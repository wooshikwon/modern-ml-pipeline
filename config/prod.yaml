# =============================================================================
# config/prod.yaml - í”„ë¡œë•ì…˜ í™˜ê²½ ì„¤ì •
# =============================================================================
# 
# ğŸ¯ PROD í™˜ê²½ ì² í•™: "ì„±ëŠ¥, ì•ˆì •ì„±, ê´€ì¸¡ ê°€ëŠ¥ì„±ì˜ ì™„ë²½í•œ ì‚¼ìœ„ì¼ì²´"
# - í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ì„œë¹„ìŠ¤ í™œìš©
# - ë¬´ì œí•œ í™•ì¥ì„±ê³¼ ìš´ì˜ ì•ˆì •ì„±
# - ì™„ì „í•œ ê´€ì¸¡ ê°€ëŠ¥ì„±ê³¼ ëª¨ë‹ˆí„°ë§
#
# ğŸ“‹ ì£¼ìš” Use Cases:
# 1. ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ ML ì„œë¹™ (1M+ requests/day)
# 2. ì—”í„°í”„ë¼ì´ì¦ˆ ë°°ì¹˜ ì¶”ë¡  (TBê¸‰ ë°ì´í„° ì²˜ë¦¬)
# 3. ë©€í‹° ë¦¬ì „ ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤
# 4. ê³ ê°€ìš©ì„± ë¯¸ì…˜ í¬ë¦¬í‹°ì»¬ ì‹œìŠ¤í…œ
# 5. ê·œì œ ì¤€ìˆ˜ ë° ê°ì‚¬ ì¶”ì 
# =============================================================================

# ğŸ“Š MLflow ì„¤ì • - í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ
mlflow:
  tracking_uri: ${MLFLOW_TRACKING_URI}  # í´ë¼ìš°ë“œ MLflow ì„œë²„
  experiment_name: "MMP-Prod-Experiment"
  
  # ğŸŒ Use Caseë³„ MLflow ì„¤ì • ì˜ˆì‹œ:
  #
  # ğŸ¢ GCP í™˜ê²½:
  #   tracking_uri: "https://mlflow.your-company.com"
  #   artifact_location: "gs://mmp-prod-artifacts"
  #
  # â˜ï¸ AWS í™˜ê²½:  
  #   tracking_uri: "https://mlflow.amazonaws.com"
  #   artifact_location: "s3://mmp-prod-artifacts"
  #
  # ğŸ”µ Azure í™˜ê²½:
  #   tracking_uri: "https://mlflow.azureml.com"
  #   artifact_location: "abfss://artifacts@company.dfs.core.windows.net"
  #
  # ğŸ­ ë©€í‹° ë¦¬ì „:
  #   tracking_uri: "${MLFLOW_TRACKING_URI_${REGION}}"
  #   experiment_name: "MMP-Prod-${REGION}-${ENVIRONMENT}"

# ğŸ—ƒï¸ ë°ì´í„° ì–´ëŒ‘í„° - ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ìŠ¤ì¼€ì¼
data_adapters:
  default_loader: "sql"              # ëŒ€ê·œëª¨ SQL ì¿¼ë¦¬ ì‹¤í–‰
  default_storage: "storage"         # í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€
  default_feature_store: "feature_store"  # Feature Store í™œìš©
  
  adapters:
    # SQL ì–´ëŒ‘í„° - ëŒ€ê·œëª¨ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤
    sql:
      class_name: SqlAdapter
      config:
        # ğŸŒ Use Caseë³„ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •:
        #
        # ğŸ¢ BigQuery (GCP):
        connection_uri: "bigquery://${GCP_PROJECT_ID}/${DATASET_ID}"
        # ì¶”ê°€ ì˜µì…˜:
        # job_config:
        #   maximum_bytes_billed: 1000000000  # 10GB ì œí•œ
        #   use_query_cache: true
        #   dry_run: false
        #
        # â„ï¸ Snowflake (Multi-cloud):
        # connection_uri: "snowflake://${SNOWFLAKE_USER}:${SNOWFLAKE_PASSWORD}@${SNOWFLAKE_ACCOUNT}/${SNOWFLAKE_DATABASE}/${SNOWFLAKE_SCHEMA}?warehouse=${SNOWFLAKE_WAREHOUSE}"
        #
        # ğŸŸ¦ Azure Synapse:
        # connection_uri: "mssql+pyodbc://${AZURE_SQL_USER}:${AZURE_SQL_PASSWORD}@${AZURE_SQL_SERVER}.database.windows.net:1433/${AZURE_SQL_DATABASE}?driver=ODBC+Driver+17+for+SQL+Server"
        #
        # ğŸ˜ PostgreSQL (Enterprise):
        # connection_uri: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/${POSTGRES_DB}"
        # pool_size: 20
        # max_overflow: 0
        # pool_pre_ping: true
    
    # Storage ì–´ëŒ‘í„° - í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€
    storage:
      class_name: StorageAdapter
      config:
        # ğŸŒ Use Caseë³„ ìŠ¤í† ë¦¬ì§€ ì„¤ì •:
        #
        # ğŸ¢ Google Cloud Storage:
        storage_options:
          project: ${GCP_PROJECT_ID}
          token: ${GCS_SERVICE_ACCOUNT_KEY}
        # ì¶”ê°€ ì˜µì…˜:
        # default_cache_type: "simple"
        # default_cache_storage: "gs://mmp-cache"
        #
        # â˜ï¸ AWS S3:
        # storage_options:
        #   key: ${AWS_ACCESS_KEY_ID}
        #   secret: ${AWS_SECRET_ACCESS_KEY}
        #   client_kwargs:
        #     region_name: ${AWS_REGION}
        #     endpoint_url: https://s3.${AWS_REGION}.amazonaws.com
        #
        # ğŸ”µ Azure Blob Storage:
        # storage_options:
        #   account_name: ${AZURE_STORAGE_ACCOUNT}
        #   account_key: ${AZURE_STORAGE_KEY}
        #   connection_string: ${AZURE_STORAGE_CONNECTION_STRING}
    
    # Feature Store ì–´ëŒ‘í„° - ì—”í„°í”„ë¼ì´ì¦ˆ Feature Store
    feature_store:
      class_name: FeastAdapter
      config: {}  # Feast ì„¤ì •ì€ feature_store ì„¹ì…˜ì—ì„œ ê´€ë¦¬

# ğŸ½ï¸ Feature Store ì„¤ì • - ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ í”¼ì²˜ ê´€ë¦¬
feature_store:
  provider: "feast"
  feast_config:
    project: "mmp-prod"
    provider: "gcp"  # ë˜ëŠ” "aws", "azure", "local"
    registry: "gs://mmp-prod-registry/registry.db"
    
    # Offline Store - ëŒ€ê·œëª¨ ë°°ì¹˜ í”¼ì²˜ ì²˜ë¦¬
    offline_store:
      type: "bigquery"
      project_id: ${GCP_PROJECT_ID}
      dataset_id: "feature_store"
      
      # ğŸŒ Alternative Offline Stores:
      #
      # â„ï¸ Snowflake:
      # type: "snowflake"
      # account: ${SNOWFLAKE_ACCOUNT}
      # user: ${SNOWFLAKE_USER}
      # password: ${SNOWFLAKE_PASSWORD}
      # database: "FEATURE_STORE"
      # warehouse: "COMPUTE_WH"
      #
      # ğŸ”µ Azure Synapse:
      # type: "synapse"
      # jdbc_driver: "com.microsoft.sqlserver.jdbc.SQLServerDriver"
      # jdbc_url: "jdbc:sqlserver://${AZURE_SYNAPSE_ENDPOINT}"
    
    # Online Store - ì‹¤ì‹œê°„ í”¼ì²˜ ì„œë¹™
    online_store:
      type: "redis"
      connection_string: ${REDIS_CONNECTION_STRING}
      ssl: true
      
      # ğŸŒ Alternative Online Stores:
      #
      # âš¡ Redis Cluster (ê³ ê°€ìš©ì„±):
      # type: "redis"
      # redis_type: "redis_cluster"
      # connection_string: ${REDIS_CLUSTER_CONNECTION_STRING}
      # startup_nodes:
      #   - host: redis-cluster-1.company.com
      #     port: 6379
      #   - host: redis-cluster-2.company.com
      #     port: 6379
      #
      # ğŸ—„ï¸ DynamoDB (AWS):
      # type: "dynamodb"
      # region: ${AWS_REGION}
      # table_name: "feast_online_store"
      # aws_access_key_id: ${AWS_ACCESS_KEY_ID}
      # aws_secret_access_key: ${AWS_SECRET_ACCESS_KEY}
      #
      # ğŸŸ¦ Azure Cosmos DB:
      # type: "cosmosdb"
      # connection_string: ${COSMOSDB_CONNECTION_STRING}
      # database_name: "feast_online_store"

# âš¡ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ - ëŒ€ê·œëª¨ ìµœì í™”
hyperparameter_tuning:
  enabled: true
  timeout: 7200           # 2ì‹œê°„ (ì¶©ë¶„í•œ íƒìƒ‰ ì‹œê°„)
  engine: "optuna"
  
  # ğŸ­ í”„ë¡œë•ì…˜ê¸‰ ìµœì í™” ì„¤ì •
  pruning:
    enabled: true
    algorithm: "MedianPruner"
    n_startup_trials: 10    # ì¶©ë¶„í•œ ì´ˆê¸° trial
    n_warmup_steps: 20
  
  parallelization:
    n_jobs: 16             # ê³ ì„±ëŠ¥ ë³‘ë ¬ ì²˜ë¦¬
    
  # ğŸ¯ Use Caseë³„ íŠœë‹ ì „ëµ:
  #
  # ğŸš€ ë¹ ë¥¸ ë°°í¬ (1ì‹œê°„ ì œí•œ):
  #   timeout: 3600
  #   parallelization:
  #     n_jobs: 32
  #
  # ğŸ”¬ ì‹¬ì¸µ ìµœì í™” (í•˜ë£¨ ì¢…ì¼):
  #   timeout: 86400        # 24ì‹œê°„
  #   parallelization:
  #     n_jobs: 64
  #   advanced_pruning:
  #     enabled: true
  #     min_trials: 100
  #
  # ğŸ’° ë¹„ìš© ìµœì í™”:
  #   timeout: 1800         # 30ë¶„
  #   cost_aware_pruning: true
  #   resource_limits:
  #     max_memory_gb: 32
  #     max_cpu_cores: 8

# ğŸŒ í™˜ê²½ ë³€ìˆ˜ - í”„ë¡œë•ì…˜ ì¸í”„ë¼
environment:
  app_env: "prod"
  gcp_project_id: ${GCP_PROJECT_ID}
  
  # ğŸ”’ ë³´ì•ˆ ì„¤ì •
  security:
    enable_authentication: true
    api_key_required: true
    rate_limiting: true
    
  # ğŸ“Š ëª¨ë‹ˆí„°ë§ ì„¤ì •
  monitoring:
    enable_metrics: true
    enable_tracing: true
    enable_profiling: false  # ì„±ëŠ¥ìƒ ê¸°ë³¸ ë¹„í™œì„±í™”
    
  # ğŸš¨ ì•Œë¦¼ ì„¤ì •
  alerting:
    enable_alerts: true
    error_threshold: 0.01    # 1% ì—ëŸ¬ìœ¨ì—ì„œ ì•Œë¦¼
    latency_threshold: 1000  # 1ì´ˆ ì´ìƒì—ì„œ ì•Œë¦¼

# ğŸ”„ API ì„œë¹™ - ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì„œë¹™
serving:
  enabled: true
  model_stage: "Production"  # MLflow ëª¨ë¸ ìŠ¤í…Œì´ì§€
  
  # ğŸ¯ Use Caseë³„ ì„œë¹™ ì„¤ì •:
  #
  # ğŸš€ ê³ ì„±ëŠ¥ ì„œë¹™:
  #   autoscaling:
  #     min_replicas: 3
  #     max_replicas: 100
  #     target_cpu_utilization: 70
  #   resources:
  #     cpu: "2000m"
  #     memory: "4Gi"
  #
  # ğŸŒ ê¸€ë¡œë²Œ ì„œë¹™:
  #   multi_region:
  #     enabled: true
  #     regions: ["us-central1", "europe-west1", "asia-southeast1"]
  #     load_balancing: "round_robin"
  #
  # ğŸ”’ ë³´ì•ˆ ê°•í™”:
  #   security:
  #     enable_https: true
  #     require_api_key: true
  #     enable_cors: false
  #     allowed_origins: ["https://company.com"]

# ğŸ“¦ ì•„í‹°íŒ©íŠ¸ ì €ì¥ì†Œ - ì—”í„°í”„ë¼ì´ì¦ˆ ë°ì´í„° ê´€ë¦¬
artifact_stores:
  # í”¼ì²˜ ì¦ê°• ë°ì´í„°
  augmented_dataset:
    enabled: true
    base_uri: "gs://mmp-prod-datasets/augmented"
    
    # ğŸŒ Use Caseë³„ ì €ì¥ì†Œ ì„¤ì •:
    # base_uri: "s3://mmp-prod-datasets/augmented"              # AWS
    # base_uri: "abfss://datasets@company.dfs.core.windows.net" # Azure
    # base_uri: "hdfs://hdfs-cluster/mmp/datasets"              # On-premise
    
  # ì „ì²˜ë¦¬ ë°ì´í„° (ê°ì‚¬ìš©)
  preprocessed_dataset:
    enabled: true
    base_uri: "gs://mmp-prod-datasets/preprocessed"
    retention_days: 90    # 90ì¼ ë³´ê´€
    
  # ì˜ˆì¸¡ ê²°ê³¼ (ê·œì œ ì¤€ìˆ˜ìš©)
  prediction_results:
    enabled: true
    base_uri: "gs://mmp-prod-predictions"
    
    # ğŸ“Š ë°ì´í„° ê±°ë²„ë„ŒìŠ¤
    governance:
      enable_lineage: true      # ë°ì´í„° ê³„ë³´ ì¶”ì 
      enable_versioning: true   # ë²„ì „ ê´€ë¦¬
      retention_policy: "5_years"  # 5ë…„ ë³´ê´€
      
    # ğŸ—„ï¸ ë°±ì—… ì„¤ì •
    backup:
      enabled: true
      schedule: "0 2 * * *"     # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
      retention: "1_year"
      
    # PostgreSQL ì €ì¥ (ì‹¤ì‹œê°„ ì¡°íšŒìš©)
    postgres_storage:
      enabled: true
      table_name: "batch_predictions"
      connection_uri: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:5432/${POSTGRES_DB}"
      
      # ğŸ¯ Use Caseë³„ DB ì„¤ì •:
      # 
      # ğŸ¢ Cloud SQL (GCP):
      # connection_uri: "postgresql://postgres:${POSTGRES_PASSWORD}@${CLOUD_SQL_IP}:5432/mmp_prod"
      # ssl_mode: "require"
      #
      # â˜ï¸ RDS (AWS):
      # connection_uri: "postgresql://postgres:${POSTGRES_PASSWORD}@${RDS_ENDPOINT}:5432/mmp_prod"
      # pool_size: 20
      #
      # ğŸ”µ Azure Database:
      # connection_uri: "postgresql://postgres@azure-server:${POSTGRES_PASSWORD}@azure-server.postgres.database.azure.com:5432/mmp_prod"

# ğŸ” ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ - ìš´ì˜ ê´€ì¸¡ì„±
logging:
  level: "INFO"
  format: "json"          # êµ¬ì¡°í™”ëœ ë¡œê¹…
  
  # ğŸ“Š ë¡œê·¸ ìˆ˜ì§‘
  aggregation:
    enabled: true
    
    # ğŸŒ Use Caseë³„ ë¡œê·¸ ìˆ˜ì§‘:
    # 
    # ğŸ¢ Google Cloud Logging:
    # service: "cloud_logging"
    # project_id: ${GCP_PROJECT_ID}
    #
    # â˜ï¸ AWS CloudWatch:
    # service: "cloudwatch"
    # log_group: "/aws/lambda/mmp-prod"
    # region: ${AWS_REGION}
    #
    # ğŸ”µ Azure Monitor:
    # service: "azure_monitor"
    # workspace_id: ${AZURE_WORKSPACE_ID}

# ğŸ’¡ ìš´ì˜ì ê°€ì´ë“œ:
#
# ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬:
#   APP_ENV=prod uv run python main.py train --recipe-file production_model
#
# ğŸ“Š ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰:
#   APP_ENV=prod uv run python main.py batch-inference --run-id <PROD_RUN_ID>
#
# ğŸŒ API ì„œë²„ ì‹œì‘:
#   APP_ENV=prod uv run python main.py serve-api --run-id <PROD_RUN_ID> --host 0.0.0.0 --port 8080
#
# ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ:
#   https://monitoring.company.com/mmp-prod
#
# ğŸš¨ ì•Œë¦¼ ì„¤ì •:
#   Slack: #mmp-alerts, PagerDuty: mmp-prod-oncall