# Blueprint v17.0 Model Catalog ğŸ¯

ì´ ë¬¸ì„œëŠ” **Blueprint v17.0 "Automated Excellence Vision"**ì—ì„œ ì œê³µí•˜ëŠ” **23ê°œ ëª¨ë¸ íŒ¨í‚¤ì§€**ì˜ ì™„ì „í•œ ì¹´íƒˆë¡œê·¸ì…ë‹ˆë‹¤. ëª¨ë“  ëª¨ë¸ì€ **ìë™í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**, **Data Leakage ë°©ì§€**, **í™˜ê²½ë³„ Feature Store ì—°ê²°**ì„ ì§€ì›í•©ë‹ˆë‹¤.

## ğŸ“Š ë¶„ë¥˜ ëª¨ë¸ (Classification) - 8ê°œ

### 1. Random Forest Classifier
- **íŒŒì¼**: `recipes/models/classification/random_forest_classifier.yaml`
- **class_path**: `sklearn.ensemble.RandomForestClassifier`
- **íŠ¹ì§•**: ì•™ìƒë¸” ê¸°ë°˜, ê°•ë ¥í•œ ì¼ë°˜í™” ì„±ëŠ¥, í•´ì„ ê°€ëŠ¥ì„±
- **ìµœì í™”**: n_estimators, max_depth, min_samples_split/leaf, max_features
- **ë©”íŠ¸ë¦­**: F1 Score (maximize)
- **trials**: 100íšŒ

### 2. Logistic Regression
- **íŒŒì¼**: `recipes/models/classification/logistic_regression.yaml`
- **class_path**: `sklearn.linear_model.LogisticRegression`
- **íŠ¹ì§•**: ê°„ë‹¨í•˜ê³  í•´ì„ ê°€ëŠ¥í•œ ì„ í˜• ë¶„ë¥˜, í™•ë¥  ì¶œë ¥
- **ìµœì í™”**: C (ì •ê·œí™” ê°•ë„), penalty (L1/L2/ElasticNet), solver
- **ë©”íŠ¸ë¦­**: ROC AUC (maximize)
- **trials**: 50íšŒ

### 3. XGBoost Classifier
- **íŒŒì¼**: `recipes/models/classification/xgboost_classifier.yaml`
- **class_path**: `xgboost.XGBClassifier`
- **íŠ¹ì§•**: Gradient Boostingì˜ í˜ì‹ , ë›°ì–´ë‚œ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„±
- **ìµœì í™”**: learning_rate, n_estimators, max_depth, subsample, colsample_bytree, regularization
- **ë©”íŠ¸ë¦­**: ROC AUC (maximize)
- **trials**: 100íšŒ

### 4. LightGBM Classifier
- **íŒŒì¼**: `recipes/models/classification/lightgbm_classifier.yaml`
- **class_path**: `lightgbm.LGBMClassifier`
- **íŠ¹ì§•**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì , ë¹ ë¥¸ Gradient Boosting
- **ìµœì í™”**: learning_rate, n_estimators, num_leaves, feature_fraction, bagging
- **ë©”íŠ¸ë¦­**: F1 Score (maximize)
- **trials**: 80íšŒ

### 5. CatBoost Classifier
- **íŒŒì¼**: `recipes/models/classification/catboost_classifier.yaml`
- **class_path**: `catboost.CatBoostClassifier`
- **íŠ¹ì§•**: ë²”ì£¼í˜• í”¼ì²˜ ìë™ ì²˜ë¦¬, ë†’ì€ ì„±ëŠ¥
- **ìµœì í™”**: learning_rate, iterations, depth, l2_leaf_reg, border_count
- **ë©”íŠ¸ë¦­**: ROC AUC (maximize)
- **trials**: 60íšŒ

### 6. Support Vector Machine
- **íŒŒì¼**: `recipes/models/classification/svm_classifier.yaml`
- **class_path**: `sklearn.svm.SVC`
- **íŠ¹ì§•**: ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ìµœì  ê²°ì • ê²½ê³„ íƒìƒ‰
- **ìµœì í™”**: C, kernel (linear/poly/rbf/sigmoid), gamma, degree
- **ë©”íŠ¸ë¦­**: Accuracy (maximize)
- **trials**: 30íšŒ

### 7. Gaussian Naive Bayes
- **íŒŒì¼**: `recipes/models/classification/naive_bayes.yaml`
- **class_path**: `sklearn.naive_bayes.GaussianNB`
- **íŠ¹ì§•**: í™•ë¥  ê¸°ë°˜, ê°„ë‹¨í•˜ê³  íš¨ê³¼ì 
- **ìµœì í™”**: var_smoothing (ìŠ¤ë¬´ë”© íŒŒë¼ë¯¸í„°)
- **ë©”íŠ¸ë¦­**: F1 Score (maximize)
- **trials**: 20íšŒ

### 8. K-Nearest Neighbors
- **íŒŒì¼**: `recipes/models/classification/knn_classifier.yaml`
- **class_path**: `sklearn.neighbors.KNeighborsClassifier`
- **íŠ¹ì§•**: ê±°ë¦¬ ê¸°ë°˜, ì§ê´€ì ì´ê³  ìœ ì—°
- **ìµœì í™”**: n_neighbors, weights, metric, algorithm
- **ë©”íŠ¸ë¦­**: Accuracy (maximize)
- **trials**: 40íšŒ

---

## ğŸ“ˆ íšŒê·€ ëª¨ë¸ (Regression) - 8ê°œ

### 1. Linear Regression
- **íŒŒì¼**: `recipes/models/regression/linear_regression.yaml`
- **class_path**: `sklearn.linear_model.LinearRegression`
- **íŠ¹ì§•**: ë‹¨ìˆœí•˜ê³  í•´ì„ ê°€ëŠ¥í•œ ì„ í˜• ê´€ê³„ ëª¨ë¸ë§
- **ìµœì í™”**: fit_intercept, positive (ì œì•½)
- **ë©”íŠ¸ë¦­**: RÂ² Score (maximize)
- **trials**: 10íšŒ

### 2. Ridge Regression
- **íŒŒì¼**: `recipes/models/regression/ridge_regression.yaml`
- **class_path**: `sklearn.linear_model.Ridge`
- **íŠ¹ì§•**: L2 ì •ê·œí™”ë¡œ ê³¼ì í•© ë°©ì§€
- **ìµœì í™”**: alpha (ì •ê·œí™” ê°•ë„), solver
- **ë©”íŠ¸ë¦­**: RÂ² Score (maximize)
- **trials**: 30íšŒ

### 3. Lasso Regression
- **íŒŒì¼**: `recipes/models/regression/lasso_regression.yaml`
- **class_path**: `sklearn.linear_model.Lasso`
- **íŠ¹ì§•**: L1 ì •ê·œí™”ë¡œ ìë™ í”¼ì²˜ ì„ íƒ
- **ìµœì í™”**: alpha (ì •ê·œí™” ê°•ë„), selection
- **ë©”íŠ¸ë¦­**: RÂ² Score (maximize)
- **trials**: 25íšŒ

### 4. Random Forest Regressor
- **íŒŒì¼**: `recipes/models/regression/random_forest_regressor.yaml`
- **class_path**: `sklearn.ensemble.RandomForestRegressor`
- **íŠ¹ì§•**: ì•™ìƒë¸” ê¸°ë°˜, ê°•ë ¥í•˜ê³  ì•ˆì •ì ì¸ ë¹„ì„ í˜• íšŒê·€
- **ìµœì í™”**: n_estimators, max_depth, min_samples_split/leaf, max_features, bootstrap
- **ë©”íŠ¸ë¦­**: RMSE (minimize)
- **trials**: 80íšŒ

### 5. XGBoost Regressor
- **íŒŒì¼**: `recipes/models/regression/xgboost_regressor.yaml`
- **class_path**: `xgboost.XGBRegressor`
- **íŠ¹ì§•**: Gradient Boostingì˜ íšŒê·€ íŠ¹í™”, ë›°ì–´ë‚œ ì„±ëŠ¥
- **ìµœì í™”**: learning_rate, n_estimators, max_depth, subsample, regularization
- **ë©”íŠ¸ë¦­**: RMSE (minimize)
- **trials**: 100íšŒ

### 6. LightGBM Regressor
- **íŒŒì¼**: `recipes/models/regression/lightgbm_regressor.yaml`
- **class_path**: `lightgbm.LGBMRegressor`
- **íŠ¹ì§•**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì , ë¹ ë¥¸ Gradient Boosting íšŒê·€
- **ìµœì í™”**: learning_rate, n_estimators, num_leaves, feature_fraction, bagging
- **ë©”íŠ¸ë¦­**: RMSE (minimize)
- **trials**: 80íšŒ

### 7. Support Vector Regressor
- **íŒŒì¼**: `recipes/models/regression/svr.yaml`
- **class_path**: `sklearn.svm.SVR`
- **íŠ¹ì§•**: ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ê°•ê±´í•œ íšŒê·€ ì˜ˆì¸¡
- **ìµœì í™”**: C, epsilon, kernel, gamma, degree
- **ë©”íŠ¸ë¦­**: RMSE (minimize)
- **trials**: 30íšŒ

### 8. Elastic Net Regression
- **íŒŒì¼**: `recipes/models/regression/elastic_net.yaml`
- **class_path**: `sklearn.linear_model.ElasticNet`
- **íŠ¹ì§•**: L1ê³¼ L2 ì •ê·œí™” ê²°í•©, ê· í˜•ì¡íŒ ì„ í˜• íšŒê·€
- **ìµœì í™”**: alpha (ì •ê·œí™” ê°•ë„), l1_ratio (L1/L2 ë¹„ìœ¨), selection
- **ë©”íŠ¸ë¦­**: RÂ² Score (maximize)
- **trials**: 40íšŒ

---

## ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ëª¨ë¸ (Clustering) - 3ê°œ

### 1. K-Means Clustering
- **íŒŒì¼**: `recipes/models/clustering/kmeans.yaml`
- **class_path**: `sklearn.cluster.KMeans`
- **íŠ¹ì§•**: ì¤‘ì‹¬ì  ê¸°ë°˜, ì§ê´€ì ì´ê³  íš¨ìœ¨ì 
- **ìµœì í™”**: n_clusters, init, n_init, algorithm
- **ë©”íŠ¸ë¦­**: Silhouette Score (maximize)
- **trials**: 30íšŒ

### 2. DBSCAN Clustering
- **íŒŒì¼**: `recipes/models/clustering/dbscan.yaml`
- **class_path**: `sklearn.cluster.DBSCAN`
- **íŠ¹ì§•**: ë°€ë„ ê¸°ë°˜, ë…¸ì´ì¦ˆ ì œê±° ë° ì„ì˜ í˜•íƒœ í´ëŸ¬ìŠ¤í„° ë°œê²¬
- **ìµœì í™”**: eps (ì´ì›ƒ ê±°ë¦¬), min_samples, metric, algorithm
- **ë©”íŠ¸ë¦­**: Calinski-Harabasz Score (maximize)
- **trials**: 50íšŒ

### 3. Hierarchical Clustering
- **íŒŒì¼**: `recipes/models/clustering/hierarchical_clustering.yaml`
- **class_path**: `sklearn.cluster.AgglomerativeClustering`
- **íŠ¹ì§•**: ê³„ì¸µì  êµ¬ì¡° ë°œê²¬, íŠ¸ë¦¬ ê¸°ë°˜
- **ìµœì í™”**: n_clusters, linkage, metric
- **ë©”íŠ¸ë¦­**: Silhouette Score (maximize)
- **trials**: 25íšŒ

---

## ğŸ­ ì¸ê³¼ì¶”ë¡  ëª¨ë¸ (Causal Inference) - 4ê°œ

### 1. Causal Random Forest
- **íŒŒì¼**: `recipes/models/causal/causal_random_forest.yaml`
- **class_path**: `causalml.inference.tree.CausalRandomForestRegressor`
- **íŠ¹ì§•**: íŠ¸ë¦¬ ê¸°ë°˜ ê°•ë ¥í•œ ì—…ë¦¬í”„íŠ¸ ëª¨ë¸ë§
- **ìµœì í™”**: n_estimators, max_depth, min_samples_split/leaf, max_features
- **ë©”íŠ¸ë¦­**: Uplift AUC (maximize)
- **trials**: 80íšŒ

### 2. XGBoost T-Learner
- **íŒŒì¼**: `recipes/models/causal/xgb_t_learner.yaml`
- **class_path**: `causalml.inference.meta.XGBTRegressor`
- **íŠ¹ì§•**: Meta-Learning ê¸°ë°˜ XGBoost ê³ ì„±ëŠ¥ ì¸ê³¼íš¨ê³¼ ì¶”ì •
- **ìµœì í™”**: learning_rate, n_estimators, max_depth, subsample, regularization
- **ë©”íŠ¸ë¦­**: Uplift AUC (maximize)
- **trials**: 100íšŒ

### 3. S-Learner (Single Model)
- **íŒŒì¼**: `recipes/models/causal/s_learner.yaml`
- **class_path**: `causalml.inference.meta.SRegressor`
- **íŠ¹ì§•**: ë‹¨ì¼ ëª¨ë¸ë¡œ ì²˜ë¦¬êµ°ê³¼ ëŒ€ì¡°êµ° í•¨ê»˜ í•™ìŠµ
- **ìµœì í™”**: n_estimators, max_depth, min_samples_split/leaf, max_features
- **ë©”íŠ¸ë¦­**: Uplift AUC (maximize)
- **trials**: 60íšŒ

### 4. T-Learner (Two Model)
- **íŒŒì¼**: `recipes/models/causal/t_learner.yaml`
- **class_path**: `causalml.inference.meta.TRegressor`
- **íŠ¹ì§•**: ì²˜ë¦¬êµ°ê³¼ ëŒ€ì¡°êµ°ì„ ë³„ë„ ëª¨ë¸ë¡œ í•™ìŠµí•˜ëŠ” í´ë˜ì‹ ì¸ê³¼ì¶”ë¡ 
- **ìµœì í™”**: n_estimators, max_depth, min_samples_split/leaf, max_features, bootstrap
- **ë©”íŠ¸ë¦­**: Uplift AUC (maximize)
- **trials**: 70íšŒ

---

## ğŸš€ ëª¨ë“  ëª¨ë¸ì˜ ê³µí†µ íŠ¹ì§•

### Blueprint v17.0 í•µì‹¬ ê¸°ëŠ¥
- âœ… **ìë™í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**: Optuna ê¸°ë°˜ ë²”ìœ„ íƒìƒ‰
- âœ… **Data Leakage ì™„ì „ ë°©ì§€**: Train-only preprocessing fit
- âœ… **í™˜ê²½ë³„ Feature Store ì—°ê²°**: ë™ì  í”¼ì²˜ ì¦ê°•
- âœ… **ì™„ì „í•œ ì¬í˜„ì„±**: ëª¨ë“  ìµœì í™” ê³¼ì • ì¶”ì  ë° ì €ì¥
- âœ… **100% í•˜ìœ„ í˜¸í™˜ì„±**: ê¸°ì¡´ ì½”ë“œ ë³€ê²½ ì—†ì´ ì ì§„ì  í™œì„±í™”

### ì‚¬ìš©ë²• ì˜ˆì‹œ
```bash
# ì„ì˜ì˜ ëª¨ë¸ë¡œ ìë™ ìµœì í™” í•™ìŠµ
python main.py train --recipe-file "models/classification/xgboost_classifier"

# ìë™ ìµœì í™” ë¹„í™œì„±í™” (ê¸°ì¡´ ë°©ì‹)
# recipe íŒŒì¼ì—ì„œ hyperparameter_tuning.enabled: falseë¡œ ì„¤ì •

# ë°°ì¹˜ ì¶”ë¡  (ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ì ìš©)
python main.py batch-inference --run-id "abc123"

# API ì„œë¹™ (ìµœì í™”ëœ ëª¨ë¸ë¡œ ì‹¤ì‹œê°„ ì„œë¹™)
python main.py serve-api --run-id "abc123"
```

---

**ğŸ‰ ì´ì œ 23ê°œì˜ ë‹¤ì–‘í•œ ëª¨ë¸ íŒ¨í‚¤ì§€ë¡œ ë¬´ì œí•œì ì¸ ì‹¤í—˜ì„ ì¦ê¸°ì„¸ìš”!** 