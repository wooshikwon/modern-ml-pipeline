# =============================================================================
# Config: {{ env_name }} environment
# Generated by MMP get-config
# =============================================================================
# Config는 "어디서 실행할지"를 정의합니다 (DB 연결, 스토리지, MLflow 등)
# Recipe는 "무엇을 학습할지"를 정의합니다 (모델, 데이터 구조, 전처리)
#
# 환경변수 문법: ${VAR_NAME:기본값} - VAR_NAME 없으면 기본값 사용
# 예: ${DB_PASSWORD:} → DB_PASSWORD 없으면 빈 문자열
# =============================================================================

environment:
  name: {{ env_name }}

# -----------------------------------------------------------------------------
# MLflow 실험 추적
# -----------------------------------------------------------------------------
{% if use_mlflow %}
mlflow:
  tracking_uri: {{ mlflow_tracking_uri }}  # 로컬: ./mlruns, 원격: http://mlflow-server:5000
  experiment_name: {{ mlflow_experiment_name }}
{% endif %}

# -----------------------------------------------------------------------------
# 데이터 소스 설정
# -----------------------------------------------------------------------------
# adapter_type은 Recipe의 source_uri 확장자와 호환 필요:
#   - sql: .sql, .sql.j2 파일용 (BigQuery, PostgreSQL 등 DB 쿼리)
#   - storage: .csv, .parquet 파일용 (Local, S3, GCS 등 파일 스토리지)
data_source:
  name: {{ data_source }}
{% if data_source == "PostgreSQL" %}
  adapter_type: sql
  config:
    # 환경변수로 민감정보 관리 권장
    connection_uri: "postgresql://${DB_USER:postgres}:${DB_PASSWORD:}@{{ db_host }}:{{ db_port }}/{{ db_name }}"
    query_timeout: 300
{% elif data_source == "BigQuery" %}
  adapter_type: sql
  config:
    connection_uri: bigquery://{{ gcp_project_id }}
    project_id: {{ gcp_project_id }}
    credentials_path: "${GOOGLE_APPLICATION_CREDENTIALS}"  # 서비스 계정 JSON 경로
    use_pandas_gbq: true
{% elif data_source == "Local Files" %}
  adapter_type: storage
  config:
    base_path: {{ local_data_path }}  # 상대경로(./data) 또는 절대경로
    storage_options: {}
{% elif data_source == "S3" %}
  adapter_type: storage
  config:
    base_path: s3://{{ s3_data_bucket }}/{{ s3_data_prefix }}
    storage_options:
      aws_access_key_id: "${AWS_ACCESS_KEY_ID}"
      aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY}"
{% elif data_source == "GCS" %}
  adapter_type: storage
  config:
    base_path: gs://{{ gcs_data_bucket }}/{{ gcs_data_prefix }}
    storage_options:
      project: {{ gcp_project_id }}
      token: "${GOOGLE_APPLICATION_CREDENTIALS}"
{% endif %}

# -----------------------------------------------------------------------------
# Feature Store 설정 (선택)
# -----------------------------------------------------------------------------
{% if use_feast %}
feature_store:
  provider: feast
  feast_config:
    project: {{ feast_project }}
    registry: {{ feast_registry_path }}  # Feast 메타데이터 저장소
    online_store:  # 실시간 피처 조회용
{% if feast_online_store == "Redis" %}
      type: redis
      connection_string: "{{ redis_host }}:{{ redis_port }}"
      password: "${REDIS_PASSWORD:}"
{% elif feast_online_store == "DynamoDB" %}
      type: dynamodb
      region: {{ dynamodb_region }}
      table_name: {{ dynamodb_table }}
{% else %}
      type: sqlite
      path: {{ feast_online_store_path }}
{% endif %}
    offline_store:  # 배치 학습용 히스토리 데이터
{% if data_source == "BigQuery" %}
      type: bigquery
      project_id: {{ gcp_project_id }}
      dataset_id: {{ bq_dataset_id|default('mmp_dataset') }}
{% elif data_source in ["S3", "GCS", "Local Files", "PostgreSQL"] %}
      type: file
      path: {{ feast_offline_path }}
{% endif %}
    entity_key_serialization_version: 2
{% else %}
feature_store:
  provider: none  # Feature Store 미사용
{% endif %}

# -----------------------------------------------------------------------------
# API 서빙 설정
# -----------------------------------------------------------------------------
serving:
  enabled: true
  model_stage: "None"              # MLflow 모델 스테이지 (None, Staging, Production)
  request_timeout_seconds: 30      # API 요청 타임아웃
  metrics_enabled: true            # Prometheus 메트릭 노출

# -----------------------------------------------------------------------------
# 로깅 설정
# -----------------------------------------------------------------------------
logging:
  base_path: {{ log_path }}        # 로그 파일 저장 경로
  level: {{ log_level }}           # DEBUG, INFO, WARNING, ERROR
  retention_days: {{ log_retention_days }}  # 로그 보관 기간
  upload_to_mlflow: true           # MLflow에 로그 아티팩트 업로드

# -----------------------------------------------------------------------------
# 배치 추론 결과 저장 설정
# -----------------------------------------------------------------------------
output:
  inference:
    name: InferenceOutput
{% if not inference_output_enabled %}
    enabled: false  # 배치 추론 결과 저장 비활성화
{% else %}
    enabled: true
{% if inference_output_source == "Local Files" %}
    adapter_type: storage
    config:
      base_path: {{ infer_local_path }}
{% if infer_file_name %}
      file_name: {{ infer_file_name }}
{% endif %}
      file_format: {{ infer_file_format|default('parquet') }}
{% elif inference_output_source == "S3" %}
    adapter_type: storage
    config:
      base_path: s3://{{ infer_s3_bucket }}/{{ infer_s3_prefix }}
{% if infer_file_name %}
      file_name: {{ infer_file_name }}
{% endif %}
      file_format: {{ infer_file_format|default('parquet') }}
      storage_options:
        aws_access_key_id: "${AWS_ACCESS_KEY_ID}"
        aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY}"
{% elif inference_output_source == "GCS" %}
    adapter_type: storage
    config:
      base_path: gs://{{ infer_gcs_bucket }}/{{ infer_gcs_prefix }}
{% if infer_file_name %}
      file_name: {{ infer_file_name }}
{% endif %}
      file_format: {{ infer_file_format|default('parquet') }}
      storage_options:
        project: {{ gcp_project_id }}
        token: "${GOOGLE_APPLICATION_CREDENTIALS}"
{% elif inference_output_source == "PostgreSQL" %}
    adapter_type: sql
    config:
      connection_uri: "postgresql://${DB_USER:postgres}:${DB_PASSWORD:}@{{ db_host }}:{{ db_port }}/{{ db_name }}"
      table: {{ infer_pg_table }}
{% elif inference_output_source == "BigQuery" %}
    adapter_type: sql
    config:
      connection_uri: bigquery://{{ gcp_project_id }}/{{ infer_bq_dataset }}
      project_id: {{ gcp_project_id }}
      dataset_id: {{ infer_bq_dataset }}
      table: {{ infer_bq_table }}
      use_pandas_gbq: true
{% endif %}
{% endif %}
