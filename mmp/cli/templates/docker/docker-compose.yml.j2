# {{ project_name }} - Docker Compose Configuration
# Generated by Modern ML Pipeline

services:
  # API Serving
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: api
    image: {{ project_name }}:api
    container_name: {{ project_name }}_api
    ports:
      - "${API_PORT:-8000}:8000"
    volumes:
      - ./configs:/app/configs:ro
      - ./mlruns:/app/mlruns:ro
      - ./logs:/app/logs
    environment:
      MODEL_RUN_ID: ${MODEL_RUN_ID}
      CONFIG_PATH: ${CONFIG_PATH:-configs/production.yaml}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-./mlruns}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/ready"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3

  # Training (on-demand)
  training:
    build:
      context: .
      dockerfile: Dockerfile
      target: training
    image: {{ project_name }}:training
    container_name: {{ project_name }}_training
    volumes:
      - ./configs:/app/configs:ro
      - ./recipes:/app/recipes:ro
      - ./data:/app/data:ro
      - ./sql:/app/sql:ro
      - ./mlruns:/app/mlruns
      - ./logs:/app/logs
    environment:
      RECIPE_PATH: ${RECIPE_PATH:-recipes/model.yaml}
      CONFIG_PATH: ${CONFIG_PATH:-configs/production.yaml}
      TRAIN_DATA_PATH: ${TRAIN_DATA_PATH:-data/train.csv}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-./mlruns}
    profiles:
      - training

  # Batch Inference (on-demand)
  inference:
    build:
      context: .
      dockerfile: Dockerfile
      target: inference
    image: {{ project_name }}:inference
    container_name: {{ project_name }}_inference
    volumes:
      - ./configs:/app/configs:ro
      - ./data:/app/data:ro
      - ./mlruns:/app/mlruns:ro
      - ./artifacts:/app/artifacts
      - ./logs:/app/logs
    environment:
      MODEL_RUN_ID: ${MODEL_RUN_ID}
      CONFIG_PATH: ${CONFIG_PATH:-configs/production.yaml}
      INFERENCE_DATA_PATH: ${INFERENCE_DATA_PATH}
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-./mlruns}
    profiles:
      - inference

  # MLflow UI (optional)
  mlflow:
    image: python:3.11-slim
    container_name: {{ project_name }}_mlflow
    command: >
      bash -c "pip install --no-cache-dir mlflow && mlflow ui --host 0.0.0.0 --port 5000 --backend-store-uri ./mlruns"
    ports:
      - "${MLFLOW_PORT:-5000}:5000"
    volumes:
      - ./mlruns:/app/mlruns
    working_dir: /app
    profiles:
      - mlflow

# Usage:
#   API 서버 실행:      docker-compose up api
#   학습 실행:          docker-compose --profile training up training
#   배치 추론:          docker-compose --profile inference up inference
#   MLflow UI:          docker-compose --profile mlflow up mlflow
