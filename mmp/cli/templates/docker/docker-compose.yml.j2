# =============================================================================
# Docker Compose for {{ project_name }}
# Generated by Modern ML Pipeline
# =============================================================================
# 단일 이미지로 학습, 추론, API 서빙 모두 지원
#
# 사용법:
#   docker-compose up api                             # API 서버 시작
#   docker-compose run --rm train                     # 학습 실행
#   docker-compose run --rm inference                 # 배치 추론 실행
#   docker-compose --profile mlflow up mlflow         # MLflow UI (선택)
#
# 환경변수 (.env 파일 또는 export):
#   MODEL_RUN_ID=<mlflow_run_id>    # API/추론 시 필수
#   CONFIG_PATH=configs/prod.yaml   # 설정 파일 경로
#   RECIPE_PATH=recipes/model.yaml  # 학습 시 레시피 경로
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # API 서버 (상시 실행)
  # ---------------------------------------------------------------------------
  api:
    build: .
    image: {{ project_name }}:latest
    container_name: {{ project_name }}_api
    command: >
      mmp serve-api
        --run-id ${MODEL_RUN_ID}
        --config ${CONFIG_PATH:-configs/production.yaml}
        --host 0.0.0.0
        --port 8000
    ports:
      - "${API_PORT:-8000}:8000"
    volumes:
      - ./configs:/app/configs:ro
      - ./mlruns:/app/mlruns:ro
      - ./logs:/app/logs
    environment:
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-./mlruns}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/ready"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3

  # ---------------------------------------------------------------------------
  # 학습 (on-demand: docker-compose run --rm train)
  # ---------------------------------------------------------------------------
  train:
    build: .
    image: {{ project_name }}:latest
    container_name: {{ project_name }}_train
    command: >
      mmp train
        --recipe ${RECIPE_PATH:-recipes/model.yaml}
        --config ${CONFIG_PATH:-configs/production.yaml}
        --data ${TRAIN_DATA_PATH:-data/train.csv}
    volumes:
      - ./configs:/app/configs:ro
      - ./recipes:/app/recipes:ro
      - ./data:/app/data:ro
      - ./mlruns:/app/mlruns
      - ./logs:/app/logs
    environment:
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-./mlruns}
    profiles:
      - tools

  # ---------------------------------------------------------------------------
  # 배치 추론 (on-demand: docker-compose run --rm inference)
  # ---------------------------------------------------------------------------
  inference:
    build: .
    image: {{ project_name }}:latest
    container_name: {{ project_name }}_inference
    command: >
      mmp batch-inference
        --run-id ${MODEL_RUN_ID}
        --config ${CONFIG_PATH:-configs/production.yaml}
        --data ${INFERENCE_DATA_PATH:-data/test.csv}
    volumes:
      - ./configs:/app/configs:ro
      - ./data:/app/data:ro
      - ./mlruns:/app/mlruns:ro
      - ./artifacts:/app/artifacts
      - ./logs:/app/logs
    environment:
      MLFLOW_TRACKING_URI: ${MLFLOW_TRACKING_URI:-./mlruns}
    profiles:
      - tools

  # ---------------------------------------------------------------------------
  # MLflow UI (선택: docker-compose --profile mlflow up mlflow)
  # ---------------------------------------------------------------------------
  mlflow:
    image: python:3.11-slim
    container_name: {{ project_name }}_mlflow
    command: >
      bash -c "pip install --no-cache-dir mlflow && mlflow ui --host 0.0.0.0 --port 5000 --backend-store-uri ./mlruns"
    ports:
      - "${MLFLOW_PORT:-5000}:5000"
    volumes:
      - ./mlruns:/app/mlruns
    working_dir: /app
    profiles:
      - mlflow
